{"title":"《深度学习》 笔记（二）","uid":"e4001ef3af977cd071aff3164f892814","slug":"深度学习2","date":"2022-11-03T13:56:49.000Z","updated":"2022-11-03T13:57:08.483Z","comments":true,"path":"api/articles/深度学习2.json","keywords":null,"cover":null,"content":"<h1 id=\"第二部分-深度网络：现代实践\"><a href=\"#第二部分-深度网络：现代实践\" class=\"headerlink\" title=\"第二部分 深度网络：现代实践\"></a>第二部分 深度网络：现代实践</h1><h2 id=\"深度前馈网络\"><a href=\"#深度前馈网络\" class=\"headerlink\" title=\"深度前馈网络\"></a>深度前馈网络</h2><p><strong>深度前馈网络</strong>，也叫做<strong>前馈神经网络</strong>或者<strong>多层感知机</strong>，是典型的深度学习模型。</p>\n<p>这种模型被称为<strong>前向</strong>的，是因为信息流过$x$的函数，流经用于定义$f$的中间计算过程，最终到达输出$y$。在模型的输出和模型本身之间没有<strong>反馈</strong>连接。当前馈神经网络被扩展成包含反馈连接时，它们被称为<strong>循环神经网络</strong>。</p>\n<p>前馈神经网络被称作<strong>网络</strong>是因为它们通常用不同函数复合在一起来表示。<br>该模型与一个有向无环图相关联，而图描述了函数是如何复合在一起的。</p>\n<p>例如，我们有三个函数$f^{(1)}$,$f^{(2)}$,$f^{(3)}$连接在一个链上以形成$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$。这些链式结构是神经网络中最常用的结构。<br>在这种情况下，$f^{(1)}$被称为网络的<strong>第一层</strong>，$f^{(2)}$被称为<strong>第二层</strong>，以此类推。<br>链的全长称为模型的<strong>深度</strong>。前馈网络的最后一层被称为 <strong>输出层</strong>。</p>\n<h3 id=\"基于梯度的学习\"><a href=\"#基于梯度的学习\" class=\"headerlink\" title=\"基于梯度的学习\"></a>基于梯度的学习</h3><p>混合密度网络<br>反向传播</p>\n<h2 id=\"深度学习中的正则化\"><a href=\"#深度学习中的正则化\" class=\"headerlink\" title=\"深度学习中的正则化\"></a>深度学习中的正则化</h2><h3 id=\"参数范数惩罚\"><a href=\"#参数范数惩罚\" class=\"headerlink\" title=\"参数范数惩罚\"></a>参数范数惩罚</h3><p>权重衰减</p>\n<p>作为约束的范数惩罚</p>\n<h3 id=\"数据集增强\"><a href=\"#数据集增强\" class=\"headerlink\" title=\"数据集增强\"></a>数据集增强</h3><p>噪声鲁棒性<br>半监督学习<br>多任务学习</p>\n<h2 id=\"深度模型中的优化\"><a href=\"#深度模型中的优化\" class=\"headerlink\" title=\"深度模型中的优化\"></a>深度模型中的优化</h2><p>机器学习算法的目标是降低期望泛化误差。</p>\n<h3 id=\"批量算法和小批量算法\"><a href=\"#批量算法和小批量算法\" class=\"headerlink\" title=\"批量算法和小批量算法\"></a>批量算法和小批量算法</h3><p>机器学习算法和一般优化算法不同的一点是，机器学习算法的目标函数通常可以分解为训练样本上的求和。机器学习中的优化算法在计算参数的每一次更新时通常仅使用整个代价函数中的一部分项来估计代价函数的期望值。</p>\n<h3 id=\"基本算法\"><a href=\"#基本算法\" class=\"headerlink\" title=\"基本算法\"></a>基本算法</h3><h4 id=\"随机梯度下降\"><a href=\"#随机梯度下降\" class=\"headerlink\" title=\"随机梯度下降\"></a>随机梯度下降</h4><p>随机梯度下降（SGD）及其变种很可能是一般机器学习中应用最多的优化算法，特别是在深度学习中。按照数据生成分布抽取$m$个小批量样本，通过计算它们的梯度均值，我们可以得到梯度的无偏估计。</p>\n<h2 id=\"卷积网络\"><a href=\"#卷积网络\" class=\"headerlink\" title=\"卷积网络\"></a>卷积网络</h2><p><strong>卷积网络</strong>，也叫做<strong>卷积神经网络</strong>，是一种专门用来处理具有类似网格结构的数据的神经网络。例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作是二维的像素网络）。</p>\n<p>“卷积神经网络”一词表明该网络使用了<strong>卷积</strong>这种数学运算。卷积是一种特殊的线性运算。<em>卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。</em></p>\n<h3 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h3><p>在通常形式中，卷积是对两个实变函数的一种数学运算。<br>卷积运算通常用星号表示：<br>$s(t)=(x<em>w)(t).$<br>在卷积网络的术语中，卷积的第一个参数（$x$）通常叫做<strong>输入</strong>，第二个参数（函数$w$）叫做核函数。输出有时被称作<em>*特征映射</em></em>。</p>\n<h3 id=\"动机\"><a href=\"#动机\" class=\"headerlink\" title=\"动机\"></a>动机</h3><p>卷积运算通过三个重要的思想来帮助改进机器学习系统：<strong>稀疏交互</strong>、<strong>参数共享</strong>、<strong>等变表示</strong>。<br>参数共享是指一个模型的多个函数中使用相同的参数。</p>\n<h3 id=\"池化\"><a href=\"#池化\" class=\"headerlink\" title=\"池化\"></a>池化</h3><p>卷积网络中一个典型层包含三级。</p>\n<ol>\n<li>在第一级中，这一层并行地计算多个卷积产生一组线性激活响应。</li>\n<li>在第二级中，每一个线性激活响应将会通过一个非线性的激活函数 ，例如 整流线性激活函数。这一级有时也被称为探测级。</li>\n<li><p>在第三级中，我们使用<strong>池化函数</strong>来进一步调整这一层的输出。</p>\n<p>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。<br>例如，<strong>最大池化</strong>函数给出相邻矩形区域内的最大值。</p>\n</li>\n</ol>\n<h2 id=\"序列建模：循环和递归网络\"><a href=\"#序列建模：循环和递归网络\" class=\"headerlink\" title=\"序列建模：循环和递归网络\"></a>序列建模：循环和递归网络</h2><p><strong>循环神经网络</strong>或<strong>RNN</strong>是一类用于处理序列数据的神经网络。</p>\n<h3 id=\"深度循环网络\"><a href=\"#深度循环网络\" class=\"headerlink\" title=\"深度循环网络\"></a>深度循环网络</h3><p>大多数RNN中的计算可以分解成三块参数及其相关的变换：</p>\n<ol>\n<li>从输入到隐藏状态</li>\n<li>从前一隐藏状态到下一隐藏状态，以及</li>\n<li><p>从隐藏状态到输出</p>\n<h3 id=\"递归神经网络\"><a href=\"#递归神经网络\" class=\"headerlink\" title=\"递归神经网络\"></a>递归神经网络</h3><p>递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不是RNN的链状结构，因此是不同类型的计算图。</p>\n</li>\n</ol>\n<h2 id=\"应用\"><a href=\"#应用\" class=\"headerlink\" title=\"应用\"></a>应用</h2><p>级联</p>\n<h3 id=\"计算机视觉\"><a href=\"#计算机视觉\" class=\"headerlink\" title=\"计算机视觉\"></a>计算机视觉</h3><h4 id=\"预处理\"><a href=\"#预处理\" class=\"headerlink\" title=\"预处理\"></a>预处理</h4><h5 id=\"对比度归一化\"><a href=\"#对比度归一化\" class=\"headerlink\" title=\"对比度归一化\"></a>对比度归一化</h5><p><strong>全局对比度归一化</strong>$（GCN)$旨在通过从每个图像中减去其平均值，然后重新缩放使得其像素上的标准差等于某个常数$s$来防止图像具有变化的对比度。</p>\n<p><strong>白化</strong>：与直觉相反的是，存在被称为$sphering$的预处理操作，并且它不同于$GCN$。<br>$sphering$并不会使数据位于球形壳上，而是将主成分重新缩放以具有相等方差。$sphering$通常被称为<strong>白化</strong>。</p>\n<p> <strong>局部对比归一化</strong>通常可以通过使用可分离卷积来计算特征映射的局部平均值和局部标准差，然后在不同的特征映射上使用逐元素的减法和除法。</p>\n<p> <strong>数据集增强</strong></p>\n<h4 id=\"语音识别\"><a href=\"#语音识别\" class=\"headerlink\" title=\"语音识别\"></a>语音识别</h4><p> 语音识别任务在于将一段包括了自然语言发音的声学信号投影到对应说话人的词序列上。</p>\n<p> 之前，最先进的语音识别系统是<strong>隐马尔可夫模型</strong>（HMM)和<strong>高斯混合模型</strong>(GMM)的结合。GMM对声学特征和音素之间的关系建模，HMM对音素序列建模。<br> 之后，随着<em>更大更深</em>的模型以及更大的数据集的出现，通过使用神经网络代替GMM来实现将声学特征转化为音素的过程可以大大提高识别的精度。</p>\n<h3 id=\"自然语言处理\"><a href=\"#自然语言处理\" class=\"headerlink\" title=\"自然语言处理\"></a>自然语言处理</h3><h4 id=\"n-gram\"><a href=\"#n-gram\" class=\"headerlink\" title=\"$n-gram$\"></a>$n-gram$</h4><p><strong>语言模型</strong>定义了自然语言中标记序列的概率分布。 </p>\n<h4 id=\"神经语言模型\"><a href=\"#神经语言模型\" class=\"headerlink\" title=\"神经语言模型\"></a>神经语言模型</h4><p><strong>神经语言模型</strong>是一类用来克服维数灾难的语言模型，它使用词的分布式表示对自然语言序列建模。</p>\n<h3 id=\"其他应用\"><a href=\"#其他应用\" class=\"headerlink\" title=\"其他应用\"></a>其他应用</h3><h4 id=\"推荐系统\"><a href=\"#推荐系统\" class=\"headerlink\" title=\"推荐系统\"></a>推荐系统</h4><p>协同过滤</p>\n<p>强化学习需要权衡<strong>探索</strong>与<strong>利用</strong>。<br><strong>利用</strong>指的是从目前学到的最好策略采取动作，也就是我们所知的将获得高奖励的动作。<br><strong>探索</strong>是指采取行动以获得更多的训练数据。</p>\n<h1 id=\"第三部分-深度学习研究\"><a href=\"#第三部分-深度学习研究\" class=\"headerlink\" title=\"第三部分  深度学习研究\"></a>第三部分  深度学习研究</h1><p><strong>线性因子模型</strong><br><strong>独立成分分析</strong></p>\n<h3 id=\"慢特征分析\"><a href=\"#慢特征分析\" class=\"headerlink\" title=\"慢特征分析\"></a>慢特征分析</h3><p><strong>慢特征分析</strong>是使用来自时间信号的信息信号不变特征的线性因子模型。<br>慢特征分析的想法源于所谓的<strong>慢性原则</strong>。其基本思想是，与场景中其描述作用的单个量度相比，场景的重要特性通常变化地非常缓慢。</p>\n<h2 id=\"蒙特卡罗方法\"><a href=\"#蒙特卡罗方法\" class=\"headerlink\" title=\"蒙特卡罗方法\"></a>蒙特卡罗方法</h2><p>随机算法可以粗略地分为两类：Las Vegas算法和蒙特卡罗算法。</p>\n<h3 id=\"采样与蒙特卡罗方法\"><a href=\"#采样与蒙特卡罗方法\" class=\"headerlink\" title=\"采样与蒙特卡罗方法\"></a>采样与蒙特卡罗方法</h3><p><strong>重要采样</strong></p>\n<ul>\n<li>最优重要采样</li>\n<li><p>有偏重要采样</p>\n<h3 id=\"马尔可夫链蒙特卡罗方法\"><a href=\"#马尔可夫链蒙特卡罗方法\" class=\"headerlink\" title=\"马尔可夫链蒙特卡罗方法\"></a>马尔可夫链蒙特卡罗方法</h3><h3 id=\"Gibbs采样\"><a href=\"#Gibbs采样\" class=\"headerlink\" title=\"Gibbs采样\"></a>Gibbs采样</h3></li>\n</ul>\n<h2 id=\"深度生成模型\"><a href=\"#深度生成模型\" class=\"headerlink\" title=\"深度生成模型\"></a>深度生成模型</h2><h3 id=\"玻尔兹曼机\"><a href=\"#玻尔兹曼机\" class=\"headerlink\" title=\"玻尔兹曼机\"></a>玻尔兹曼机</h3><p>玻尔兹曼机的学习算法基于最大似然。</p>\n<h3 id=\"深度信念网络\"><a href=\"#深度信念网络\" class=\"headerlink\" title=\"深度信念网络\"></a>深度信念网络</h3><p>深度信念网络是第一批成功应用深度架构训练的非卷积模型之一。</p>\n<p>主要有两种方法可以处理深度玻尔兹曼机的联合训练问题。<br>第一个是<strong>中心化深度玻尔兹曼机</strong>。<br>第二个是使用多预测深度<strong>玻尔兹曼机</strong></p>\n","text":"第二部分 深度网络：现代实践深度前馈网络深度前馈网络，也叫做前馈神经网络或者多层感知机，是典型的深度学习模型。 这种模型被称为前向的，是因为信息流过$x$的函数，流经用于定义$f$的中间计算过程，最终到达输出$y$。在模型的输出和模型本身之间没有反馈连接。当前馈神经网络被扩展成包...","link":"","photos":[],"count_time":{"symbolsCount":"2.4k","symbolsTime":"2 mins."},"categories":[],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86-%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%EF%BC%9A%E7%8E%B0%E4%BB%A3%E5%AE%9E%E8%B7%B5\"><span class=\"toc-text\">第二部分 深度网络：现代实践</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">深度前馈网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">基于梯度的学习</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96\"><span class=\"toc-text\">深度学习中的正则化</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8F%82%E6%95%B0%E8%8C%83%E6%95%B0%E6%83%A9%E7%BD%9A\"><span class=\"toc-text\">参数范数惩罚</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A2%9E%E5%BC%BA\"><span class=\"toc-text\">数据集增强</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">深度模型中的优化</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%89%B9%E9%87%8F%E7%AE%97%E6%B3%95%E5%92%8C%E5%B0%8F%E6%89%B9%E9%87%8F%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">批量算法和小批量算法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">基本算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D\"><span class=\"toc-text\">随机梯度下降</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">卷积网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97\"><span class=\"toc-text\">卷积运算</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8A%A8%E6%9C%BA\"><span class=\"toc-text\">动机</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B1%A0%E5%8C%96\"><span class=\"toc-text\">池化</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%92%8C%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">序列建模：循环和递归网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">深度循环网络</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">递归神经网络</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%BA%94%E7%94%A8\"><span class=\"toc-text\">应用</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89\"><span class=\"toc-text\">计算机视觉</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E9%A2%84%E5%A4%84%E7%90%86\"><span class=\"toc-text\">预处理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-5\"><a class=\"toc-link\" href=\"#%E5%AF%B9%E6%AF%94%E5%BA%A6%E5%BD%92%E4%B8%80%E5%8C%96\"><span class=\"toc-text\">对比度归一化</span></a></li></ol></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB\"><span class=\"toc-text\">语音识别</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86\"><span class=\"toc-text\">自然语言处理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#n-gram\"><span class=\"toc-text\">$n-gram$</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%A5%9E%E7%BB%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">神经语言模型</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%85%B6%E4%BB%96%E5%BA%94%E7%94%A8\"><span class=\"toc-text\">其他应用</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F\"><span class=\"toc-text\">推荐系统</span></a></li></ol></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A0%94%E7%A9%B6\"><span class=\"toc-text\">第三部分  深度学习研究</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%85%A2%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90\"><span class=\"toc-text\">慢特征分析</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">蒙特卡罗方法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%87%87%E6%A0%B7%E4%B8%8E%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">采样与蒙特卡罗方法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95\"><span class=\"toc-text\">马尔可夫链蒙特卡罗方法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Gibbs%E9%87%87%E6%A0%B7\"><span class=\"toc-text\">Gibbs采样</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">深度生成模型</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA\"><span class=\"toc-text\">玻尔兹曼机</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">深度信念网络</span></a></li></ol></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"【电子羊的奇妙冒险】初试深度学习（1）","uid":"7cd45e73325ddad87a2769240b2106a6","slug":"电子羊1","date":"2022-11-03T13:58:49.000Z","updated":"2022-11-03T15:57:53.959Z","comments":true,"path":"api/articles/电子羊1.json","keywords":null,"cover":[],"text":"最近忙于浩繁的学习任务，深感知识体系庞大，而面向百度学习又免不了亦步亦趋的情况，于是特开一个板块用于记录学习历程，也顺便作为笔记，适时阶段性总结。 环境配置配置清单 硬件：联想拯救者（GTX 1050ti） 系统：Ubuntu 20.04 64位 首先，因为《优雅的使用Matla...","link":"","photos":[],"count_time":{"symbolsCount":"17k","symbolsTime":"15 mins."},"categories":[],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"《深度学习》 笔记（一）","uid":"6e741307528ed1e909d0d5dbdbcaa4c0","slug":"深度学习1","date":"2022-11-03T13:54:49.000Z","updated":"2022-11-03T13:56:10.735Z","comments":true,"path":"api/articles/深度学习1.json","keywords":null,"cover":null,"text":"深度学习的另一个最大大成就是其在强化学习领域的扩展。在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来完成任务。 第一部分 应用数学与机器学习基础线性代数范数在机器学习中，我们经常使用被称为范数的函数衡量向量大小。p=2时，$L^2$范数被称为欧几里得范数...","link":"","photos":[],"count_time":{"symbolsCount":"4.4k","symbolsTime":"4 mins."},"categories":[],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}