{"title":"【大数据基础】2020年美国新冠肺炎疫情数据分析","uid":"a66db97004d2cce682fbaf0db4f8334b","slug":"大数据7","date":"2023-03-27T13:50:49.000Z","updated":"2023-04-04T02:39:09.538Z","comments":true,"path":"api/articles/大数据7.json","keywords":null,"cover":[],"content":"<p><a href=\"https://dblab.xmu.edu.cn/blog/2738\">https://dblab.xmu.edu.cn/blog/2738</a></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p><a href=\"https://dblab.xmu.edu.cn/blog/2636/\">https://dblab.xmu.edu.cn/blog/2636/</a></p></blockquote>\n<h2 id=\"spark-安装\"><a href=\"#spark-安装\" class=\"headerlink\" title=\"spark 安装\"></a>spark 安装</h2><h3 id=\"安装-Spark2-4-0\"><a href=\"#安装-Spark2-4-0\" class=\"headerlink\" title=\"安装 Spark2.4.0\"></a>安装 Spark2.4.0</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">sudo tar -zxf ~&#x2F;下载&#x2F;spark-2.4.0-bin-without-hadoop.tgz -C &#x2F;usr&#x2F;local&#x2F;\ncd &#x2F;usr&#x2F;local\nsudo mv .&#x2F;spark-2.4.0-bin-without-hadoop&#x2F; .&#x2F;spark\nsudo chown -R hadoop:hadoop .&#x2F;spark          # 此处的 hadoop 为你的用户名</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/fe986b7a5e90490a8123a18401438a13.png\" alt=\"在这里插入图片描述\"><br>安装后，还需要修改Spark的配置文件spark-env.sh</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;usr&#x2F;local&#x2F;spark\ncp .&#x2F;conf&#x2F;spark-env.sh.template .&#x2F;conf&#x2F;spark-env.sh</code></pre>\n<p>编辑spark-env.sh文件(vim ./conf/spark-env.sh)，在第一行添加以下配置信息:</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">export SPARK_DIST_CLASSPATH&#x3D;$(&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;bin&#x2F;hadoop classpath)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/e4f83e2b56c64f3d9dfbfc2031f9da30.png\" alt=\"在这里插入图片描述\"><br>配置完成后就可以直接使用，不需要像Hadoop运行启动命令。<br>通过运行Spark自带的示例，验证Spark是否安装成功。</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;usr&#x2F;local&#x2F;spark\nbin&#x2F;run-example SparkPi</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/35455374471f49fda00d604b711b4614.png\" alt=\"在这里插入图片描述\"><br>执行时会输出非常多的运行信息，输出结果不容易找到，可以通过 grep 命令进行过滤（命令中的 2&gt;&amp;1 可以将所有的信息都输出到 stdout 中，否则由于输出日志的性质，还是会输出到屏幕中）:<br><img src=\"https://img-blog.csdnimg.cn/a84dd55bee0648e6b4580fe78c34070f.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-c\" data-language=\"c\"><code class=\"language-c\">hadoop@algernon-virtual-machine:&#x2F;usr&#x2F;local&#x2F;spark$ bin&#x2F;run-example SparkPi\n2023-03-26 23:34:21 WARN  Utils:66 - Your hostname, algernon-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.46.140 instead (on interface ens33)\n2023-03-26 23:34:21 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n2023-03-26 23:34:23 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2023-03-26 23:34:26 INFO  SparkContext:54 - Running Spark version 2.4.0\n2023-03-26 23:34:26 INFO  SparkContext:54 - Submitted application: Spark Pi\n2023-03-26 23:34:26 INFO  SecurityManager:54 - Changing view acls to: hadoop\n2023-03-26 23:34:26 INFO  SecurityManager:54 - Changing modify acls to: hadoop\n2023-03-26 23:34:26 INFO  SecurityManager:54 - Changing view acls groups to: \n2023-03-26 23:34:26 INFO  SecurityManager:54 - Changing modify acls groups to: \n2023-03-26 23:34:26 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()\n2023-03-26 23:34:26 INFO  Utils:54 - Successfully started service &#39;sparkDriver&#39; on port 33901.\n2023-03-26 23:34:26 INFO  SparkEnv:54 - Registering MapOutputTracker\n2023-03-26 23:34:26 INFO  SparkEnv:54 - Registering BlockManagerMaster\n2023-03-26 23:34:26 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n2023-03-26 23:34:26 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n2023-03-26 23:34:26 INFO  DiskBlockManager:54 - Created local directory at &#x2F;tmp&#x2F;blockmgr-0ad19652-f78b-4bfc-9b4a-c3597666be10\n2023-03-26 23:34:26 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n2023-03-26 23:34:26 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n2023-03-26 23:34:26 INFO  log:192 - Logging initialized @27558ms\n2023-03-26 23:34:26 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-06T01:11:56+08:00, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\n2023-03-26 23:34:27 INFO  Server:419 - Started @27702ms\n2023-03-26 23:34:27 INFO  AbstractConnector:278 - Started ServerConnector@7ba63fe5&#123;HTTP&#x2F;1.1,[http&#x2F;1.1]&#125;&#123;0.0.0.0:4040&#125;\n2023-03-26 23:34:27 INFO  Utils:54 - Successfully started service &#39;SparkUI&#39; on port 4040.\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2a3a299&#123;&#x2F;jobs,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@64d43929&#123;&#x2F;jobs&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1d269ed7&#123;&#x2F;jobs&#x2F;job,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41c89d2f&#123;&#x2F;jobs&#x2F;job&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@410e94e&#123;&#x2F;stages,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2d691f3d&#123;&#x2F;stages&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1bdbf9be&#123;&#x2F;stages&#x2F;stage,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@78d39a69&#123;&#x2F;stages&#x2F;stage&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3c818ac4&#123;&#x2F;stages&#x2F;pool,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5b69d40d&#123;&#x2F;stages&#x2F;pool&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71154f21&#123;&#x2F;storage,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@15f193b8&#123;&#x2F;storage&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2516fc68&#123;&#x2F;storage&#x2F;rdd,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@304a9d7b&#123;&#x2F;storage&#x2F;rdd&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6bfdb014&#123;&#x2F;environment,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@72889280&#123;&#x2F;environment&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@606fc505&#123;&#x2F;executors,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4aa3d36&#123;&#x2F;executors&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2d140a7&#123;&#x2F;executors&#x2F;threadDump,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@347bdeef&#123;&#x2F;executors&#x2F;threadDump&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2aa27288&#123;&#x2F;static,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2c30b71f&#123;&#x2F;,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1d81e101&#123;&#x2F;api,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30cdae70&#123;&#x2F;jobs&#x2F;job&#x2F;kill,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1654a892&#123;&#x2F;stages&#x2F;stage&#x2F;kill,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http:&#x2F;&#x2F;192.168.46.140:4040\n2023-03-26 23:34:27 INFO  SparkContext:54 - Added JAR file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;examples&#x2F;jars&#x2F;scopt_2.11-3.7.0.jar at spark:&#x2F;&#x2F;192.168.46.140:33901&#x2F;jars&#x2F;scopt_2.11-3.7.0.jar with timestamp 1679844867099\n2023-03-26 23:34:27 INFO  SparkContext:54 - Added JAR file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.0.jar at spark:&#x2F;&#x2F;192.168.46.140:33901&#x2F;jars&#x2F;spark-examples_2.11-2.4.0.jar with timestamp 1679844867099\n2023-03-26 23:34:27 INFO  Executor:54 - Starting executor ID driver on host localhost\n2023-03-26 23:34:27 INFO  Utils:54 - Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 40833.\n2023-03-26 23:34:27 INFO  NettyBlockTransferService:54 - Server created on 192.168.46.140:40833\n2023-03-26 23:34:27 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n2023-03-26 23:34:27 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.46.140, 40833, None)\n2023-03-26 23:34:27 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.46.140:40833 with 366.3 MB RAM, BlockManagerId(driver, 192.168.46.140, 40833, None)\n2023-03-26 23:34:27 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.46.140, 40833, None)\n2023-03-26 23:34:27 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.46.140, 40833, None)\n2023-03-26 23:34:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34dc85a&#123;&#x2F;metrics&#x2F;json,null,AVAILABLE,@Spark&#125;\n2023-03-26 23:34:27 INFO  SparkContext:54 - Starting job: reduce at SparkPi.scala:38\n2023-03-26 23:34:27 INFO  DAGScheduler:54 - Got job 0 (reduce at SparkPi.scala:38) with 2 output partitions\n2023-03-26 23:34:27 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n2023-03-26 23:34:27 INFO  DAGScheduler:54 - Parents of final stage: List()\n2023-03-26 23:34:27 INFO  DAGScheduler:54 - Missing parents: List()\n2023-03-26 23:34:27 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n2023-03-26 23:34:28 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 1936.0 B, free 366.3 MB)\n2023-03-26 23:34:28 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1256.0 B, free 366.3 MB)\n2023-03-26 23:34:28 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 192.168.46.140:40833 (size: 1256.0 B, free: 366.3 MB)\n2023-03-26 23:34:28 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1161\n2023-03-26 23:34:28 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1))\n2023-03-26 23:34:28 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks\n2023-03-26 23:34:28 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7866 bytes)\n2023-03-26 23:34:28 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7866 bytes)\n2023-03-26 23:34:28 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)\n2023-03-26 23:34:28 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)\n2023-03-26 23:34:28 INFO  Executor:54 - Fetching spark:&#x2F;&#x2F;192.168.46.140:33901&#x2F;jars&#x2F;scopt_2.11-3.7.0.jar with timestamp 1679844867099\n2023-03-26 23:34:28 INFO  TransportClientFactory:267 - Successfully created connection to &#x2F;192.168.46.140:33901 after 28 ms (0 ms spent in bootstraps)\n2023-03-26 23:34:28 INFO  Utils:54 - Fetching spark:&#x2F;&#x2F;192.168.46.140:33901&#x2F;jars&#x2F;scopt_2.11-3.7.0.jar to &#x2F;tmp&#x2F;spark-b78a374f-111d-4ff3-81ec-6e806aa62da3&#x2F;userFiles-d3452412-f3fd-4537-b848-a839e587e22d&#x2F;fetchFileTemp8254869077053879079.tmp\n2023-03-26 23:34:28 INFO  Executor:54 - Adding file:&#x2F;tmp&#x2F;spark-b78a374f-111d-4ff3-81ec-6e806aa62da3&#x2F;userFiles-d3452412-f3fd-4537-b848-a839e587e22d&#x2F;scopt_2.11-3.7.0.jar to class loader\n2023-03-26 23:34:28 INFO  Executor:54 - Fetching spark:&#x2F;&#x2F;192.168.46.140:33901&#x2F;jars&#x2F;spark-examples_2.11-2.4.0.jar with timestamp 1679844867099\n2023-03-26 23:34:28 INFO  Utils:54 - Fetching spark:&#x2F;&#x2F;192.168.46.140:33901&#x2F;jars&#x2F;spark-examples_2.11-2.4.0.jar to &#x2F;tmp&#x2F;spark-b78a374f-111d-4ff3-81ec-6e806aa62da3&#x2F;userFiles-d3452412-f3fd-4537-b848-a839e587e22d&#x2F;fetchFileTemp8998672681604896365.tmp\n2023-03-26 23:34:28 INFO  Executor:54 - Adding file:&#x2F;tmp&#x2F;spark-b78a374f-111d-4ff3-81ec-6e806aa62da3&#x2F;userFiles-d3452412-f3fd-4537-b848-a839e587e22d&#x2F;spark-examples_2.11-2.4.0.jar to class loader\n2023-03-26 23:34:28 INFO  Executor:54 - Finished task 1.0 in stage 0.0 (TID 1). 867 bytes result sent to driver\n2023-03-26 23:34:28 INFO  Executor:54 - Finished task 0.0 in stage 0.0 (TID 0). 867 bytes result sent to driver\n2023-03-26 23:34:28 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 404 ms on localhost (executor driver) (1&#x2F;2)\n2023-03-26 23:34:28 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 389 ms on localhost (executor driver) (2&#x2F;2)\n2023-03-26 23:34:28 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \n2023-03-26 23:34:28 INFO  DAGScheduler:54 - ResultStage 0 (reduce at SparkPi.scala:38) finished in 0.666 s\n2023-03-26 23:34:28 INFO  DAGScheduler:54 - Job 0 finished: reduce at SparkPi.scala:38, took 0.747248 s\nPi is roughly 3.1462957314786575\n2023-03-26 23:34:28 INFO  AbstractConnector:318 - Stopped Spark@7ba63fe5&#123;HTTP&#x2F;1.1,[http&#x2F;1.1]&#125;&#123;0.0.0.0:4040&#125;\n2023-03-26 23:34:28 INFO  SparkUI:54 - Stopped Spark web UI at http:&#x2F;&#x2F;192.168.46.140:4040\n2023-03-26 23:34:28 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\n2023-03-26 23:34:28 INFO  MemoryStore:54 - MemoryStore cleared\n2023-03-26 23:34:28 INFO  BlockManager:54 - BlockManager stopped\n2023-03-26 23:34:28 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\n2023-03-26 23:34:28 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\n2023-03-26 23:34:28 INFO  SparkContext:54 - Successfully stopped SparkContext\n2023-03-26 23:34:28 INFO  ShutdownHookManager:54 - Shutdown hook called\n2023-03-26 23:34:28 INFO  ShutdownHookManager:54 - Deleting directory &#x2F;tmp&#x2F;spark-7dd0b4a9-d5ee-4945-9fe9-3c330363584d\n2023-03-26 23:34:28 INFO  ShutdownHookManager:54 - Deleting directory &#x2F;tmp&#x2F;spark-b78a374f-111d-4ff3-81ec-6e806aa62da3\nhadoop@algernon-virtual-machine:&#x2F;usr&#x2F;local&#x2F;spark$ \n</code></pre>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;usr&#x2F;local&#x2F;spark\nbin&#x2F;run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is&quot;</code></pre>\n<p>过滤后的运行结果如下图示，可以得到π 的 5 位小数近似值：<br><img src=\"https://img-blog.csdnimg.cn/de64f5f02c80443dae88e2f70dbfeb06.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"启动Spark-Shell\"><a href=\"#启动Spark-Shell\" class=\"headerlink\" title=\"启动Spark Shell\"></a>启动Spark Shell</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;usr&#x2F;local&#x2F;spark\nbin&#x2F;spark-shell</code></pre>\n<p>启动spark-shell后，会自动创建名为sc的SparkContext对象和名为spark的SparkSession对象,如图：<br><img src=\"https://img-blog.csdnimg.cn/ae4df84731514125bc31303def14d0c7.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"加载text文件\"><a href=\"#加载text文件\" class=\"headerlink\" title=\"加载text文件\"></a>加载text文件</h3><p>spark创建sc，可以加载本地文件和HDFS文件创建RDD。这里用Spark自带的本地文件README.md文件测试。</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">val textFile &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;README.md&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/1c1d3cc1789b4834bf912218754d5024.png\" alt=\"在这里插入图片描述\"><br>加载HDFS文件和本地文件都是使用textFile，区别是添加前缀(hdfs://和file:///)进行标识。</p>\n<h3 id=\"简单RDD操作\"><a href=\"#简单RDD操作\" class=\"headerlink\" title=\"简单RDD操作\"></a>简单RDD操作</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;&#x2F;获取RDD文件textFile的第一行内容\ntextFile.first()\n&#x2F;&#x2F;获取RDD文件textFile所有项的计数\ntextFile.count()\n&#x2F;&#x2F;抽取含有“Spark”的行，返回一个新的RDD\nval lineWithSpark &#x3D; textFile.filter(line &#x3D;&gt; line.contains(&quot;Spark&quot;))\n&#x2F;&#x2F;统计新的RDD的行数\nlineWithSpark.count()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/0b214939181943159cf185ced2d3f4b6.png\" alt=\"在这里插入图片描述\"><br>可以通过组合RDD操作进行组合，可以实现简易MapReduce操作</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;&#x2F;找出文本中每行的最多单词数\ntextFile.map(line &#x3D;&gt; line.split(&quot; &quot;).size).reduce((a, b) &#x3D;&gt; if (a &gt; b) a else b)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/2f36f30cfd3640be81e77624aa85b6ab.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"退出Spark-Shell\"><a href=\"#退出Spark-Shell\" class=\"headerlink\" title=\"退出Spark Shell\"></a>退出Spark Shell</h3><p> 输入exit，即可退出spark shell</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">:quit</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/af8f5401baaf44ceaae546ecf40d2b27.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"独立应用程序编程\"><a href=\"#独立应用程序编程\" class=\"headerlink\" title=\"独立应用程序编程\"></a>独立应用程序编程</h3><p> 安装sbt</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">sudo mkdir &#x2F;usr&#x2F;local&#x2F;sbt             　　　# 创建安装目录\ncd ~&#x2F;Downloads \nsudo tar -zxvf .&#x2F;sbt-1.3.8.tgz -C &#x2F;usr&#x2F;local \ncd &#x2F;usr&#x2F;local&#x2F;sbt\nsudo chown -R hadoop &#x2F;usr&#x2F;local&#x2F;sbt  　　 # 此处的hadoop为系统当前用户名\ncp .&#x2F;bin&#x2F;sbt-launch.jar .&#x2F;  #把bin目录下的sbt-launch.jar复制到sbt安装目录下</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/8b2d43710f5f48aebda016cce8093775.png\" alt=\"在这里插入图片描述\"><br>接着在安装目录中使用下面命令创建一个Shell脚本文件，用于启动sbt：<br><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">vim &#x2F;usr&#x2F;local&#x2F;sbt&#x2F;sbt</code></pre><br>该脚本文件中的代码如下：<br><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">#!&#x2F;bin&#x2F;bash\nSBT_OPTS&#x3D;&quot;-Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize&#x3D;256M&quot;\njava $SBT_OPTS -jar &#96;dirname $0&#96;&#x2F;sbt-launch.jar &quot;$@&quot;</code></pre></p>\n<p><img src=\"https://img-blog.csdnimg.cn/ad9f6dc7f7eb43b8bbf7f37afe265ef6.png\" alt=\"在这里插入图片描述\"><br>保存后，还需要为该Shell脚本文件增加可执行权限：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">chmod u+x &#x2F;usr&#x2F;local&#x2F;sbt&#x2F;sbt</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/8b2aa26c09ac4facbe83daa660fca5e3.png\" alt=\"在这里插入图片描述\"><br>然后，可以使用如下命令查看sbt版本信息：<br><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;usr&#x2F;local&#x2F;sbt\n.&#x2F;sbt sbtVersion\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize&#x3D;256M; support was removed in 8.0\n[warn] No sbt.version set in project&#x2F;build.properties, base directory: &#x2F;usr&#x2F;local&#x2F;sbt\n[info] Set current project to sbt (in build file:&#x2F;usr&#x2F;local&#x2F;sbt&#x2F;)\n[info] 1.3.8</code></pre></p>\n<p><img src=\"https://img-blog.csdnimg.cn/f6de464e685f4b48be15eed53a219e85.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/27c2104ed01a4d95a777b2c4d2a11455.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"Scala应用程序代码\"><a href=\"#Scala应用程序代码\" class=\"headerlink\" title=\"Scala应用程序代码\"></a>Scala应用程序代码</h3><p>在终端中执行如下命令创建一个文件夹 sparkapp 作为应用程序根目录：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd ~           # 进入用户主文件夹\nmkdir .&#x2F;sparkapp        # 创建应用程序根目录\nmkdir -p .&#x2F;sparkapp&#x2F;src&#x2F;main&#x2F;scala     # 创建所需的文件夹结构</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/a7282e44319a483faa9fbc919dc8eb92.png\" alt=\"在这里插入图片描述\"><br>在 ./sparkapp/src/main/scala 下建立一个名为 SimpleApp.scala 的文件（vim ./sparkapp/src/main/scala/SimpleApp.scala），添加代码如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;* SimpleApp.scala *&#x2F;\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n \nobject SimpleApp &#123;\n        def main(args: Array[String]) &#123;\n            val logFile &#x3D; &quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;README.md&quot; &#x2F;&#x2F; Should be some file on your system\n            val conf &#x3D; new SparkConf().setAppName(&quot;Simple Application&quot;)\n            val sc &#x3D; new SparkContext(conf)\n            val logData &#x3D; sc.textFile(logFile, 2).cache()\n            val numAs &#x3D; logData.filter(line &#x3D;&gt; line.contains(&quot;a&quot;)).count()\n            val numBs &#x3D; logData.filter(line &#x3D;&gt; line.contains(&quot;b&quot;)).count()\n            println(&quot;Lines with a: %s, Lines with b: %s&quot;.format(numAs, numBs))\n        &#125;\n    &#125;</code></pre>\n<p>该程序计算 /usr/local/spark/README 文件中包含 “a” 的行数 和包含 “b” 的行数。代码第8行的 /usr/local/spark 为 Spark 的安装目录，如果不是该目录请自行修改。不同于 Spark shell，独立应用程序需要通过 val sc = new SparkContext(conf) 初始化 SparkContext，SparkContext 的参数 SparkConf 包含了应用程序的信息。</p>\n<p>该程序依赖 Spark API，因此我们需要通过 sbt 进行编译打包。 在~/sparkapp这个目录中新建文件simple.sbt，命令如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd ~&#x2F;sparkapp\nvim simple.sbt</code></pre>\n<p>在simple.sbt中添加如下内容，声明该独立应用程序的信息以及与 Spark 的依赖关系：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">name :&#x3D; &quot;Simple Project&quot;\nversion :&#x3D; &quot;1.0&quot;\nscalaVersion :&#x3D; &quot;2.11.12&quot;\nlibraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.4.0&quot;</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/4b8a2cff4c4a4874a461b90942285cf4.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"使用-sbt-打包-Scala-程序\"><a href=\"#使用-sbt-打包-Scala-程序\" class=\"headerlink\" title=\"使用 sbt 打包 Scala 程序\"></a>使用 sbt 打包 Scala 程序</h3><p>为保证 sbt 能正常运行，先执行如下命令检查整个应用程序的文件结构：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd ~&#x2F;sparkapp\nfind .</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/25b5f4f324e24086889e6132c6984df1.png\" alt=\"在这里插入图片描述\"><br>接着，我们就可以通过如下代码将整个应用程序打包成 JAR（首次运行同样需要下载依赖包 ）：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;usr&#x2F;local&#x2F;sbt&#x2F;sbt package</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/5d3b9e52b191422d8f89607863367a9a.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/6eff72749ac24c608d514d91a02167e5.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/9481521cffd743f5a24472c2a2bf974d.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"通过-spark-submit-运行程序\"><a href=\"#通过-spark-submit-运行程序\" class=\"headerlink\" title=\"通过 spark-submit 运行程序\"></a>通过 spark-submit 运行程序</h3><p>最后，我们就可以将生成的 jar 包通过 spark-submit 提交到 Spark 中运行了，命令如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;usr&#x2F;local&#x2F;spark&#x2F;bin&#x2F;spark-submit --class &quot;SimpleApp&quot; ~&#x2F;sparkapp&#x2F;target&#x2F;scala-2.11&#x2F;simple-project_2.11-1.0.jar\n# 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果\n&#x2F;usr&#x2F;local&#x2F;spark&#x2F;bin&#x2F;spark-submit --class &quot;SimpleApp&quot; ~&#x2F;sparkapp&#x2F;target&#x2F;scala-2.11&#x2F;simple-project_2.11-1.0.jar 2&gt;&amp;1 | grep &quot;Lines with a:&quot;</code></pre>\n<p>最终结果如下：<br><img src=\"https://img-blog.csdnimg.cn/aef896177cc9477995b7e08838be3943.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"使用Maven对Java独立应用程序进行编译打包\"><a href=\"#使用Maven对Java独立应用程序进行编译打包\" class=\"headerlink\" title=\"使用Maven对Java独立应用程序进行编译打包\"></a>使用Maven对Java独立应用程序进行编译打包</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">sudo unzip ~&#x2F;下载&#x2F;apache-maven-3.6.3-bin.zip -d &#x2F;usr&#x2F;local\ncd &#x2F;usr&#x2F;local\nsudo mv apache-maven-3.6.3&#x2F; .&#x2F;maven\nsudo chown -R hadoop .&#x2F;maven</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/e4762a4532e74390a88c4063757f5326.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd ~ #进入用户主文件夹\nmkdir -p .&#x2F;sparkapp2&#x2F;src&#x2F;main&#x2F;java</code></pre>\n<p>在 ./sparkapp2/src/main/java 下建立一个名为 SimpleApp.java 的文件（vim ./sparkapp2/src/main/java/SimpleApp.java），添加代码如下：<br><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;*** SimpleApp.java ***&#x2F;\nimport org.apache.spark.api.java.*;\nimport org.apache.spark.api.java.function.Function;\nimport org.apache.spark.SparkConf;\n \npublic class SimpleApp &#123;\n    public static void main(String[] args) &#123;\n        String logFile &#x3D; &quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;README.md&quot;; &#x2F;&#x2F; Should be some file on your system\n        SparkConf conf&#x3D;new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;SimpleApp&quot;);\n        JavaSparkContext sc&#x3D;new JavaSparkContext(conf);\n        JavaRDD&lt;String&gt; logData &#x3D; sc.textFile(logFile).cache(); \n        long numAs &#x3D; logData.filter(new Function&lt;String, Boolean&gt;() &#123;\n            public Boolean call(String s) &#123; return s.contains(&quot;a&quot;); &#125;\n        &#125;).count(); \n        long numBs &#x3D; logData.filter(new Function&lt;String, Boolean&gt;() &#123;\n            public Boolean call(String s) &#123; return s.contains(&quot;b&quot;); &#125;\n        &#125;).count(); \n        System.out.println(&quot;Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);\n    &#125;\n&#125;</code></pre></p>\n<p><img src=\"https://img-blog.csdnimg.cn/34f47f742c024d01b89c7f6d5668a2b1.png\" alt=\"在这里插入图片描述\"><br>该程序依赖Spark Java API,因此我们需要通过Maven进行编译打包。在./sparkapp2目录中新建文件pom.xml，命令如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd ~&#x2F;sparkapp2\nvim pom.xml</code></pre>\n<p>在pom.xml文件中添加内容如下，声明该独立应用程序的信息以及与Spark的依赖关系：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&lt;project&gt;\n    &lt;groupId&gt;cn.edu.xmu&lt;&#x2F;groupId&gt;\n    &lt;artifactId&gt;simple-project&lt;&#x2F;artifactId&gt;\n    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;\n    &lt;name&gt;Simple Project&lt;&#x2F;name&gt;\n    &lt;packaging&gt;jar&lt;&#x2F;packaging&gt;\n    &lt;version&gt;1.0&lt;&#x2F;version&gt;\n    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;jboss&lt;&#x2F;id&gt;\n            &lt;name&gt;JBoss Repository&lt;&#x2F;name&gt;\n            &lt;url&gt;http:&#x2F;&#x2F;repository.jboss.com&#x2F;maven2&#x2F;&lt;&#x2F;url&gt;\n        &lt;&#x2F;repository&gt;\n    &lt;&#x2F;repositories&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt; &lt;!-- Spark dependency --&gt;\n            &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;\n            &lt;artifactId&gt;spark-core_2.11&lt;&#x2F;artifactId&gt;\n            &lt;version&gt;2.4.0&lt;&#x2F;version&gt;\n        &lt;&#x2F;dependency&gt;\n    &lt;&#x2F;dependencies&gt;\n&lt;&#x2F;project&gt;  </code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/baab06f305314379bc7b8921583a4e73.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/2413155daac744279e3cf8391b17fc5a.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/8b431ceb53664875b56f614bfd10ba64.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/4c3d1590e9a74cecb7156c65661959ce.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/28acc31682c3464eaef10e09f5c7b692.png\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"jupyter-notebook-安装及使用\"><a href=\"#jupyter-notebook-安装及使用\" class=\"headerlink\" title=\"jupyter notebook 安装及使用\"></a>jupyter notebook 安装及使用</h2><h3 id=\"安装anaconda\"><a href=\"#安装anaconda\" class=\"headerlink\" title=\"安装anaconda\"></a>安装anaconda</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;home&#x2F;hadoop\nbash Anaconda3-2020.02-Linux-x86_64.sh</code></pre>\n<p>安装成功以后，可以看到如下信息。<br><img src=\"https://img-blog.csdnimg.cn/a7246248fbff4671a1df18a403a68ff5.png\" alt=\"在这里插入图片描述\"><br>安装结束后，要关闭当前终端。然后重新打开一个终端，输入命令：conda -V，可以查看版本信息，如下图所示。<br><img src=\"https://img-blog.csdnimg.cn/a4ed42aac5874c17bfbc5e1054472fa9.png\" alt=\"在这里插入图片描述\"></p>\n<p>可以查看Anaconda的版本信息，命令如下：<br><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">anaconda -V</code></pre></p>\n<p><img src=\"https://img-blog.csdnimg.cn/d982c851e2c243dba6f1ecc9f590bda9.png\" alt=\"在这里插入图片描述\"><br>这时，你会发现，在命令提示符的开头多了一个(base)，看着很难受，可以在终端中运行如下命令，消除这个(base)：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">conda config --set auto_activate_base false</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/e7b4189e3c064a2eb6ce418f4bf91779.png\" alt=\"在这里插入图片描述\"><br>然后，关闭终端，再次新建一个终端，可以看到，已经没有（base）了。但是，这时，输入“anaconda -V”命令就会失败，提示找不到命令。</p>\n<p>这时，需要到~/.bashrc文件中修改配置，执行如下命令打开文件：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">vim ~&#x2F;.bashrc</code></pre>\n<p>打开文件以后，按键盘上的i键，进入编辑状态，然后，在PATH环境配置中，把“/home/hadoop/anaconda3/bin”增加到PATH的末尾，也就是用英文冒号和PATH的其他部分连接起来，</p>\n<p>然后保存退出文件（先按Esc键退出文件编辑状态，再输入:wq（注意是英文冒号），再回车，就可以保存退出文件）。再执行如下命令使得配置立即生效：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">source ~&#x2F;.bashrc</code></pre>\n<p>执行完source命令以后，就可以成功执行“anaconda -V”命令了。</p>\n<h3 id=\"配置Jupyter-Notebook\"><a href=\"#配置Jupyter-Notebook\" class=\"headerlink\" title=\"配置Jupyter Notebook\"></a>配置Jupyter Notebook</h3><p>下面开始配置Jupyter Notebook，在终端中执行如下命令：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">jupyter notebook --generate-config</code></pre>\n<p>然后，在终端中执行如下命令：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;bin\n.&#x2F;python</code></pre>\n<p>然后，在Python命令提示符（不是Linux Shell命令提示符）后面输入如下命令：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&gt;&gt;&gt;from notebook.auth import passwd\n&gt;&gt;&gt;passwd()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/d669bcec9d3e49c58519c982bd62aed4.png\" alt=\"在这里插入图片描述\"><br>此时系统会让输入密码，并让你确认密码（如:123456），这个密码是后面进入到Jupyter网页页面的密码。然后系统会生成一个密码字符串，比如sha1:7c7990750e83:965c1466a4fab0849051ca5f3c5661110813795，把这个sha1字符串复制粘贴到一个文件中保存起来，后面用于配置密码。具体如下图所示：<br><pre class=\"line-numbers language-c\" data-language=\"c\"><code class=\"language-c\">&#39;sha1:591c60fedd74:916538f684789e5c0220be599bda59741fedadbe&#39;\n&#39;sha1:8f6545b8d0cf:41ec531eaf0df13e77f2846221a97516020a16f5&#39;</code></pre><br>然后，在Python命令提示符后面输入“exit()”，退出Python。</p>\n<p>下面开始配置文件。<br>在终端输入如下命令：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">vim ~&#x2F;.jupyter&#x2F;jupyter_notebook_config.py</code></pre>\n<p>进入到配置文件页面，在文件的开头增加以下内容：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">c.NotebookApp.ip&#x3D;&#39;*&#39;                     # 就是设置所有ip皆可访问  \nc.NotebookApp.password &#x3D; &#39;sha1:7c7990750e83:965c1466a4fab0849051ca5f3c5661110813795b&#39;     # 上面复制的那个sha密文&#39;  \nc.NotebookApp.open_browser &#x3D; False       # 禁止自动打开浏览器  \nc.NotebookApp.port &#x3D;8888                 # 端口\nc.NotebookApp.notebook_dir &#x3D; &#39;&#x2F;home&#x2F;hadoop&#x2F;jupyternotebook&#39;  #设置Notebook启动进入的目录</code></pre>\n<p>配置文件如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/61282097d08643c8938bc4fd58b4b784.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/ae3fe2024973418899f6564ee6d1a794.png\" alt=\"在这里插入图片描述\"><br>然后保存并退出vim文件（Esc键，输入:wq）<br>需要注意的是，在配置文件中，c.NotebookApp.password的值，就是刚才前面生成以后保存到文件中的sha1密文。另外，c.NotebookApp.notebook_dir = ‘/home/hadoop/jupyternotebook’ 这行用于设置Notebook启动进入的目录，由于该目录还不存在，所以需要在终端中执行如下命令创建：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cd &#x2F;home&#x2F;hadoop\nmkdir jupyternotebook</code></pre>\n<h3 id=\"运行Jupyter-Notebook\"><a href=\"#运行Jupyter-Notebook\" class=\"headerlink\" title=\"运行Jupyter Notebook\"></a>运行Jupyter Notebook</h3><p>下面开始运行Jupyter Notebook。<br>在终端输入如下命令：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">jupyter notebook</code></pre>\n<p>打开浏览器，输入<a href=\"http://localhost:8888\">http://localhost:8888</a><br>会弹出对话框，输入Python密码123456，点击“Log in”，如下图所示。<br><img src=\"https://img-blog.csdnimg.cn/a988257dac3643bea15f983bf750cf68.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/8805f51ef7c34028b366884fa7e15602.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"配置Jupyter-Notebook实现和PySpark交互\"><a href=\"#配置Jupyter-Notebook实现和PySpark交互\" class=\"headerlink\" title=\"配置Jupyter Notebook实现和PySpark交互\"></a>配置Jupyter Notebook实现和PySpark交互</h3><p>在终端中输入如下命令：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">vim ~&#x2F;.bashrc</code></pre>\n<p>然后，在.bashrc文件中把原来已经存在的一行“export PYSPARK_PYTHON=python3”删除，然后，在该文件中增加如下两行：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">export PYSPARK_PYTHON&#x3D;&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;bin&#x2F;python\nexport PYSPARK_DRIVER_PYTHON&#x3D;&#x2F;home&#x2F;hadoop&#x2F;anaconda3&#x2F;bin&#x2F;python</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/51118a8c88ec4c6bab2e5b60a96af0b2.png\" alt=\"在这里插入图片描述\"></p>\n<p>然后，保存退出该文件。然后执行如下命令让配置生效：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">source ~&#x2F;.bashrc</code></pre>\n<p>然后，在Jupyter Notebook首页中，点击“New”，再点击“Python3”，另外新建一个代码文件，把文件保存名称为CountLine，在文件中输入如下内容：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">from pyspark import SparkConf, SparkContext\nconf &#x3D; SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;My App&quot;)\nsc &#x3D; SparkContext(conf &#x3D; conf)\nlogFile &#x3D; &quot;file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;spark&#x2F;README.md&quot;\nlogData &#x3D; sc.textFile(logFile, 2).cache()\nnumAs &#x3D; logData.filter(lambda line: &#39;a&#39; in line).count()\nnumBs &#x3D; logData.filter(lambda line: &#39;b&#39; in line).count()\nprint(&#39;Lines with a: %s, Lines with b: %s&#39; % (numAs, numBs))</code></pre>\n<p>然后，点击界面上的“Run”按钮运行该代码，会出现统计结果“Lines with a: 62, Lines with b: 31”，执行效果如下：<br><img src=\"https://img-blog.csdnimg.cn/c2206a99bdfb4d519aeb2e6f394907b8.png\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"数据分析\"><a href=\"#数据分析\" class=\"headerlink\" title=\"数据分析\"></a>数据分析</h2><h3 id=\"格式转换\"><a href=\"#格式转换\" class=\"headerlink\" title=\"格式转换\"></a>格式转换</h3><p>原始数据集是以.csv文件组织的，为了方便spark读取生成RDD或者DataFrame，首先将us-counties.csv转换为.txt格式文件us-counties.txt。转换操作使用python实现，代码组织在toTxt.py中，具体代码如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">import pandas as pd\n \n#.csv-&gt;.txt\ndata &#x3D; pd.read_csv(&#39;&#x2F;home&#x2F;hadoop&#x2F;us-counties.csv&#39;)\nwith open(&#39;&#x2F;home&#x2F;hadoop&#x2F;us-counties.txt&#39;,&#39;a+&#39;,encoding&#x3D;&#39;utf-8&#39;) as f:\n    for line in data.values:\n        f.write((str(line[0])+&#39;\\t&#39;+str(line[1])+&#39;\\t&#39;\n                +str(line[2])+&#39;\\t&#39;+str(line[3])+&#39;\\t&#39;+str(line[4])+&#39;\\n&#39;))</code></pre>\n<h3 id=\"将文件上传至HDFS文件系统中\"><a href=\"#将文件上传至HDFS文件系统中\" class=\"headerlink\" title=\"将文件上传至HDFS文件系统中\"></a>将文件上传至HDFS文件系统中</h3><p>然后使用如下命令把本地文件系统的“/home/hadoop/us-counties.txt”上传到HDFS文件系统中，具体路径是“/user/hadoop/us-counties.txt”。具体命令如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">.&#x2F;bin&#x2F;hdfs dfs -put &#x2F;home&#x2F;hadoop&#x2F;us-counties.txt &#x2F;user&#x2F;hadoop</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/a997a3c17fad4f6e920b85c84b832900.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"使用Spark对数据进行分析\"><a href=\"#使用Spark对数据进行分析\" class=\"headerlink\" title=\"使用Spark对数据进行分析\"></a>使用Spark对数据进行分析</h3><p>记得先启动hadoop</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">from pyspark import SparkConf,SparkContext\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime\nimport pyspark.sql.functions as func\n \ndef toDate(inputStr):\n    newStr &#x3D; &quot;&quot;\n    if len(inputStr) &#x3D;&#x3D; 8:\n        s1 &#x3D; inputStr[0:4]\n        s2 &#x3D; inputStr[5:6]\n        s3 &#x3D; inputStr[7]\n        newStr &#x3D; s1+&quot;-&quot;+&quot;0&quot;+s2+&quot;-&quot;+&quot;0&quot;+s3\n    else:\n        s1 &#x3D; inputStr[0:4]\n        s2 &#x3D; inputStr[5:6]\n        s3 &#x3D; inputStr[7:]\n        newStr &#x3D; s1+&quot;-&quot;+&quot;0&quot;+s2+&quot;-&quot;+s3\n    date &#x3D; datetime.strptime(newStr, &quot;%Y-%m-%d&quot;)\n    return date\n \n \n \n#主程序:\nspark &#x3D; SparkSession.builder.config(conf &#x3D; SparkConf()).getOrCreate()\n \nfields &#x3D; [StructField(&quot;date&quot;, DateType(),False),StructField(&quot;county&quot;, StringType(),False),StructField(&quot;state&quot;, StringType(),False),\n                    StructField(&quot;cases&quot;, IntegerType(),False),StructField(&quot;deaths&quot;, IntegerType(),False),]\nschema &#x3D; StructType(fields)\n \nrdd0 &#x3D; spark.sparkContext.textFile(&quot;&#x2F;user&#x2F;hadoop&#x2F;us-counties.txt&quot;)\nrdd1 &#x3D; rdd0.map(lambda x:x.split(&quot;\\t&quot;)).map(lambda p: Row(toDate(p[0]),p[1],p[2],int(p[3]),int(p[4])))\n \n \nshemaUsInfo &#x3D; spark.createDataFrame(rdd1,schema)\n \nshemaUsInfo.createOrReplaceTempView(&quot;usInfo&quot;)\n \n#1.计算每日的累计确诊病例数和死亡数\ndf &#x3D; shemaUsInfo.groupBy(&quot;date&quot;).agg(func.sum(&quot;cases&quot;),func.sum(&quot;deaths&quot;)).sort(shemaUsInfo[&quot;date&quot;].asc())\n \n#列重命名\ndf1 &#x3D; df.withColumnRenamed(&quot;sum(cases)&quot;,&quot;cases&quot;).withColumnRenamed(&quot;sum(deaths)&quot;,&quot;deaths&quot;)\ndf1.repartition(1).write.json(&quot;result1.json&quot;)                               #写入hdfs\n \n#注册为临时表供下一步使用\ndf1.createOrReplaceTempView(&quot;ustotal&quot;)\n \n#2.计算每日较昨日的新增确诊病例数和死亡病例数\ndf2 &#x3D; spark.sql(&quot;select t1.date,t1.cases-t2.cases as caseIncrease,t1.deaths-t2.deaths as deathIncrease from ustotal t1,ustotal t2 where t1.date &#x3D; date_add(t2.date,1)&quot;)\n \ndf2.sort(df2[&quot;date&quot;].asc()).repartition(1).write.json(&quot;result2.json&quot;)           #写入hdfs\n \n#3.统计截止5.19日 美国各州的累计确诊人数和死亡人数\ndf3 &#x3D; spark.sql(&quot;select date,state,sum(cases) as totalCases,sum(deaths) as totalDeaths,round(sum(deaths)&#x2F;sum(cases),4) as deathRate from usInfo  where date &#x3D; to_date(&#39;2020-05-19&#39;,&#39;yyyy-MM-dd&#39;) group by date,state&quot;)\n \ndf3.sort(df3[&quot;totalCases&quot;].desc()).repartition(1).write.json(&quot;result3.json&quot;) #写入hdfs\n \ndf3.createOrReplaceTempView(&quot;eachStateInfo&quot;)\n \n#4.找出美国确诊最多的10个州\ndf4 &#x3D; spark.sql(&quot;select date,state,totalCases from eachStateInfo  order by totalCases desc limit 10&quot;)\ndf4.repartition(1).write.json(&quot;result4.json&quot;)\n \n#5.找出美国死亡最多的10个州\ndf5 &#x3D; spark.sql(&quot;select date,state,totalDeaths from eachStateInfo  order by totalDeaths desc limit 10&quot;)\ndf5.repartition(1).write.json(&quot;result5.json&quot;)\n \n#6.找出美国确诊最少的10个州\ndf6 &#x3D; spark.sql(&quot;select date,state,totalCases from eachStateInfo  order by totalCases asc limit 10&quot;)\ndf6.repartition(1).write.json(&quot;result6.json&quot;)\n \n#7.找出美国死亡最少的10个州\ndf7 &#x3D; spark.sql(&quot;select date,state,totalDeaths from eachStateInfo  order by totalDeaths asc limit 10&quot;)\ndf7.repartition(1).write.json(&quot;result7.json&quot;)\n \n#8.统计截止5.19全美和各州的病死率\ndf8 &#x3D; spark.sql(&quot;select 1 as sign,date,&#39;USA&#39; as state,round(sum(totalDeaths)&#x2F;sum(totalCases),4) as deathRate from eachStateInfo group by date union select 2 as sign,date,state,deathRate from eachStateInfo&quot;).cache()\ndf8.sort(df8[&quot;sign&quot;].asc(),df8[&quot;deathRate&quot;].desc()).repartition(1).write.json(&quot;result8.json&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/ef77ac4b399242aaaed8c0db3b6a9a8f.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/6d04ab4d7cad435e8627f6a8560dbbb1.png\" alt=\"在这里插入图片描述\"><br>输出结果：<br><img src=\"https://img-blog.csdnimg.cn/c75975ce422d4badbb4390cf9e7aab3f.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"读取文件生成DataFrame\"><a href=\"#读取文件生成DataFrame\" class=\"headerlink\" title=\"读取文件生成DataFrame\"></a>读取文件生成DataFrame</h3><p>上面已经给出了完整代码。下面我们再对代码做一些简要介绍。首先看看读取文件生成DataFrame。<br>由于本实验中使用的数据为结构化数据，因此可以使用spark读取源文件生成DataFrame以方便进行后续分析实现。<br>本部分代码组织在analyst.py中，读取us-counties.txt生成DataFrame的代码如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">from pyspark import SparkConf,SparkContext\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SparkSession\nfrom datetime import datetime\nimport pyspark.sql.functions as func\n \ndef toDate(inputStr):\n    newStr &#x3D; &quot;&quot;\n    if len(inputStr) &#x3D;&#x3D; 8:\n        s1 &#x3D; inputStr[0:4]\n        s2 &#x3D; inputStr[5:6]\n        s3 &#x3D; inputStr[7]\n        newStr &#x3D; s1+&quot;-&quot;+&quot;0&quot;+s2+&quot;-&quot;+&quot;0&quot;+s3\n    else:\n        s1 &#x3D; inputStr[0:4]\n        s2 &#x3D; inputStr[5:6]\n        s3 &#x3D; inputStr[7:]\n        newStr &#x3D; s1+&quot;-&quot;+&quot;0&quot;+s2+&quot;-&quot;+s3\n    date &#x3D; datetime.strptime(newStr, &quot;%Y-%m-%d&quot;)\n    return date\n \n \n#主程序:\nspark &#x3D; SparkSession.builder.config(conf &#x3D; SparkConf()).getOrCreate()\n \nfields &#x3D; [StructField(&quot;date&quot;, DateType(),False),StructField(&quot;county&quot;, StringType(),False),StructField(&quot;state&quot;, StringType(),False),\n                    StructField(&quot;cases&quot;, IntegerType(),False),StructField(&quot;deaths&quot;, IntegerType(),False),]\nschema &#x3D; StructType(fields)\n \nrdd0 &#x3D; spark.sparkContext.textFile(&quot;&#x2F;user&#x2F;hadoop&#x2F;us-counties.txt&quot;)\nrdd1 &#x3D; rdd0.map(lambda x:x.split(&quot;\\t&quot;)).map(lambda p: Row(toDate(p[0]),p[1],p[2],int(p[3]),int(p[4])))\n \nshemaUsInfo &#x3D; spark.createDataFrame(rdd1,schema)\n \nshemaUsInfo.createOrReplaceTempView(&quot;usInfo&quot;)\n </code></pre>\n<h3 id=\"进行数据分析\"><a href=\"#进行数据分析\" class=\"headerlink\" title=\"进行数据分析\"></a>进行数据分析</h3><p>本实验主要统计以下8个指标，分别是：<br>1) 统计美国截止每日的累计确诊人数和累计死亡人数。做法是以date作为分组字段，对cases和deaths字段进行汇总统计。<br>2) 统计美国每日的新增确诊人数和新增死亡人数。因为新增数=今日数-昨日数，所以考虑使用自连接，连接条件是t1.date = t2.date + 1，然后使用t1.totalCases – t2.totalCases计算该日新增。<br>3) 统计截止5.19日，美国各州的累计确诊人数和死亡人数。首先筛选出5.19日的数据，然后以state作为分组字段，对cases和deaths字段进行汇总统计。<br>4) 统计截止5.19日，美国确诊人数最多的十个州。对3)的结果DataFrame注册临时表，然后按确诊人数降序排列，并取前10个州。<br>5) 统计截止5.19日，美国死亡人数最多的十个州。对3)的结果DataFrame注册临时表，然后按死亡人数降序排列，并取前10个州。<br>6) 统计截止5.19日，美国确诊人数最少的十个州。对3)的结果DataFrame注册临时表，然后按确诊人数升序排列，并取前10个州。<br>7) 统计截止5.19日，美国死亡人数最少的十个州。对3)的结果DataFrame注册临时表，然后按死亡人数升序排列，并取前10个州<br>8) 统计截止5.19日，全美和各州的病死率。病死率 = 死亡数/确诊数，对3)的结果DataFrame注册临时表，然后按公式计算。<br>在计算以上几个指标过程中，根据实现的简易程度，既采用了DataFrame自带的操作函数，又采用了spark sql进行操作。</p>\n<h3 id=\"数据可视化\"><a href=\"#数据可视化\" class=\"headerlink\" title=\"数据可视化\"></a>数据可视化</h3><p>选择使用python第三方库pyecharts作为可视化工具。<br>在使用前，需要安装pyecharts，安装代码如下：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">pip install pyecharts</code></pre>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">from pyecharts import options as opts\nfrom pyecharts.charts import Bar\nfrom pyecharts.charts import Line\nfrom pyecharts.components import Table\nfrom pyecharts.charts import WordCloud\nfrom pyecharts.charts import Pie\nfrom pyecharts.charts import Funnel\nfrom pyecharts.charts import Scatter\nfrom pyecharts.charts import PictorialBar\nfrom pyecharts.options import ComponentTitleOpts\nfrom pyecharts.globals import SymbolType\nimport json\n \n \n \n#1.画出每日的累计确诊病例数和死亡数——&gt;双柱状图\ndef drawChart_1(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    date &#x3D; []\n    cases &#x3D; []\n    deaths &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            date.append(str(js[&#39;date&#39;]))\n            cases.append(int(js[&#39;cases&#39;]))\n            deaths.append(int(js[&#39;deaths&#39;]))\n \n    d &#x3D; (\n    Bar()\n    .add_xaxis(date)\n    .add_yaxis(&quot;累计确诊人数&quot;, cases, stack&#x3D;&quot;stack1&quot;)\n    .add_yaxis(&quot;累计死亡人数&quot;, deaths, stack&#x3D;&quot;stack1&quot;)\n    .set_series_opts(label_opts&#x3D;opts.LabelOpts(is_show&#x3D;False))\n    .set_global_opts(title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;美国每日累计确诊和死亡人数&quot;))\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result1&#x2F;result1.html&quot;)\n    )\n \n \n#2.画出每日的新增确诊病例数和死亡数——&gt;折线图\ndef drawChart_2(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    date &#x3D; []\n    cases &#x3D; []\n    deaths &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            date.append(str(js[&#39;date&#39;]))\n            cases.append(int(js[&#39;caseIncrease&#39;]))\n            deaths.append(int(js[&#39;deathIncrease&#39;]))\n \n    (\n    Line(init_opts&#x3D;opts.InitOpts(width&#x3D;&quot;1600px&quot;, height&#x3D;&quot;800px&quot;))\n    .add_xaxis(xaxis_data&#x3D;date)\n    .add_yaxis(\n        series_name&#x3D;&quot;新增确诊&quot;,\n        y_axis&#x3D;cases,\n        markpoint_opts&#x3D;opts.MarkPointOpts(\n            data&#x3D;[\n                opts.MarkPointItem(type_&#x3D;&quot;max&quot;, name&#x3D;&quot;最大值&quot;)\n \n            ]\n        ),\n        markline_opts&#x3D;opts.MarkLineOpts(\n            data&#x3D;[opts.MarkLineItem(type_&#x3D;&quot;average&quot;, name&#x3D;&quot;平均值&quot;)]\n        ),\n    )\n    .set_global_opts(\n        title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;美国每日新增确诊折线图&quot;, subtitle&#x3D;&quot;&quot;),\n        tooltip_opts&#x3D;opts.TooltipOpts(trigger&#x3D;&quot;axis&quot;),\n        toolbox_opts&#x3D;opts.ToolboxOpts(is_show&#x3D;True),\n        xaxis_opts&#x3D;opts.AxisOpts(type_&#x3D;&quot;category&quot;, boundary_gap&#x3D;False),\n    )\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result2&#x2F;result1.html&quot;)\n    )\n    (\n    Line(init_opts&#x3D;opts.InitOpts(width&#x3D;&quot;1600px&quot;, height&#x3D;&quot;800px&quot;))\n    .add_xaxis(xaxis_data&#x3D;date)\n    .add_yaxis(\n        series_name&#x3D;&quot;新增死亡&quot;,\n        y_axis&#x3D;deaths,\n        markpoint_opts&#x3D;opts.MarkPointOpts(\n            data&#x3D;[opts.MarkPointItem(type_&#x3D;&quot;max&quot;, name&#x3D;&quot;最大值&quot;)]\n        ),\n        markline_opts&#x3D;opts.MarkLineOpts(\n            data&#x3D;[\n                opts.MarkLineItem(type_&#x3D;&quot;average&quot;, name&#x3D;&quot;平均值&quot;),\n                opts.MarkLineItem(symbol&#x3D;&quot;none&quot;, x&#x3D;&quot;90%&quot;, y&#x3D;&quot;max&quot;),\n                opts.MarkLineItem(symbol&#x3D;&quot;circle&quot;, type_&#x3D;&quot;max&quot;, name&#x3D;&quot;最高点&quot;),\n            ]\n        ),\n    )\n    .set_global_opts(\n        title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;美国每日新增死亡折线图&quot;, subtitle&#x3D;&quot;&quot;),\n        tooltip_opts&#x3D;opts.TooltipOpts(trigger&#x3D;&quot;axis&quot;),\n        toolbox_opts&#x3D;opts.ToolboxOpts(is_show&#x3D;True),\n        xaxis_opts&#x3D;opts.AxisOpts(type_&#x3D;&quot;category&quot;, boundary_gap&#x3D;False),\n    )\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result2&#x2F;result2.html&quot;)\n    )\n \n \n \n \n#3.画出截止5.19，美国各州累计确诊、死亡人数和病死率---&gt;表格\ndef drawChart_3(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    allState &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            row &#x3D; []\n            row.append(str(js[&#39;state&#39;]))\n            row.append(int(js[&#39;totalCases&#39;]))\n            row.append(int(js[&#39;totalDeaths&#39;]))\n            row.append(float(js[&#39;deathRate&#39;]))\n            allState.append(row)\n \n    table &#x3D; Table()\n \n    headers &#x3D; [&quot;State name&quot;, &quot;Total cases&quot;, &quot;Total deaths&quot;, &quot;Death rate&quot;]\n    rows &#x3D; allState\n    table.add(headers, rows)\n    table.set_global_opts(\n        title_opts&#x3D;ComponentTitleOpts(title&#x3D;&quot;美国各州疫情一览&quot;, subtitle&#x3D;&quot;&quot;)\n    )\n    table.render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result3&#x2F;result1.html&quot;)\n \n \n#4.画出美国确诊最多的10个州——&gt;词云图\ndef drawChart_4(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    data &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            row&#x3D;(str(js[&#39;state&#39;]),int(js[&#39;totalCases&#39;]))\n            data.append(row)\n \n    c &#x3D; (\n    WordCloud()\n    .add(&quot;&quot;, data, word_size_range&#x3D;[20, 100], shape&#x3D;SymbolType.DIAMOND)\n    .set_global_opts(title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;美国各州确诊Top10&quot;))\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result4&#x2F;result1.html&quot;)\n    )\n \n \n \n \n#5.画出美国死亡最多的10个州——&gt;象柱状图\ndef drawChart_5(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    state &#x3D; []\n    totalDeath &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            state.insert(0,str(js[&#39;state&#39;]))\n            totalDeath.insert(0,int(js[&#39;totalDeaths&#39;]))\n \n    c &#x3D; (\n    PictorialBar()\n    .add_xaxis(state)\n    .add_yaxis(\n        &quot;&quot;,\n        totalDeath,\n        label_opts&#x3D;opts.LabelOpts(is_show&#x3D;False),\n        symbol_size&#x3D;18,\n        symbol_repeat&#x3D;&quot;fixed&quot;,\n        symbol_offset&#x3D;[0, 0],\n        is_symbol_clip&#x3D;True,\n        symbol&#x3D;SymbolType.ROUND_RECT,\n    )\n    .reversal_axis()\n    .set_global_opts(\n        title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;PictorialBar-美国各州死亡人数Top10&quot;),\n        xaxis_opts&#x3D;opts.AxisOpts(is_show&#x3D;False),\n        yaxis_opts&#x3D;opts.AxisOpts(\n            axistick_opts&#x3D;opts.AxisTickOpts(is_show&#x3D;False),\n            axisline_opts&#x3D;opts.AxisLineOpts(\n                linestyle_opts&#x3D;opts.LineStyleOpts(opacity&#x3D;0)\n            ),\n        ),\n    )\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result5&#x2F;result1.html&quot;)\n    )\n \n \n \n#6.找出美国确诊最少的10个州——&gt;词云图\ndef drawChart_6(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    data &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            row&#x3D;(str(js[&#39;state&#39;]),int(js[&#39;totalCases&#39;]))\n            data.append(row)\n \n    c &#x3D; (\n    WordCloud()\n    .add(&quot;&quot;, data, word_size_range&#x3D;[100, 20], shape&#x3D;SymbolType.DIAMOND)\n    .set_global_opts(title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;美国各州确诊最少的10个州&quot;))\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result6&#x2F;result1.html&quot;)\n    )\n \n \n \n \n#7.找出美国死亡最少的10个州——&gt;漏斗图\ndef drawChart_7(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    data &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            data.insert(0,[str(js[&#39;state&#39;]),int(js[&#39;totalDeaths&#39;])])\n \n    c &#x3D; (\n    Funnel()\n    .add(\n        &quot;State&quot;,\n        data,\n        sort_&#x3D;&quot;ascending&quot;,\n        label_opts&#x3D;opts.LabelOpts(position&#x3D;&quot;inside&quot;),\n    )\n    .set_global_opts(title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;&quot;))\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result7&#x2F;result1.html&quot;)\n    )\n \n \n#8.美国的病死率---&gt;饼状图\ndef drawChart_8(index):\n    root &#x3D; &quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result&quot; + str(index) +&quot;&#x2F;part-00000.json&quot;\n    values &#x3D; []\n    with open(root, &#39;r&#39;) as f:\n        while True:\n            line &#x3D; f.readline()\n            if not line:                            # 到 EOF，返回空字符串，则终止循环\n                break\n            js &#x3D; json.loads(line)\n            if str(js[&#39;state&#39;])&#x3D;&#x3D;&quot;USA&quot;:\n                values.append([&quot;Death(%)&quot;,round(float(js[&#39;deathRate&#39;])*100,2)])\n                values.append([&quot;No-Death(%)&quot;,100-round(float(js[&#39;deathRate&#39;])*100,2)])\n    c &#x3D; (\n    Pie()\n    .add(&quot;&quot;, values)\n    .set_colors([&quot;blcak&quot;,&quot;orange&quot;])\n    .set_global_opts(title_opts&#x3D;opts.TitleOpts(title&#x3D;&quot;全美的病死率&quot;))\n    .set_series_opts(label_opts&#x3D;opts.LabelOpts(formatter&#x3D;&quot;&#123;b&#125;: &#123;c&#125;&quot;))\n    .render(&quot;&#x2F;home&#x2F;hadoop&#x2F;result&#x2F;result8&#x2F;result1.html&quot;)\n    )\n \n \n#可视化主程序：\nindex &#x3D; 1\nwhile index&lt;9:\n    funcStr &#x3D; &quot;drawChart_&quot; + str(index)\n    eval(funcStr)(index)\n    index+&#x3D;1</code></pre>\n<h3 id=\"结果展示\"><a href=\"#结果展示\" class=\"headerlink\" title=\"结果展示\"></a>结果展示</h3><p>（1）美国每日的累计确诊病例数和死亡数——&gt;双柱状图<br><img src=\"https://img-blog.csdnimg.cn/444b873b287e497a897e571bcbe3875e.png\" alt=\"在这里插入图片描述\"><br>（2）美国每日的新增确诊病例数——&gt;折线图<br><img src=\"https://img-blog.csdnimg.cn/8dcd99ea6bf84b0fa6531d5c22b55d1e.png\" alt=\"在这里插入图片描述\"><br>（3）美国每日的新增死亡病例数——&gt;折线图</p>\n<p><img src=\"https://img-blog.csdnimg.cn/b47afa8e7e65439bb3691c3fd07d6fc4.png\" alt=\"在这里插入图片描述\"><br>（4）截止5.19，美国各州累计确诊、死亡人数和病死率—-&gt;表格<br><img src=\"https://img-blog.csdnimg.cn/ac883c87ff9848829457b2e30164f99b.png\" alt=\"在这里插入图片描述\"><br>（5）截止5.19，美国累计确诊人数前10的州—-&gt;词云图<br><img src=\"https://img-blog.csdnimg.cn/8920796e83cd4f9c9810e73139f1a0fb.png\" alt=\"在这里插入图片描述\"><br>（6）截止5.19，美国累计死亡人数前10的州—-&gt;象柱状图<br><img src=\"https://img-blog.csdnimg.cn/38f8b93813ff4ad5bf1a03cadfa8feef.png\" alt=\"在这里插入图片描述\"><br>（7）截止5.19，美国累计确诊人数最少的10个州—-&gt;词云图<br><img src=\"https://img-blog.csdnimg.cn/c1b2583ab7094fee9b5a5caf4e85a76e.png\" alt=\"在这里插入图片描述\"><br>（8）截止5.19，美国累计死亡人数最少的10个州—-&gt;漏斗图<br><img src=\"https://img-blog.csdnimg.cn/0f804162d75b4ae18147b9e3b953c153.png\" alt=\"在这里插入图片描述\"><br>（9）截止5.19，美国的病死率—-&gt;饼状图<br><img src=\"https://img-blog.csdnimg.cn/8b99e413f515449e806b4a49b796e96d.png\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><h3 id=\"1-无法上传到hdfs中\"><a href=\"#1-无法上传到hdfs中\" class=\"headerlink\" title=\"1 无法上传到hdfs中\"></a>1 无法上传到hdfs中</h3><p><img src=\"https://img-blog.csdnimg.cn/c52021dbd97542adb78b5999b1928704.png\" alt=\"在这里插入图片描述\"><br>解决方法：需要先开启hadoop</p>\n<h3 id=\"2-路径出现错误\"><a href=\"#2-路径出现错误\" class=\"headerlink\" title=\"2 路径出现错误\"></a>2 路径出现错误</h3><p><img src=\"https://img-blog.csdnimg.cn/b7a79425dd4f4317bc8e7e6a69c1d9b7.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-c\" data-language=\"c\"><code class=\"language-c\">Caused by: java.io.IOException: Input path does not exist: file:&#x2F;user&#x2F;hadoop&#x2F;us-counties.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 101 more</code></pre>\n<p>解决方法：修改路径</p>\n<h3 id=\"3-pip下载pyecharts网络出错\"><a href=\"#3-pip下载pyecharts网络出错\" class=\"headerlink\" title=\"3 pip下载pyecharts网络出错\"></a>3 pip下载pyecharts网络出错</h3><p>解决方法：换源，且换成手机热点下载</p>\n","text":"https://dblab.xmu.edu.cn/blog/2738 https://dblab.xmu.edu.cn/blog/2636/ spark 安装安装 Spark2.4.0sudo tar -zxf ~&#x2F;下载&#x2F;spark-2.4.0-bin-wit...","link":"","photos":[],"count_time":{"symbolsCount":"43k","symbolsTime":"39 mins."},"categories":[{"name":"信管","slug":"信管","count":19,"path":"api/categories/信管.json"}],"tags":[{"name":"大数据","slug":"大数据","count":7,"path":"api/tags/大数据.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#spark-%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">spark 安装</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AE%89%E8%A3%85-Spark2-4-0\"><span class=\"toc-text\">安装 Spark2.4.0</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%90%AF%E5%8A%A8Spark-Shell\"><span class=\"toc-text\">启动Spark Shell</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8A%A0%E8%BD%BDtext%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">加载text文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%AE%80%E5%8D%95RDD%E6%93%8D%E4%BD%9C\"><span class=\"toc-text\">简单RDD操作</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%80%E5%87%BASpark-Shell\"><span class=\"toc-text\">退出Spark Shell</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%BC%96%E7%A8%8B\"><span class=\"toc-text\">独立应用程序编程</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Scala%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81\"><span class=\"toc-text\">Scala应用程序代码</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8-sbt-%E6%89%93%E5%8C%85-Scala-%E7%A8%8B%E5%BA%8F\"><span class=\"toc-text\">使用 sbt 打包 Scala 程序</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%9A%E8%BF%87-spark-submit-%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F\"><span class=\"toc-text\">通过 spark-submit 运行程序</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8Maven%E5%AF%B9Java%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85\"><span class=\"toc-text\">使用Maven对Java独立应用程序进行编译打包</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#jupyter-notebook-%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8\"><span class=\"toc-text\">jupyter notebook 安装及使用</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%AE%89%E8%A3%85anaconda\"><span class=\"toc-text\">安装anaconda</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%85%8D%E7%BD%AEJupyter-Notebook\"><span class=\"toc-text\">配置Jupyter Notebook</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BF%90%E8%A1%8CJupyter-Notebook\"><span class=\"toc-text\">运行Jupyter Notebook</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%85%8D%E7%BD%AEJupyter-Notebook%E5%AE%9E%E7%8E%B0%E5%92%8CPySpark%E4%BA%A4%E4%BA%92\"><span class=\"toc-text\">配置Jupyter Notebook实现和PySpark交互</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90\"><span class=\"toc-text\">数据分析</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2\"><span class=\"toc-text\">格式转换</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%B0%86%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E8%87%B3HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%AD\"><span class=\"toc-text\">将文件上传至HDFS文件系统中</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8Spark%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90\"><span class=\"toc-text\">使用Spark对数据进行分析</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E7%94%9F%E6%88%90DataFrame\"><span class=\"toc-text\">读取文件生成DataFrame</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90\"><span class=\"toc-text\">进行数据分析</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96\"><span class=\"toc-text\">数据可视化</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA\"><span class=\"toc-text\">结果展示</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%97%AE%E9%A2%98\"><span class=\"toc-text\">问题</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-%E6%97%A0%E6%B3%95%E4%B8%8A%E4%BC%A0%E5%88%B0hdfs%E4%B8%AD\"><span class=\"toc-text\">1 无法上传到hdfs中</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-%E8%B7%AF%E5%BE%84%E5%87%BA%E7%8E%B0%E9%94%99%E8%AF%AF\"><span class=\"toc-text\">2 路径出现错误</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#3-pip%E4%B8%8B%E8%BD%BDpyecharts%E7%BD%91%E7%BB%9C%E5%87%BA%E9%94%99\"><span class=\"toc-text\">3 pip下载pyecharts网络出错</span></a></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"2021APMCM赛后总结——边缘检测","uid":"1940527d2a1b736857ad6c179b7a19e4","slug":"边缘检测","date":"2022-11-03T13:52:49.000Z","updated":"2022-11-04T06:39:15.731Z","comments":true,"path":"api/articles/边缘检测.json","keywords":null,"cover":[],"text":"APMCM被称为小美赛，和美赛一样使用英文提交论文，比赛时间又在国赛后美赛前，因此很适合作为MCM的热身赛。第一次参加这种正规比赛，而且是英文写作，作为对美赛的预热，花四天时间提前演练一下是很有必要的。在日常上课和（几乎）按时休息的条件下，最终，我们队能够拿到二等奖，算是比较满意...","link":"","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[],"tags":[{"name":"数模","slug":"数模","count":4,"path":"api/tags/数模.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"feature":true},"next_post":{"title":"【大数据基础】MapReduce 实验","uid":"35c915c8f544b6b45f887f7e156148c0","slug":"大数据6","date":"2023-03-15T13:50:49.000Z","updated":"2023-04-04T02:37:44.214Z","comments":true,"path":"api/articles/大数据6.json","keywords":null,"cover":[],"text":"https://dblab.xmu.edu.cn/blog/631/ https://dblab.xmu.edu.cn/blog/31/ 实验过程下载hadoop-eclipse-plugin，将 release 中的 hadoop-eclipse-kepler-plugin-2...","link":"","photos":[],"count_time":{"symbolsCount":"7.1k","symbolsTime":"6 mins."},"categories":[{"name":"信管","slug":"信管","count":19,"path":"api/categories/信管.json"}],"tags":[{"name":"大数据","slug":"大数据","count":7,"path":"api/tags/大数据.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}