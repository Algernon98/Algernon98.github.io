{"title":"【python机器学习基础教程】（二）","uid":"f0027b7e0577432cae210c02a7d12c2f","slug":"python机器学习2","date":"2022-11-03T14:15:49.000Z","updated":"2022-11-04T02:57:39.413Z","comments":true,"path":"api/articles/python机器学习2.json","keywords":null,"cover":[],"content":"<h1 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h1><h2 id=\"监督学习算法\"><a href=\"#监督学习算法\" class=\"headerlink\" title=\"监督学习算法\"></a>监督学习算法</h2><h3 id=\"朴素贝叶斯分类器\"><a href=\"#朴素贝叶斯分类器\" class=\"headerlink\" title=\"朴素贝叶斯分类器\"></a>朴素贝叶斯分类器</h3><p>朴素贝叶斯分类器通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。<br>scikit-learn中实现了三种朴素贝叶斯分类器：GaussianNB、BernoulliNB和MultinomialNB。GaussianNB可应用于任意连续数据，而BernoulliNB假定输入数据为二分类数据，MultinomialNB假定输入数据为计数数据（即每个特征代表某个对象的整数计数，比如一个单词在句子里出现的次数）。BernoulliNB和MultinomialNB主要用于文本数据分类。<br>BernoulliNB分类器计算每个类别中每个特征不为0的元素个数。</p>\n<h3 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h3><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_animal_tree()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/7aa4578ad19a4cae8ce7d98f42fe9afa.png#pic_center\" alt=\"在这里插入图片描述\"><br>在这张图中，树的每一个结点代表一个问题或一个包含答案的终结点（也叫<strong>叶结点</strong>）</p>\n<p>防止过拟合有两种常见的策略：一种是及早停止树的生长，也叫预剪枝；另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫后剪枝。</p>\n<h3 id=\"决策树集成\"><a href=\"#决策树集成\" class=\"headerlink\" title=\"决策树集成\"></a>决策树集成</h3><p>集成是合并多个机器学习模型来构建更强大模型的方法。<br>已证明有两种集成模型对大量分类和回归 的数据集都是有效的，二者都以决策树为基础，分别是<strong>随机森林</strong>和<strong>梯度提升决策树</strong></p>\n<h4 id=\"随机森林\"><a href=\"#随机森林\" class=\"headerlink\" title=\"随机森林\"></a>随机森林</h4><p>构造随机森林<br>分析随机森林<br>下面将5棵树组成的随机森林应用到two_moons数据集</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_moons\n\nX,y&#x3D;make_moons(n_samples&#x3D;100,noise&#x3D;0.25,random_state&#x3D;3)\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X,y,stratify&#x3D;y,random_state&#x3D;42)\n\nforest&#x3D;RandomForestClassifier(n_estimators&#x3D;5,random_state&#x3D;2)\nforest.fit(X_train,y_train)</code></pre>\n<p>作为随机森林的一部分，树被保存在estimator_属性中。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">fig,axes&#x3D;plt.subplots(2,3,figsize&#x3D;(20,10))\nfor i,(ax,tree) in enumerate(zip(axes.ravel(),forest.estimators_)):\n    ax.set_title(&quot;Tree&#123;&#125;&quot;.format(i))\n    mglearn.plots.plot_tree_partition(X_train,y_train,tree,ax&#x3D;ax)\nmglearn.plots.plot_2d_separator(forest,X_train,fill&#x3D;True,ax&#x3D;axes[-1,1],alpha&#x3D;.4)\naxes[-1,1].set_title(&quot;Random Forest&quot;)\nmglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/93c42c73332940cdac41eb4c5d14dd4c.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">def plot_feature_importances_cancer(model):\n     n_features&#x3D;cancer.data.shape[1]\n     plt.barh(range(n_features),model.feature_importances_,align&#x3D;&#39;center&#39;)\n     plt.yticks(np.arange(n_features),cancer.feature_names)\n     plt.xlabel(&quot;Feature importance&quot;)\n     plt.ylabel(&quot;Feature&quot;)\n\n\nfrom sklearn.datasets import load_breast_cancer\ncancer&#x3D;load_breast_cancer()\nX_train,X_test,y_train,y_test&#x3D;train_test_split(cancer.data,cancer.target,random_state&#x3D;0)\nforest&#x3D; RandomForestClassifier(n_estimators&#x3D;100,random_state&#x3D;0)\nforest.fit(X_train,y_train)\n\nplot_feature_importances_cancer(forest)</code></pre>\n<p>拟合乳腺癌数据集得到的随机森林的特征重要性。<br><img src=\"https://img-blog.csdnimg.cn/e5e6f850ea4441b68fa30d64b8bab42e.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"梯度提升-回归树（梯度提升机）\"><a href=\"#梯度提升-回归树（梯度提升机）\" class=\"headerlink\" title=\"梯度提升 回归树（梯度提升机）\"></a>梯度提升 回归树（梯度提升机）</h4><p>梯度提升回归树是另一种集成方法，通过合并多个决策树来构建一个更强大的模型。</p>\n<p>梯度提升背后的主要思想是合并许多简单的模型（在这个语境中叫<strong>做弱学习器</strong>），比如深度较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越来越多，可以不断提高迭代性能。</p>\n<p>除了预剪枝与集成中树的数量外，梯度提升的另一个重要参数是learning_rate（学习率），用以控制每棵树纠正前一棵树的错误的强度。</p>\n<h3 id=\"核支持向量机\"><a href=\"#核支持向量机\" class=\"headerlink\" title=\"核支持向量机\"></a>核支持向量机</h3><p>核支持向量机（SVM）是可以推广到更复杂模型的扩展，这些模型无法被输入空间的超平面定义。</p>\n<h4 id=\"线性模型和非线性特征\"><a href=\"#线性模型和非线性特征\" class=\"headerlink\" title=\"线性模型和非线性特征\"></a>线性模型和非线性特征</h4><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import make_blobs\nX,y&#x3D;make_blobs(centers&#x3D;4,random_state&#x3D;8)\ny&#x3D;y%2\n\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)</code></pre>\n<p><strong>二分类数据集，其类别并不是线性可分的：</strong><br><img src=\"https://img-blog.csdnimg.cn/82c38a6e327844e2ba80c45e26c1e2ff.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.svm import LinearSVC\nlinear_svm &#x3D; LinearSVC().fit(X,y)\n\nmglearn.plots.plot_2d_separator(linear_svm ,X)\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/c4b915af63b6438781bdb6c9ac5f2718.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在我们对输入特征进行扩展，比如说添加第二个特征的平方（feature1 **2）作为一个新特征。现在我们将每个数据点表示为三维数据点，这个新的表示可以画成三维散点图：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X_new &#x3D; np.hstack([X,X[:,1:]**2])\n\nfrom mpl_toolkits.mplot3d import Axes3D,axes3d\nfigure&#x3D;plt.figure()\n\nax&#x3D; Axes3D(figure,elev&#x3D;-152,azim&#x3D;-26)\n\nmask&#x3D;y&#x3D;&#x3D;0\nax.scatter(X_new[mask,0],X_new[mask,1],X_new[mask,2],c&#x3D;&#39;b&#39;,cmap&#x3D;mglearn.cm2,s&#x3D;60)\nax.scatter(X_new[~mask,0],X_new[~mask,1],X_new[~mask,2],c&#x3D;&#39;r&#39;,marker&#x3D;&#39;^&#39;,cmap&#x3D;mglearn.cm2,s&#x3D;60)\nax.set_xlabel(&quot;feature0&quot;)\nax.set_ylabel(&quot;feature1&quot;)\nax.set_zlabel(&quot;feature1 **2&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/81234dd86f8049e9be3146f1af58d866.png#pic_center\" alt=\"在这里插入图片描述\"><br>在数据的新表示中，现在可以用线性模型（三维空间中的平面）将这两个类别分开。<br>我们可以用线性模型拟合扩展后的数据来验证这一点：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">linear_svm_3d&#x3D;LinearSVC().fit(X_new,y)\ncoef,intercept&#x3D;linear_svm_3d.coef_.ravel(),linear_svm_3d.intercept_\n\nfigure&#x3D;plt.figure()\nax&#x3D; Axes3D(figure,elev&#x3D;-152,azim&#x3D;-26)\nxx&#x3D;np.linspace(X_new[:,0].min()-2,X_new[:,0].max()+2,50)\nyy&#x3D;np.linspace(X_new[:,1].min()-2,X_new[:,1].max()+2,50)\n\nXX,YY &#x3D;np.meshgrid(xx,yy)\nZZ&#x3D;(coef[0]*XX+coef[1]*YY+intercept)&#x2F;-coef[2]\nax.plot_surface(XX,YY,ZZ,rstride&#x3D;8,cstride&#x3D;8,alpha&#x3D;0.3)\nax.scatter(X_new[mask,0],X_new[mask,1],X_new[mask,2],c&#x3D;&#39;b&#39;,cmap&#x3D;mglearn.cm2,s&#x3D;60)\nax.scatter(X_new[~mask,0],X_new[~mask,1],X_new[~mask,2],c&#x3D;&#39;r&#39;,marker&#x3D;&#39;^&#39;,cmap&#x3D;mglearn.cm2,s&#x3D;60)\nax.set_xlabel(&quot;feature0&quot;)\nax.set_ylabel(&quot;feature1&quot;)\nax.set_zlabel(&quot;feature1 **2&quot;)</code></pre>\n<p>线性SVM对扩展后的三维数据集给出的决策边界<br><img src=\"https://img-blog.csdnimg.cn/1408c1bc0b7043f7b7601e0f3c9cf7c0.png#pic_center\" alt=\"在这里插入图片描述\"><br>如果将线性SVM模型看作原始特征的函数，那么它实际上已经不是线性的了。它不是一条直线，而是一个椭圆。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">ZZ&#x3D;YY**2\ndec&#x3D;linear_svm_3d.decision_function(np.c_[XX.ravel(),YY.ravel(),ZZ.ravel()])\nplt.contourf(XX,YY,dec.reshape(XX.shape),levels&#x3D;[dec.min(),0,dec.max()],cmap&#x3D;mglearn.cm2,alpha&#x3D;0.5)\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/b2c1ceefa781420e9c149c57421f75f5.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"核技巧\"><a href=\"#核技巧\" class=\"headerlink\" title=\"核技巧\"></a>核技巧</h4><p><strong>核技巧</strong>的原理是：直接计算扩展特征表示中数据点之间的距离（内积），而不用实际对扩展进行计算。</p>\n<p>对于支持向量机，将数据映射到更高维空间中有两种常用 方法：<br>一种是多项式核，在一定阶数内计算原始特征所有可能的多项式；<br>另一种是径向基函数核，也叫高斯核。</p>\n<h4 id=\"理解SVM\"><a href=\"#理解SVM\" class=\"headerlink\" title=\"理解SVM\"></a>理解SVM</h4><p>在训练过程中，SVM学习每个训练数据点对于两个类别之间的决策边界的重要性。<br>通常只有一部分训练数据点对于定义决策边界来说很重要：位于类别之间边界上的那些点。<br>这些点叫做<strong>支持向量</strong>，支持向量机正是由此得名。</p>\n<p>下图是支持向量机对一个二维二分类数据集上的训练结果。<br>决策边界用黑色表示，支持向量是尺寸较大的点。<br>下列代码将在forge数据集上训练SVM并创建此图。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.svm import SVC\nX,y&#x3D;mglearn.tools.make_handcrafted_dataset()\nsvm&#x3D;SVC(kernel&#x3D;&#39;rbf&#39;,C&#x3D;10,gamma&#x3D;0.1).fit(X,y)\nmglearn.plots.plot_2d_separator(svm,X,eps&#x3D;.5)\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\n\nsv&#x3D;svm.support_vectors_\n\nsv_labels&#x3D;svm.dual_coef_.ravel()&gt;0\nmglearn.discrete_scatter(sv[:,0],sv[:,1],sv_labels,s&#x3D;15,markeredgewidth&#x3D;3)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)</code></pre>\n<p>RBF核SVM给出的决策边界和支持向量<br><img src=\"https://img-blog.csdnimg.cn/06001c5b17414f3cb65a79a12ca132b6.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">fig,axes&#x3D;plt.subplots(3,3,figsize&#x3D;(15,10))\nfor ax,C in zip(axes,[-1,0,3]):\n    for a,gamma in zip(ax,range(-1,2)):\n        mglearn.plots.plot_svm(log_C&#x3D;C,log_gamma&#x3D;gamma,ax&#x3D;a)\naxes[0,0].legend([&quot;class 0&quot;,&quot;class 1&quot;,&quot;sv class 0&quot;,&quot;sv class 1&quot;],ncol&#x3D;4,loc&#x3D;(.9,1.2))</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/182a56130f0b4961a1bcbac84e5d84bf.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"神经网络（深度学习）\"><a href=\"#神经网络（深度学习）\" class=\"headerlink\" title=\"神经网络（深度学习）\"></a>神经网络（深度学习）</h2><p>此时此处只讨论相对简单的方法，即用于分类和回归的<strong>多层感知机</strong>（MLP）。</p>\n<p>MLP也被称为（普通）前馈神经网络，有时也简称为神经网络。<br>线性回归的预测公式为：</p>\n<p>$\\hat{y}=w[0]<em>x[0]+w[1]</em>x[1]+···+w[p]*x[p]+b$<br>简单来说，$\\hat{y}$是输入特征$x[0]$到$x[p]$的加权求和，权重为学到的系数$w[0]$到$w[p]$.<br>我们将这个公式可视化：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">display(mglearn.plots.plot_logistic_regression_graph())</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>digraph {<br>graph [rankdir=LR splines=line]<br>node [fixedsize=True shape=circle]<br>subgraph cluster_0 {<br>    node [shape=circle]<br>    “x[0]” [labelloc=c]<br>    “x[1]” [labelloc=c]<br>    “x[2]” [labelloc=c]<br>    “x[3]” [labelloc=c]<br>label = “inputs”    color = “white”    }<br>subgraph cluster_2 {<br>    node [shape=circle]<br>label = “output”    color = “white”        y<br>}<br>“x[0]” -&gt; y [label=”w[0]”]<br>“x[1]” -&gt; y [label=”w[1]”]<br>“x[2]” -&gt; y [label=”w[2]”]<br>“x[3]” -&gt; y [label=”w[3]”]<br>}</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">display(mglearn.plots.plot_single_hidden_layer_graph())</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>digraph {<br>graph [rankdir=LR splines=line]<br>node [fixedsize=True shape=circle]<br>subgraph cluster_0 {<br>    node [shape=circle]<br>    “x[0]”<br>    “x[1]”<br>    “x[2]”<br>    “x[3]”<br>label = “inputs”    color = “white”    }<br>subgraph cluster_1 {<br>    node [shape=circle]<br>label = “hidden layer”    color = “white”        h0 [label=”h[0]”]<br>    h1 [label=”h[1]”]<br>    h2 [label=”h[2]”]<br>}<br>subgraph cluster_2 {<br>    node [shape=circle]<br>    y<br>label = “output”    color = “white”    }<br>“x[0]” -&gt; h0<br>“x[0]” -&gt; h1<br>“x[0]” -&gt; h2<br>“x[1]” -&gt; h0<br>“x[1]” -&gt; h1<br>“x[1]” -&gt; h2<br>“x[2]” -&gt; h0<br>“x[2]” -&gt; h1<br>“x[2]” -&gt; h2<br>“x[3]” -&gt; h0<br>“x[3]” -&gt; h1<br>“x[3]” -&gt; h2<br>h0 -&gt; y<br>h1 -&gt; y<br>h2 -&gt; y<br>}</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">line &#x3D; np.linspace(-3, 3, 100)\nplt.plot(line, np.tanh(line), label&#x3D;&quot;tanh&quot;)\nplt.plot(line, np.maximum(line, 0), label&#x3D;&quot;relu&quot;)\nplt.legend(loc&#x3D;&quot;best&quot;)\nplt.xlabel(&quot;x&quot;)\nplt.ylabel(&quot;relu(x), tanh(x)&quot;)\nplt.show()</code></pre>\n<p><strong>双曲正切激活函数与校正线性激活函数</strong><br><img src=\"https://img-blog.csdnimg.cn/1d85ae5b3c2644e395cd7ce66281f804.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"神经网络调参\"><a href=\"#神经网络调参\" class=\"headerlink\" title=\"神经网络调参\"></a>神经网络调参</h3><p>我们将MLPClassifier应用到之前用过的teo_moons数据集上，以此研究MLP的工作原理。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\n\nX,y&#x3D;make_moons(n_samples&#x3D;100,noise&#x3D;0.25,random_state&#x3D;3)\n\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X,y,stratify&#x3D;y,random_state&#x3D;42)\n\nmlp&#x3D;MLPClassifier(solver&#x3D;&#39;lbfgs&#39;,random_state&#x3D;0).fit(X_train,y_train)\nmglearn.plots.plot_2d_separator(mlp,X_train,fill&#x3D;True,alpha&#x3D;.3)\nmglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)\n</code></pre>\n<p><strong>包含100个隐单元的神经网络在two_moons数据集上学到的决策边界</strong><br><img src=\"https://img-blog.csdnimg.cn/81ce5a303d0940abbd98e8d6fffd2fdf.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\n\nX,y&#x3D;make_moons(n_samples&#x3D;100,noise&#x3D;0.25,random_state&#x3D;3)\n\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X,y,stratify&#x3D;y,random_state&#x3D;42)\n\nmlp&#x3D;MLPClassifier(solver&#x3D;&#39;lbfgs&#39;,random_state&#x3D;0,hidden_layer_sizes&#x3D;[10])\nmlp.fit(X_train,y_train)\nmglearn.plots.plot_2d_separator(mlp,X_train,fill&#x3D;True,alpha&#x3D;.3)\nmglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)\n</code></pre>\n<p><strong>包含10个隐单元的神经网络在two_moons数据集上学到的决策边界</strong><br><img src=\"https://img-blog.csdnimg.cn/f49ec310fdc84beeb479746748a252f2.png#pic_center\" alt=\"在这里插入图片描述\"><br>只有10个隐单元时，决策边界看起来更加参差不齐。默认的非线性是relu。如果使用单隐层，那么决策函数将由10个直线段组成。如果想要得到更加平滑的决策边界，可以添加更多的隐单元、添加第二个隐层或者使用tanh非线性。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#使用2个隐层，每个包含10个单元\nmlp&#x3D;MLPClassifier(solver&#x3D;&#39;lbfgs&#39;,random_state&#x3D;0,hidden_layer_sizes&#x3D;[10,10])\nmlp.fit(X_train,y_train)\nmglearn.plots.plot_2d_separator(mlp,X_train,fill&#x3D;True,alpha&#x3D;.3)\nmglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)</code></pre>\n<p><strong>包含两个隐层、每个隐层包含10个隐单元的神经网络学到的决策边界(激活函数为relu)</strong><br><img src=\"https://img-blog.csdnimg.cn/03ad0c1a3c944a7986906ea889dcc22a.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#使用2个隐层，每个包含10个单元，这次使用tanh非线性\nmlp&#x3D;MLPClassifier(solver&#x3D;&#39;lbfgs&#39;,activation&#x3D;&#39;tanh&#39;,random_state&#x3D;0,hidden_layer_sizes&#x3D;[10,10])\nmlp.fit(X_train,y_train)\nmglearn.plots.plot_2d_separator(mlp,X_train,fill&#x3D;True,alpha&#x3D;.3)\nmglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)</code></pre>\n<p><strong>包含两个隐层、每个隐层包含10个隐单元的神经网络学到的决策边界(激活函数为tanh)</strong></p>\n<p><img src=\"https://img-blog.csdnimg.cn/af686c0b86da42c6933d9b3a7affc9b1.png#pic_center\" alt=\"在这里插入图片描述\"><br>为了在现实世界的数据上进一步理解神经网络，我们将MLPClassifier应用在乳腺癌数据集上。<br>首先使用默认参数：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_breast_cancer\ncancer&#x3D;load_breast_cancer()\nprint(&quot;Cancer data per-feature maxima:\\n&#123;&#125;&quot;.format(cancer.data.max(axis&#x3D;0)))\n</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Cancer data per-feature maxima:<br>[2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 3.454e-01 4.268e-01<br> 2.012e-01 3.040e-01 9.744e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02<br> 3.113e-02 1.354e-01 3.960e-01 5.279e-02 7.895e-02 2.984e-02 3.604e+01<br> 4.954e+01 2.512e+02 4.254e+03 2.226e-01 1.058e+00 1.252e+00 2.910e-01<br> 6.638e-01 2.075e-01]</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X_train,X_test,y_train,y_test&#x3D;train_test_split(cancer.data,cancer.target,random_state&#x3D;0)\n\nmlp&#x3D;MLPClassifier(random_state&#x3D;42)\nmlp.fit(X_train,y_train)\n\nmean_on_train&#x3D;X_train.mean(axis&#x3D;0)\nstd_on_train&#x3D;X_train.std(axis&#x3D;0)\nX_train_scaled&#x3D;(X_train-mean_on_train)&#x2F;std_on_train\nX_test_scaled&#x3D;(X_test-mean_on_train)&#x2F;std_on_train\n\nmlp&#x3D;MLPClassifier(random_state&#x3D;0)\nmlp.fit(X_train_scaled,y_train)\n\nmlp&#x3D;MLPClassifier(max_iter&#x3D;1000,random_state&#x3D;0)\nmlp.fit(X_train,y_train)\n\nmlp&#x3D;MLPClassifier(max_iter&#x3D;1000,alpha&#x3D;1,random_state&#x3D;0)\nmlp.fit(X_train,y_train)\n\nplt.figure(figsize&#x3D;(20,5))\nplt.imshow(mlp.coefs_[0],interpolation&#x3D;&#39;none&#39;,cmap&#x3D;&#39;viridis&#39;)\nplt.yticks(range(30),cancer.feature_names)\nplt.xlabel(&quot;Columns in weight matrix&quot;)\nplt.ylabel(&quot;Input feature &quot;)\nplt.colorbar()\n</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/a44a1fc6f7fb41508bd3b2bbd487008f.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"分类器的不确定度估计\"><a href=\"#分类器的不确定度估计\" class=\"headerlink\" title=\"分类器的不确定度估计\"></a>分类器的不确定度估计</h2><h3 id=\"决策函数\"><a href=\"#决策函数\" class=\"headerlink\" title=\"决策函数\"></a>决策函数</h3><p>对于二分类的情况，decision_function返回值的形状是（n_samples,）,为每个样本都返回一个浮点数：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_circles\nX,y&#x3D;make_circles(noise&#x3D;0.25,factor&#x3D;0.5,random_state&#x3D;1)\n\ny_named&#x3D;np.array([&quot;blue&quot;,&quot;red&quot;])[y]\n\nX_train,X_test,y_train_named,y_test_named,y_train,y_test&#x3D;train_test_split(X,y_named,y,random_state&#x3D;0)\n\ngbrt&#x3D;GradientBoostingClassifier(random_state&#x3D;0)\ngbrt.fit(X_train,y_train_named)\nprint(&quot;X_test.shape:&#123;&#125;&quot;.format(X_test.shape))\nprint(&quot;Decision function shape:&#123;&#125;&quot;.format(gbrt.decision_function(X_test).shape))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>X_test.shape:(25, 2)<br>Decision function shape:(25,)</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">decision_function&#x3D;gbrt.decision_function(X_test)\n\nfig,axes&#x3D;plt.subplots(1,2,figsize&#x3D;(13,5))\nmglearn.tools.plot_2d_separator(gbrt,X,ax&#x3D;axes[0],alpha&#x3D;.4,fill&#x3D;True,cm&#x3D;mglearn.cm2)\nscores_image&#x3D;mglearn.tools.plot_2d_scores(gbrt,X,ax&#x3D;axes[1],alpha&#x3D;.4,cm&#x3D;mglearn.ReBl)\n\nfor ax in axes:\n    mglearn.discrete_scatter(X_test[:,0],X_test[:,1],y_test,markers&#x3D;&#39;^&#39;,ax&#x3D;ax)\n    mglearn.discrete_scatter(X_train[:,0],X_train[:,1],y_train,markers&#x3D;&#39;o&#39;,ax&#x3D;ax)\n    ax.set_xlabel(&quot;Feature 0&quot;)\n    ax.set_ylabel(&quot;Feature 1&quot;)\ncbar&#x3D;plt.colorbar(scores_image,ax&#x3D;axes.tolist())\naxes[0].legend([&quot;Test class 0&quot;,&quot;Test class 1&quot;,&quot;Train class 0&quot;,&quot;Train class 1&quot;],ncol&#x3D;4,loc&#x3D;(.1,1.1))</code></pre>\n<p><strong>梯度提升模型在一个二维玩具数据集上的决策边界（左）和决策函数（右）</strong><br><img src=\"https://img-blog.csdnimg.cn/983fe0451e7b49698fdcc44604ac2760.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"预测概率\"><a href=\"#预测概率\" class=\"headerlink\" title=\"预测概率\"></a>预测概率</h3><h3 id=\"多分类问题的不确定度\"><a href=\"#多分类问题的不确定度\" class=\"headerlink\" title=\"多分类问题的不确定度\"></a>多分类问题的不确定度</h3>","text":"监督学习监督学习算法朴素贝叶斯分类器朴素贝叶斯分类器通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。scikit-learn中实现了三种朴素贝叶斯分类器：GaussianNB、BernoulliNB和MultinomialNB。GaussianNB可应用于任...","link":"","photos":[],"count_time":{"symbolsCount":"13k","symbolsTime":"12 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">监督学习</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">监督学习算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8\"><span class=\"toc-text\">朴素贝叶斯分类器</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%B3%E7%AD%96%E6%A0%91\"><span class=\"toc-text\">决策树</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90\"><span class=\"toc-text\">决策树集成</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97\"><span class=\"toc-text\">随机森林</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87-%E5%9B%9E%E5%BD%92%E6%A0%91%EF%BC%88%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%9C%BA%EF%BC%89\"><span class=\"toc-text\">梯度提升 回归树（梯度提升机）</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A0%B8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\"><span class=\"toc-text\">核支持向量机</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%89%B9%E5%BE%81\"><span class=\"toc-text\">线性模型和非线性特征</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%A0%B8%E6%8A%80%E5%B7%A7\"><span class=\"toc-text\">核技巧</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%90%86%E8%A7%A3SVM\"><span class=\"toc-text\">理解SVM</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%89\"><span class=\"toc-text\">神经网络（深度学习）</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82\"><span class=\"toc-text\">神经网络调参</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1\"><span class=\"toc-text\">分类器的不确定度估计</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%86%B3%E7%AD%96%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">决策函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87\"><span class=\"toc-text\">预测概率</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6\"><span class=\"toc-text\">多分类问题的不确定度</span></a></li></ol></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"【电子羊的奇妙冒险 】初试深度学习（4）","uid":"0ace423a65608d78c82e5378fc3daeb8","slug":"电子羊4","date":"2022-11-03T14:18:49.000Z","updated":"2022-11-08T16:16:30.225Z","comments":true,"path":"api/articles/电子羊4.json","keywords":null,"cover":[],"text":"代码注释随机数 出于我们的目的，我将创建一组具有根的此类多项式。实际上，我将首先创建根，然后创建多项式，如下所示： import numpy as np MIN_ROOT &#x3D; -1 MAX_ROOT &#x3D; 1 def make(n_samples, n_degr...","link":"","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"13 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":9,"path":"api/categories/机器学习.json"}],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"【数学建模】课程笔记","uid":"679a1ca1ed196b2867a4735b2a0d1db1","slug":"数模课程笔记","date":"2022-11-03T14:03:49.000Z","updated":"2022-11-03T14:03:50.062Z","comments":true,"path":"api/articles/数模课程笔记.json","keywords":null,"cover":[],"text":"插值及案例实验插值是对给定数据的处理方法，它在工程实践和科学实验中有非常广泛、重要的应用。主要应用在如下两个方面：1、通过数据进行预测分析，例如：工程实验数据与模型的分析、天气预报以及社会、经济行为的统计分析等。2、图形绘制与可视化，例如：图像重建、工程外观设计等。 插值基本概念...","link":"","photos":[],"count_time":{"symbolsCount":"3.6k","symbolsTime":"3 mins."},"categories":[],"tags":[{"name":"数模","slug":"数模","count":4,"path":"api/tags/数模.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}