{"title":"《深度学习》 笔记（一）","uid":"6e741307528ed1e909d0d5dbdbcaa4c0","slug":"深度学习1","date":"2022-11-03T13:54:49.000Z","updated":"2022-11-03T13:56:10.735Z","comments":true,"path":"api/articles/深度学习1.json","keywords":null,"cover":null,"content":"<p>深度学习的另一个最大大成就是其在<strong>强化学习</strong>领域的扩展。<br>在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来完成任务。</p>\n<h1 id=\"第一部分-应用数学与机器学习基础\"><a href=\"#第一部分-应用数学与机器学习基础\" class=\"headerlink\" title=\"第一部分 应用数学与机器学习基础\"></a>第一部分 应用数学与机器学习基础</h1><h2 id=\"线性代数\"><a href=\"#线性代数\" class=\"headerlink\" title=\"线性代数\"></a>线性代数</h2><p><strong>范数</strong><br>在机器学习中，我们经常使用被称为<em>范数</em>的函数衡量向量大小。<br>p=2时，$L^2$范数被称为欧几里得范数。它表示从原点出发到向量$x$确定的点的欧几里得距离。</p>\n<h2 id=\"概率与信息论\"><a href=\"#概率与信息论\" class=\"headerlink\" title=\"概率与信息论\"></a>概率与信息论</h2><p>不确定性有三种可能的来源：</p>\n<ol>\n<li>被建模系统内在的随机性</li>\n<li>不完全观测</li>\n<li><p>不完全建模</p>\n<p><strong>随机变量</strong><br>随机变量是可以随机地取不同值的变量。<br>我们通常用无格式字体中的小写字母来表示随机变量本身，而用手写体中的小写字母来表示随机变量能够取到的值。</p>\n</li>\n</ol>\n<p><strong>概率分布</strong><br>概率分布用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散的还是连续的。</p>\n<h3 id=\"离散型变量和概率质量函数\"><a href=\"#离散型变量和概率质量函数\" class=\"headerlink\" title=\"离散型变量和概率质量函数\"></a>离散型变量和概率质量函数</h3><p>离散型变量的概率分布可以用<strong>概率质量函数</strong>来描述。通常用大写字母$P$来表示。<br>概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称为<strong>联合概率分布</strong>。</p>\n<h3 id=\"连续型变量和概率密度函数\"><a href=\"#连续型变量和概率密度函数\" class=\"headerlink\" title=\"连续型变量和概率密度函数\"></a>连续型变量和概率密度函数</h3><p>当我们研究的对象是连续型随机变量时，我们用<strong>概率密度函数</strong>来描述概率分布。<br>概率密度函数$p(x)$并没有直接对特定状态给出概率，相对的，它给出了落在面积为$\\sigma x$的无限小的区域内的概率为$p(x)\\sigma x$。</p>\n<h3 id=\"边缘概率\"><a href=\"#边缘概率\" class=\"headerlink\" title=\"边缘概率\"></a>边缘概率</h3><p>有时，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概率分布。这种定义在子集上的概率分布被称为<strong>边缘概率分布</strong>。<br>例如，假设有离散型随机变量x和y，并且我们知道$P({\\rm x},{\\rm y})$。我们可以依据下面的求和法则来计算$P(x)$:</p>\n<script type=\"math/tex; mode=display\">{\\forall}x\\in {\\rm x},P({\\rm x}=x )=\\sum_{y}P({\\rm x}=x, {\\rm y}=y)</script><p>“边缘概率”的名称来源于手算边缘概率的计算过程。当$P({\\rm x},{\\rm y})$的每个值被写在由每行表示不同的$x$值，每列表示不同的$y$值形成的网络中时，对网格中的每行求和，然后将求和的结果$P(x)$写在每行右边的纸的边缘处。</p>\n<p>对于连续型变量，我们需要用积分替代求和：<br>   $p(x)=\\int p(x,y)dy.$</p>\n<h3 id=\"条件概率\"><a href=\"#条件概率\" class=\"headerlink\" title=\"条件概率\"></a>条件概率</h3><p>条件概率的链式法则<br>独立性和条件独立性</p>\n<h3 id=\"常用概率分布\"><a href=\"#常用概率分布\" class=\"headerlink\" title=\"常用概率分布\"></a>常用概率分布</h3><p>许多简单的概率分布在机器学习的众多领域中都是有用的。</p>\n<h4 id=\"Bernoulli分布\"><a href=\"#Bernoulli分布\" class=\"headerlink\" title=\"Bernoulli分布\"></a>Bernoulli分布</h4><p><strong>Bernoulli分布</strong>是单个二值随机变量的分布。<br>它由单个参数$\\phi\\in[0,1]$控制，$\\phi$给出了随机变量等于1的概率。<br>它具有如下的一些性质：<br>$P({\\rm x}=1)=\\phi$<br>$P({\\rm x}=0)=1-\\phi$<br>$P({\\rm x}=x)={\\phi}^x （1-{\\phi}）^{1-x}$<br>$\\Epsilon_{\\rm x}[{\\rm x}]=\\phi$<br>$Var_{\\rm x}[{\\rm x}]=\\phi(1-\\phi)$</p>\n<h4 id=\"Multinoulli分布\"><a href=\"#Multinoulli分布\" class=\"headerlink\" title=\"Multinoulli分布\"></a>Multinoulli分布</h4><p><strong>Multinoulli分布</strong>或者<strong>范畴分布</strong>是指在具有$k$个不同状态的单个离散型随机变量上的分布，其中$k$是一个有限值。<br>Multinoulli分布由向量$p\\in[0,1]^{k-1}$参数化，其中每个分量$p_i$表示第$i$个状态的概率。<br>最后的第$k$个状态的概率可以通过1-$1^\\top p$给出。<br>注意，我们必须限制$1^\\top \\le1$。</p>\n<h4 id=\"高斯分布\"><a href=\"#高斯分布\" class=\"headerlink\" title=\"高斯分布\"></a>高斯分布</h4><p>实数上最常用的分布就是<strong>正态分布</strong>，也称为<strong>高斯分布</strong>：<br>$\\mathcal N(x;\\mu,\\sigma^2)=\\sqrt[3]{\\frac{1}{2\\pi\\sigma^2}}{\\rm exp}(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)$</p>\n<p>采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因：</p>\n<p>第一，<strong>中心极限定理</strong>说明很多独立随机变量的和近似服从正态分布。<br>这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。</p>\n<p>第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。<br>正态分布可以推广到$R^n$空间，这种情况下被称为<strong>多维正态分布</strong>。</p>\n<h4 id=\"指数分布和Laplace分布\"><a href=\"#指数分布和Laplace分布\" class=\"headerlink\" title=\"指数分布和Laplace分布\"></a>指数分布和Laplace分布</h4><p>在深度学习中，我们经常会需要一个在$x=0$点处取得边界点的分布。<br>为了实现这一目的，我们可以使用<strong>指数分布：</strong><br>$p(x;\\lambda)=\\lambda1_{x\\ge0}{\\rm exp}(-\\lambda x).$</p>\n<p>一个联系紧密的概率分布是<strong>Laplace分布</strong>，它允许我们在任意一点$\\mu$处设置概率质量的峰值<br>Laplace$(x;\\mu,\\gamma)=\\frac{1}{2\\gamma}{\\rm exp}(-\\frac{\\vert x-\\mu\\vert}{\\gamma})$</p>\n<h4 id=\"Dirac分布和经验分布\"><a href=\"#Dirac分布和经验分布\" class=\"headerlink\" title=\"Dirac分布和经验分布\"></a>Dirac分布和经验分布</h4><p>在一些情况下，我们希望概率分布中的所有质量都集中到一个点上。这可以通过<strong>Dirac delta 函数</strong>$\\delta(x)$定义概率密度来实现：<br>$p(x)=\\delta(x-\\mu)$<br>Dirac delta 函数不像普通函数一样对$x$的每一个值都有一个实数值的输出，它是一种不同类型的数学对象，被称为<strong>广义函数</strong>，广义函数是依据积分性质定义的数学对象。<br>Dirac分布经常作为经验分布的一个组成部分出现。</p>\n<h4 id=\"分布的混合\"><a href=\"#分布的混合\" class=\"headerlink\" title=\"分布的混合\"></a>分布的混合</h4><p>一个非常强大且常见的混合模型是<strong>高斯混合模型</strong>。</p>\n<h4 id=\"常用函数的有用性质\"><a href=\"#常用函数的有用性质\" class=\"headerlink\" title=\"常用函数的有用性质\"></a>常用函数的有用性质</h4><p>某些函数在处理概率分布时经常会出现，尤其是深度学习的模型中用到的概率分布。<br>其中一个函数是<strong>logistic sigmoid</strong> 函数：<br>$\\sigma(x)=\\frac{1}{1+{\\rm exp}(-x)}.$<br>另外一个经常用到的函数是softplus函数：<br>$\\zeta(x)=log(1+{\\rm exp}(x)).$</p>\n<h3 id=\"信息论\"><a href=\"#信息论\" class=\"headerlink\" title=\"信息论\"></a>信息论</h3><p>信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。<br>…我们想用这种基本想法来量化信息。特别地，</p>\n<ul>\n<li>非常可能发生的时间信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。</li>\n<li>较不可能发生的事件具有更高的信息量。</li>\n<li><p>独立事件应具有增量的信息。</p>\n<p>为了满足上述三个性质，我们定义一个时间${\\rm x}=x$的<strong>自信息</strong>为：<br>$I(x)=-logP(x).$<br><strong>本书中</strong>，我们用log来表示自然对数，其底数为$e$。因此，我们定义的$I(x)$单位是<strong>奈特</strong>。<br>一奈特是以$\\frac{1}{e}$的概率观测到一个事件时获得的信息量。<br>其他的材料中使用底数为2的对数，单位是<strong>比特</strong>或者<strong>香农</strong>。<br>自信息只处理单个的输出。我们可以用香农熵来对整个概率分布中的不确定性总量进行量化。<br>如果我们对同一个随机变量x有两个单独的概率分布<em>P</em>（x）和<em>Q</em>（x），我们可以使用KL散度来衡量这两个分布的差异。</p>\n</li>\n</ul>\n<h3 id=\"结构化概率模型\"><a href=\"#结构化概率模型\" class=\"headerlink\" title=\"结构化概率模型\"></a>结构化概率模型</h3><p>机器学习的算法经常会涉及到在非常多的随机变量上的概率分布。<br>我们可以把概率分布分解成许多因子的乘积形式，而不是使用单一的函数来表示概率分布。<br>我们可以用图来描述这种分解。这里我们使用的是图论中的“图”的概念：由一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分解，我们把它称为<strong>结构化概率模型</strong>或者<strong>图模型</strong><br>有两种主要的结构化概率模型：有向的和无向的。</p>\n<h2 id=\"数值计算\"><a href=\"#数值计算\" class=\"headerlink\" title=\"数值计算\"></a>数值计算</h2><h3 id=\"上溢和下溢\"><a href=\"#上溢和下溢\" class=\"headerlink\" title=\"上溢和下溢\"></a>上溢和下溢</h3><p>必须对上溢和下溢进行数值稳定的一个例子是<strong>softmax</strong>函数，该函数经常用于预测与Multinoulli分布相关联的概率。</p>\n<h3 id=\"基于梯度的优化算法\"><a href=\"#基于梯度的优化算法\" class=\"headerlink\" title=\"基于梯度的优化算法\"></a>基于梯度的优化算法</h3><p>我们把要最小化或最大化的函数称为<strong>目标函数</strong>或<strong>准则</strong>。当我们对其进行最小化时，我们也把它称为<strong>代价函数</strong>、<strong>损失函数</strong>或<strong>误差函数</strong></p>\n<h4 id=\"梯度之上：Jacobian和Hessian矩阵\"><a href=\"#梯度之上：Jacobian和Hessian矩阵\" class=\"headerlink\" title=\"梯度之上：Jacobian和Hessian矩阵\"></a>梯度之上：Jacobian和Hessian矩阵</h4><p>有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的偏导数的矩阵被称为<strong>Jacobian</strong>矩阵。<br>当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并为一个矩阵，称为<br><strong>Hessian</strong>矩阵。<br>仅使用梯度信息的优化算法被称为<strong>一阶优化算法</strong>，如梯度下降。<br>使用Hessian矩阵的优化算法被称为<strong>二阶最优化算法</strong>，如牛顿法。</p>\n<h2 id=\"机器学习基础\"><a href=\"#机器学习基础\" class=\"headerlink\" title=\"机器学习基础\"></a>机器学习基础</h2><h3 id=\"算法\"><a href=\"#算法\" class=\"headerlink\" title=\"算法\"></a>算法</h3><p>通常机器学习任务定义为机器学习系统应该如何处理<strong>样本</strong>。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的<strong>特征</strong>的集合。<br>一些常见的机器学习任务列举如下：</p>\n<ul>\n<li>分类</li>\n<li>输入缺失分类</li>\n<li>回归</li>\n<li>转录</li>\n<li>机器翻译</li>\n<li>结构化输出</li>\n<li>异常检测</li>\n<li>合成与采样</li>\n<li>缺失值填补</li>\n<li>去噪</li>\n<li><p>密度估计或概率质量函数估计</p>\n<p>根据学习过程中的不同经验，机器学习算法可以大致分类为<strong>无监督</strong>算法和<strong>监督</strong>算法。</p>\n<p><strong>无监督学习算法</strong>训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。在深度学习中，我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或者是隐式地，比如合成或去噪。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。<br><strong>监督学习算法</strong>训练含有很多特征的数据集，不过数据集中的样本都有一个<strong>标签</strong>或<strong>目标</strong>。<br>有些机器学习算法并不是训练于一个固定的数据集上。例如，<strong>强化学习</strong>算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。<br><strong>贝叶斯误差</strong></p>\n<h4 id=\"没有免费午餐定理\"><a href=\"#没有免费午餐定理\" class=\"headerlink\" title=\"没有免费午餐定理\"></a>没有免费午餐定理</h4><p>机器学习的<strong>没有免费午餐定理</strong>表明，在所有可能的数据生成分布上平均之后，每一个分类算法在未实现观测的点上都有相同的错误率。<br><strong>正则化</strong>是指我们修改学习算法，使其降低泛化误差而非训练误差。</p>\n<h3 id=\"估计、偏差和方差\"><a href=\"#估计、偏差和方差\" class=\"headerlink\" title=\"估计、偏差和方差\"></a>估计、偏差和方差</h3><h4 id=\"点估计\"><a href=\"#点估计\" class=\"headerlink\" title=\"点估计\"></a>点估计</h4><p><strong>点估计</strong>试图为一些感兴趣的量提供单个“最优”预测。<br><strong>函数估计</strong></p>\n<h4 id=\"偏差\"><a href=\"#偏差\" class=\"headerlink\" title=\"偏差\"></a>偏差</h4><p>无偏<br>渐进无偏<br>样本方差<br>无偏样本方差</p>\n<h3 id=\"最大似然估计\"><a href=\"#最大似然估计\" class=\"headerlink\" title=\"最大似然估计\"></a>最大似然估计</h3><p>在合适的条件下，最大似然估计具有一致性，意味着训练样品数目趋向无穷大时，参数的最大似然估计会收敛到参数的真实值。</p>\n<h3 id=\"贝叶斯统计\"><a href=\"#贝叶斯统计\" class=\"headerlink\" title=\"贝叶斯统计\"></a>贝叶斯统计</h3><p>先验概率分布</p>\n<h3 id=\"监督学习算法\"><a href=\"#监督学习算法\" class=\"headerlink\" title=\"监督学习算法\"></a>监督学习算法</h3><h4 id=\"概率监督学习\"><a href=\"#概率监督学习\" class=\"headerlink\" title=\"概率监督学习\"></a>概率监督学习</h4><p>逻辑回归</p>\n<h4 id=\"支持向量机\"><a href=\"#支持向量机\" class=\"headerlink\" title=\"支持向量机\"></a>支持向量机</h4><p><strong>支持向量机</strong>是监督学习中最有影响力的方法之一。支持向量机不输出概率，只输出类别。<br>支持向量机的一个重要创新是<strong>核技巧</strong><br>最常用的核函数是<strong>高斯核</strong><br>$k(\\mu,\\nu)=\\mathcal N(\\mu-\\nu;0,\\sigma^2I)$<br>这个核也被称为<strong>径向基函数</strong>核，因为其值沿$\\nu$中从$\\mu$向外辐射的方向减小。</p>\n<h4 id=\"其他简单的监督学习算法\"><a href=\"#其他简单的监督学习算法\" class=\"headerlink\" title=\"其他简单的监督学习算法\"></a>其他简单的监督学习算法</h4><p>最近邻回归——<strong>k-最近邻</strong><br><strong>决策树</strong>及其变种是另一类将输入空间分成不同的区域，每个区域有独立参数的算法。</p>\n<h3 id=\"无监督学习算法\"><a href=\"#无监督学习算法\" class=\"headerlink\" title=\"无监督学习算法\"></a>无监督学习算法</h3><p><strong>主成分分析</strong><br><strong>$k-$均值聚类</strong></p>\n<p>$k-$均值聚类算法将训练集分成$k$个靠近彼此的不同样本聚类。</p>\n<h3 id=\"随机梯度下降\"><a href=\"#随机梯度下降\" class=\"headerlink\" title=\"随机梯度下降\"></a>随机梯度下降</h3><p>随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。</p>\n<h3 id=\"流形学习\"><a href=\"#流形学习\" class=\"headerlink\" title=\"流形学习\"></a>流形学习</h3><p><strong>流形</strong>是指连接在一起的区域。</p>\n</li>\n</ul>\n","text":"深度学习的另一个最大大成就是其在强化学习领域的扩展。在强化学习中，一个自主的智能体必须在没有人类操作者指导的情况下，通过试错来完成任务。 第一部分 应用数学与机器学习基础线性代数范数在机器学习中，我们经常使用被称为范数的函数衡量向量大小。p=2时，$L^2$范数被称为欧几里得范数...","link":"","photos":[],"count_time":{"symbolsCount":"4.4k","symbolsTime":"4 mins."},"categories":[],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E5%BA%94%E7%94%A8%E6%95%B0%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80\"><span class=\"toc-text\">第一部分 应用数学与机器学习基础</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0\"><span class=\"toc-text\">线性代数</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA\"><span class=\"toc-text\">概率与信息论</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%A6%BB%E6%95%A3%E5%9E%8B%E5%8F%98%E9%87%8F%E5%92%8C%E6%A6%82%E7%8E%87%E8%B4%A8%E9%87%8F%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">离散型变量和概率质量函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E5%8F%98%E9%87%8F%E5%92%8C%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">连续型变量和概率密度函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87\"><span class=\"toc-text\">边缘概率</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87\"><span class=\"toc-text\">条件概率</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83\"><span class=\"toc-text\">常用概率分布</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Bernoulli%E5%88%86%E5%B8%83\"><span class=\"toc-text\">Bernoulli分布</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Multinoulli%E5%88%86%E5%B8%83\"><span class=\"toc-text\">Multinoulli分布</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83\"><span class=\"toc-text\">高斯分布</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E5%92%8CLaplace%E5%88%86%E5%B8%83\"><span class=\"toc-text\">指数分布和Laplace分布</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#Dirac%E5%88%86%E5%B8%83%E5%92%8C%E7%BB%8F%E9%AA%8C%E5%88%86%E5%B8%83\"><span class=\"toc-text\">Dirac分布和经验分布</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%88%86%E5%B8%83%E7%9A%84%E6%B7%B7%E5%90%88\"><span class=\"toc-text\">分布的混合</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%89%E7%94%A8%E6%80%A7%E8%B4%A8\"><span class=\"toc-text\">常用函数的有用性质</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BF%A1%E6%81%AF%E8%AE%BA\"><span class=\"toc-text\">信息论</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">结构化概率模型</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97\"><span class=\"toc-text\">数值计算</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%B8%8A%E6%BA%A2%E5%92%8C%E4%B8%8B%E6%BA%A2\"><span class=\"toc-text\">上溢和下溢</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">基于梯度的优化算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%A2%AF%E5%BA%A6%E4%B9%8B%E4%B8%8A%EF%BC%9AJacobian%E5%92%8CHessian%E7%9F%A9%E9%98%B5\"><span class=\"toc-text\">梯度之上：Jacobian和Hessian矩阵</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80\"><span class=\"toc-text\">机器学习基础</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86\"><span class=\"toc-text\">没有免费午餐定理</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BC%B0%E8%AE%A1%E3%80%81%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE\"><span class=\"toc-text\">估计、偏差和方差</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%82%B9%E4%BC%B0%E8%AE%A1\"><span class=\"toc-text\">点估计</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%81%8F%E5%B7%AE\"><span class=\"toc-text\">偏差</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1\"><span class=\"toc-text\">最大似然估计</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1\"><span class=\"toc-text\">贝叶斯统计</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">监督学习算法</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%A6%82%E7%8E%87%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">概率监督学习</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA\"><span class=\"toc-text\">支持向量机</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%85%B6%E4%BB%96%E7%AE%80%E5%8D%95%E7%9A%84%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">其他简单的监督学习算法</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">无监督学习算法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D\"><span class=\"toc-text\">随机梯度下降</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">流形学习</span></a></li></ol></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"《深度学习》 笔记（二）","uid":"e4001ef3af977cd071aff3164f892814","slug":"深度学习2","date":"2022-11-03T13:56:49.000Z","updated":"2022-11-03T13:57:08.483Z","comments":true,"path":"api/articles/深度学习2.json","keywords":null,"cover":null,"text":"第二部分 深度网络：现代实践深度前馈网络深度前馈网络，也叫做前馈神经网络或者多层感知机，是典型的深度学习模型。 这种模型被称为前向的，是因为信息流过$x$的函数，流经用于定义$f$的中间计算过程，最终到达输出$y$。在模型的输出和模型本身之间没有反馈连接。当前馈神经网络被扩展成包...","link":"","photos":[],"count_time":{"symbolsCount":"2.4k","symbolsTime":"2 mins."},"categories":[],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"Python实训题目","uid":"6165171c95ee293a7ed71a60e0ed0daa","slug":"python实训题目","date":"2022-11-03T13:52:49.000Z","updated":"2022-11-03T13:53:09.445Z","comments":true,"path":"api/articles/python实训题目.json","keywords":null,"cover":[],"text":"写在前面学校python课实训平台的oj太敏感辣，少一个空格都会报错——面向答案编程。为纪念在python实践课上花费的不多但受苦的经历，特将代码保存于博客上。宁愿刷leetcode也不愿意跑python代码 2.1 表达式和基本输入输出2.1.1 数据输入与输出 任务描述本关任...","link":"","photos":[],"count_time":{"symbolsCount":"43k","symbolsTime":"39 mins."},"categories":[],"tags":[{"name":"python","slug":"python","count":9,"path":"api/tags/python.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}