{"title":"【python机器学习基础教程】(一)","uid":"a4dfed67ba141d1dc18bc0e6d1e3a85f","slug":"python机器学习1","date":"2022-11-03T14:51:49.000Z","updated":"2022-11-08T15:54:23.151Z","comments":true,"path":"api/articles/python机器学习1.json","keywords":null,"cover":[],"content":"<p>主要使用python和scikit-learn库</p>\n<h1 id=\"必要的库和工具\"><a href=\"#必要的库和工具\" class=\"headerlink\" title=\"必要的库和工具\"></a>必要的库和工具</h1><h2 id=\"SciPy\"><a href=\"#SciPy\" class=\"headerlink\" title=\"SciPy\"></a>SciPy</h2><p>所有代码默认导入以下库：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn</code></pre>\n<p>SciPy<br>如果想保存一个大部分元素都是0的二维数组，可以使用稀疏矩阵：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from scipy import sparse\nimport numpy as np\n#创建一个二维NumPy数组，对角线为 1，其余都为0\neye &#x3D; np.eye(4)\nprint(&quot;NumPy array:\\n&#123;&#125;&quot;.format(eye))</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">NumPy array:\n[[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#将NumPy数组转换为CSR格式的SciPy稀疏矩阵\n#只保存非零元素\nsparse_matrix &#x3D; sparse.csr_matrix(eye)\nprint(&quot;\\nSciPy sparse CSR matrix:\\n&#123;&#125;&quot;.format(sparse_matrix))</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">SciPy sparse CSR matrix:\n  (0, 0)\t1.0\n  (1, 1)\t1.0\n  (2, 2)\t1.0\n  (3, 3)\t1.0</code></pre>\n<h2 id=\"matplotlib\"><a href=\"#matplotlib\" class=\"headerlink\" title=\"matplotlib\"></a>matplotlib</h2><p>matplotlib是Python主要的科学绘图库，其功能为生成可发布的可视化内容，如折线图、直方图、散点图等。<br>举例：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">%matplotlib inline\nimport matplotlib.pyplot as plt\n\n#在-10和10之间生成一个数列，共100个数\nx&#x3D;np.linspace(-10,10,100)\n#用正弦函数创建第二个数组\ny&#x3D;np.sin(x)\n#plot函数绘制一个数组关于另一个数组的折线图\nplt.plot(x,y,marker&#x3D;&quot;x&quot;)</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>需要指出的是，这个库在pycharm上并不能跑，要画图的话需要用其他库。</p></blockquote>\n<h2 id=\"pandas\"><a href=\"#pandas\" class=\"headerlink\" title=\"pandas\"></a>pandas</h2><p>pandas是用于处理和分析数据的Python库。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">import pandas as pd\nfrom IPython.display import display\n#创建关于人的简单数据集\ndata&#x3D;&#123;&#39;Name&#39;:[&quot;John&quot;,&quot;Anna&quot;,&quot;Peter&quot;,&quot;Linda&quot;],\n&#39;location&#39;:[&quot;New York&quot;,&quot;Paris&quot;,&quot;Berlin&quot;,&quot;London&quot;],\n&#39;Age&#39;:[24,13,53,33]\n&#125;\ndata_pandas&#x3D;pd.DataFrame(data)\ndisplay(data_pandas)</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">    Name  location  Age\n0   John  New York   24\n1   Anna     Paris   13\n2  Peter    Berlin   53\n3  Linda    London   33</code></pre>\n<p>查询这个表格的方法有很多种。举个例子：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#选择年龄大于30的所有行\ndisplay(data_pandas[data_pandas.Age &gt;30])</code></pre>\n<p>输出结果如下：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">    Name location  Age\n2  Peter   Berlin   53\n3  Linda   London   33</code></pre>\n<h1 id=\"第一个应用：鸢尾花分类\"><a href=\"#第一个应用：鸢尾花分类\" class=\"headerlink\" title=\"第一个应用：鸢尾花分类\"></a>第一个应用：鸢尾花分类</h1><p>我们的目标是构建一个机器学习模型，可以从这些已知品种的鸢尾花测量数据中进行学习，从而可以预测新鸢尾花的品种。</p>\n<p>因为我们有已知品种的鸢尾花的测量数据，所以这是一个监督学习问题。<br>在这个问题中，我们要在多个选项中预测其中一个（鸢尾花的品种）。这是一个<strong>分类</strong>问题。<br>可能的输出（鸢尾花的不同品种)叫做类别。<br>数据集中的每朵鸢尾花都属于三个类别之一，所以这是一个三分类问题。</p>\n<p>单个数据点（一朵鸢尾花）的预期输出是这朵花的品种。对于一个数据点来说，它的品种叫做<strong>标签</strong>。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_iris\niris_dataset&#x3D;load_iris()\nprint(&quot;Keys of iris_dataset:\\n&#123;&#125;&quot;.format(iris_dataset.keys()))</code></pre>\n<p>输出：<br><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">Keys of iris_dataset:\ndict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;data_module&#39;])</code></pre></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(iris_dataset[&#39;DESCR&#39;][:193]+&quot;\\n...&quot;)</code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, pre\n...\n</code></pre>\n<p>data数据的每一行对应一朵花，列代表每朵花的四个测量数据：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Shape of data:&#123;&#125;&quot;.format(iris_dataset[&#39;data&#39;].shape))</code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">Shape of data:(150, 4)</code></pre>\n<p>可以看出，数组中包含150朵不同的花的测量数据。</p>\n<p>机器学习中的个体叫做<strong>样本</strong>，其属性叫做<strong>特征</strong>。<br>data数组的<strong>形状</strong>（shape)是样本数乘以特征数。</p>\n<h2 id=\"训练数据与测试数据\"><a href=\"#训练数据与测试数据\" class=\"headerlink\" title=\"训练数据与测试数据\"></a>训练数据与测试数据</h2><p>我们使用新数据来评估模型的性能。<br>通常的做法是将收集好的带标签数据分成两部分，一部分用于构建机器学习模型，叫做<strong>训练数据</strong>或<strong>训练集</strong>。其余的数据用来评估模型性能，叫做<strong>测试数据</strong>、<strong>测试集</strong>或<strong>留出集</strong>。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>scikit-learn中的train_test_split函数可以打乱数据集并进行拆分。这个函数将75%的行数据及其对应标签作为训练集，剩下25%的数据及其标签作为测试集。<br>训练集和测试集的分配比例可以随意，但使用25%的数据作为测试集是很好的经验法则。</p></blockquote>\n<p>scikit-learn中的数据通常用大写的X表示，而标签用小写的y表示。我们用大写的X是因为数据是一个二维数组（矩阵），用小写的y是因为目标是一个一维数组（向量)。<br>对数据调用train_test_split，并对输出结果采用下面这种命名方法：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test&#x3D;train_test_split(iris_dataset[&#39;data&#39;],iris_dataset[&#39;target&#39;],random_state&#x3D;0)</code></pre>\n<p><strong>为了确保多次运行同一函数能够得到相同的输出，我们利用random_state参数指定了 随机数生成器的种子。这样函数输出就是固定不变的，所以这行代码的输出始终相同。</strong></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>train_test_split函数的输出为X_train、X_test、y_train和y_test，它们都是Numpy数组。X_train包含75%的行数据，X_test包含剩下 的25%：</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;X_train shape:&#123;&#125;&quot;.format(X_train.shape))\nprint(&quot;y_train shape:&#123;&#125;&quot;.format(y_train.shape))\n</code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X_train shape:(112, 4)\ny_train shape:(112,)\n</code></pre>\n<p>输入：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;X_test shape:&#123;&#125;&quot;.format(X_test.shape))\nprint(&quot;y_test shape:&#123;&#125;&quot;.format(y_test.shape))</code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X_test shape:(38, 4)\ny_test shape:(38,)\n</code></pre>\n<h2 id=\"观察数据\"><a href=\"#观察数据\" class=\"headerlink\" title=\"观察数据\"></a>观察数据</h2><p>检查数据的最佳方法之一是将其可视化。<br>一种可视化方法是绘制<strong>散点图</strong><br>数据不多——<strong>散点图矩阵</strong>，从而两两查看所有的特征。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#利用X_train中的数据创建DataFrame\n#利用iris_dataset.feature_names中的字符串对数据列进行标记\niris_dataframe&#x3D;pd.DataFrame(X_train,columns&#x3D;iris_dataset.feature_names)\n\n#利用DataFrame创建散点图矩阵，按y_train着色\ngrr&#x3D;pd.scatter_matrix(iris_dataframe,c&#x3D;y_train,figsize&#x3D;(15,15),marker&#x3D;&#39;o&#39;,hist_kwds&#x3D;&#123;&#39;bins&#39;:20&#125;,s&#x3D;60,alpha&#x3D;.8,cmap&#x3D;mglearn.cm3)</code></pre>\n<p>运行文件正式代码：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nimport matplotlib.pyplot as plt\nfrom  sklearn.datasets import load_iris\nimport pandas as pd\niris_dataset&#x3D;load_iris()\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test&#x3D;train_test_split(iris_dataset[&#39;data&#39;],iris_dataset[&#39;target&#39;],random_state&#x3D;0)\n#利用X_train中的数据创建DataFrame\n#利用iris_dataset.feature_names中的字符串对数据列进行标记\niris_dataframe&#x3D;pd.DataFrame(X_train,columns&#x3D;iris_dataset.feature_names)\n\n#利用DataFrame创建散点图矩阵，按y_train着色\ngrr&#x3D;pd.plotting.scatter_matrix(iris_dataframe,c&#x3D;y_train,figsize&#x3D;(15,15),marker&#x3D;&#39;o&#39;,hist_kwds&#x3D;&#123;&#39;bins&#39;:20&#125;,s&#x3D;60,alpha&#x3D;.8,cmap&#x3D;mglearn.cm3)\nplt.show()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/be218353c96340febf324d6a3128b6be.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Lu_55Sf56iL5bqP5ZGY5Lya5qKm6KeB55S15a2Q576K5ZCX,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"在这里插入图片描述\"><br>从图中可以看出，利用花瓣和花萼的测量数据基本可以将三个类别区分开。</p>\n<h2 id=\"k近邻算法\"><a href=\"#k近邻算法\" class=\"headerlink\" title=\"k近邻算法\"></a>k近邻算法</h2><p>K近邻分类器<br>构建此模型只需要保存训练集即可。要对一个新的数据点做出预测，算法会在训练集中寻找与这个新数据点距离最近的数据点，然后将找到的数据点的标签赋值给这个新数据点。</p>\n<p>k近邻算法中k的含义是，我们可以考虑训练集里与新数据点最近的任意k个邻居（比如最近的3个或5个邻居），而非只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>scikit-learn中所有的机器学习模型都在各自的类中实现，这些类被称为Estimator类。<br>k近邻分类算法是在neighbors模块的KNeighborsClassifier类中实现的。我们需要将这个类实例化为一个对象，然后才能使用这个模型。<br>这时我们需要设置模型的参数。<br>NeighborsClassifier最重要的参数就是邻居的数目，这里我们设为1：</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.neighbors import KNeighborsClassifier\nknn&#x3D;KNeighborsClassifier(n_neighbors&#x3D;1)</code></pre>\n<p>想要基于训练集来构建模型，需要调用knn对象的fit方法，输入参数为X_train和y_train，二者都是Numpy数组，前者包含训练数据，后者包含相应的训练标签：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">knn.fit(X_train,y_train)</code></pre>\n<h2 id=\"做出预测\"><a href=\"#做出预测\" class=\"headerlink\" title=\"做出预测\"></a>做出预测</h2><p>假设：我们在野外发现了一朵鸢尾花，花萼长5cm宽2.9cm，花瓣长1cm宽0.2cm。这朵鸢尾花属于哪个品种？我们可以将这些数据放在一个Numpy数组中，再次计算形状，样本形状为样本数（1）乘以特征数（4）：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X_new &#x3D; np.array([[5,2.9,1,0.2]])\nprint(&quot;X_new.shape:&#123;&#125;&quot;.format(X_new.shape))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>X_new.shape:(1, 4)</p></blockquote>\n<p>我们必须将这朵花的测量数据转换为二维Numpy数组的一行，这是因为scikit-learn的输入数据必须是二维数组。</p>\n<p>我们调用knn对象的predict方法来进行预测：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">prediction&#x3D;knn.predict(X_new)\nprint(&quot;Prediction:&#123;&#125;&quot;.format(prediction))\nprint(&quot;Predict target name :&#123;&#125;&quot;.format(iris_dataset[&#39;target_names&#39;][prediction]))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Prediction:[0]<br>Predict target name :[‘setosa’]</p></blockquote>\n<h2 id=\"评估模型\"><a href=\"#评估模型\" class=\"headerlink\" title=\"评估模型\"></a>评估模型</h2><p>这里需要用到之前创建的测试集，这些数据并没有用于构建模型，但我们知道测试集中每朵鸢尾花的实际品种。</p>\n<p>因此，我们可以对测试数据中的每朵鸢尾花进行预测，并将预测结果与标签（已知的品种）进行对比。<br>我们可以通过计算<strong>精度</strong>来衡量模型的优劣，精度就是品种预测正确的花所占的比例：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">y_pred&#x3D;knn.predict(X_test)\nprint(&quot;Test set prediction:\\n&#123;&#125;&quot;.format(y_pred))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set prediction:<br>[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0<br> 2]</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Test set score:&#123;:.2f&#125;&quot;.format(np.mean(y_pred&#x3D;&#x3D;y_test)))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set score:0.97</p></blockquote>\n<p>我们还可以使用knn对象的score方法来计算测试集的精度：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Test set score:&#123;:.2f&#125;&quot;.format(knn.score(X_test,y_test)))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set score:0.97</p></blockquote>\n<p>模型的精度是0.97，也就是说，对于测试集中的鸢尾花，我们的预测有97%是正确的。</p>\n<h1 id=\"监督学习\"><a href=\"#监督学习\" class=\"headerlink\" title=\"监督学习\"></a>监督学习</h1><h2 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h2><p>监督机器学习问题主要有两种，分别叫做<strong>分类</strong>和<strong>回归</strong>。</p>\n<p>分类问题的目标是预测<strong>类别标签</strong>，这些标签来自预定义的可选列表。<br>分类问题有时分为<strong>二分类</strong>和<strong>多分类</strong>。</p>\n<p>在二分类问题中，我们通常将其中一个类别称为<strong>正类</strong>，另一个类别称为<strong>反类</strong>。</p>\n<h2 id=\"泛化、过拟合与欠拟合\"><a href=\"#泛化、过拟合与欠拟合\" class=\"headerlink\" title=\"泛化、过拟合与欠拟合\"></a>泛化、过拟合与欠拟合</h2><p>如果一个模型能够对没见过的数据做出准确预测，我们就说它能够从训练集<strong>泛化</strong>到测试集。<br><strong>过拟合</strong>：训练集上表现很好，不能新数据上的模型——对现有信息量而言过于复杂<br><strong>欠拟合</strong>：过于简单的模型，甚至在训练集上表现就很差。</p>\n<h2 id=\"监督学习算法\"><a href=\"#监督学习算法\" class=\"headerlink\" title=\"监督学习算法\"></a>监督学习算法</h2><p>一个模拟的二分类数据集示例是forge数据集，它有两个特征。<br>下列代码将绘制一个散点图，将此数据集中的所有数据点可视化。<br>图像以第一个特征为x轴，第二个特征为y轴。<br>每个数据点对应图像中一点，每个点的颜色和形状对于其类别。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#生成数据集\nX,y&#x3D;mglearn.datasets.make_forge()\n#数据集绘图\nmglearn.discrete_scatter(X[:0],X[:,1],y)\nplt.legend([&quot;Class 0&quot;,&quot;Class 1&quot;],loc&#x3D;4)\nplt.xlabel(&quot;First feature&quot;)\nplt.ylabel(&quot;Second feature&quot;)\nprint(&quot;X.shape:&#123;&#125;&quot;.format(X.shape))\n\n</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>X.shape:(26,2)<br><img src=\"https://img-blog.csdnimg.cn/e35b07d6f7214ec6aca273dc9ba5d6d4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Lu_55Sf56iL5bqP5ZGY5Lya5qKm6KeB55S15a2Q576K5ZCX,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"在这里插入图片描述\"></p></blockquote>\n<p>从X.shape可以看出，这个数据集包含26个数据点和2给特征。</p>\n<p>用模拟的wave数据集来说明回归算法。<br>wave数据集只有一个输入特征和一个连续的目标变量（或响应），后者是模型想要预测的对象。<br>下面绘制的图像中单一特征位于x轴，回归目标（输出）位于y轴。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X,y&#x3D;mglearn.datasets.make_wave(n_samples&#x3D;40)\nplt.plot(X,y,&#39;o&#39;)\nplt.ylim(-3,3)\nplt.xlabel(&quot;Feature&quot;)\nplt.ylabel(&quot;Target&quot;)\nplt.show()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/d9cd061170054d2a8aa0b6df262e99ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Lu_55Sf56iL5bqP5ZGY5Lya5qKm6KeB55S15a2Q576K5ZCX,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"k近邻\"><a href=\"#k近邻\" class=\"headerlink\" title=\"k近邻\"></a>k近邻</h2><h3 id=\"k近邻分类\"><a href=\"#k近邻分类\" class=\"headerlink\" title=\"k近邻分类\"></a>k近邻分类</h3><p>k-NN算法最简单的版本只考虑一个最近邻，也就是与我们想要预测的数据点最近的训练数据点。<br>预测结果就是这个训练数据点的已知输出。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_knn_classification(n_neighbors&#x3D;1)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/019e971d6f95436ab98112c7646499fb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Lu_55Sf56iL5bqP5ZGY5Lya5qKm6KeB55S15a2Q576K5ZCX,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>除了仅考虑最近邻，还可以考虑任意个（k个）邻居。<br>3个近邻的例子：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_knn_classification(n_neighbors&#x3D;3)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/475b2c4feb3e46598f69e5e18b84f31c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Lu_55Sf56iL5bqP5ZGY5Lya5qKm6KeB55S15a2Q576K5ZCX,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center\" alt=\"在这里插入图片描述\"><br>现在看一下如何通过scikit-learn来应用k近邻算法。<br>首先，将数据分为训练集和测试集，以便评估泛化性能：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.model_selection import train_test_split\nX,y &#x3D; mglearn.datasets.make_forge()\n\nX_train,X_test,y_train,y_test &#x3D; train_test_split(X,y,random_state&#x3D;0)</code></pre>\n<p>然后，导入类并将其实例化。<br>这时可以设置参数，比如邻居的个数。<br>这里我们将其设为3：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.neighbors import KNeighborsClassifier\nclf&#x3D;KNeighborsClassifier(n_neighbors&#x3D;3)</code></pre>\n<p>现在，利用训练集对分类器进行拟合。<br>对于KNeighborsClassifier来说就是保存数据，以便在预测时计算与邻居之间的距离：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">clf.fit(X_train,y_train)</code></pre>\n<p>调用predict方法来对测试数据进行预测。<br>对于测试集中的每个数据点，都要计算它在训练集的最近邻，然后找出其中出现次数最多的类别。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Test set predictions :&#123;&#125;&quot;.format(clf.predict(X_test)))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set predictions :[1 0 1 0 1 0 0]</p></blockquote>\n<p>为了评估模型的泛化能力好坏，我们可以对测试数据和测试标签调用score方法:</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Test set accuracy :&#123;:.2f&#125;&quot;.format(clf.score(X_test,y_test)))</code></pre>\n<p>输出：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set accuracy :0.86</p></blockquote>\n<p>模型精度约为86%</p>\n<h3 id=\"分析KNeighborsClassifier\"><a href=\"#分析KNeighborsClassifier\" class=\"headerlink\" title=\"分析KNeighborsClassifier\"></a>分析KNeighborsClassifier</h3><p>对于二维数据集，我们还可以在xy平面上画出所有可能的测试点的预测结果。<br>我们根据平面中每个点所属的类别对平面进行着色。<br>这样可以查看<strong>决策边界</strong>，即算法对类别0和类别1的分界线。</p>\n<p>下面代码分别将1个、3个和9个邻居三种情况的决策边界可视化。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">fig ,axes &#x3D; plt.subplots(1,3,figsize&#x3D;(10,3))\n\nfor n_neighbors,ax in zip([1,3,9],axes):\n   clf &#x3D; KNeighborsClassifier(n_neighbors&#x3D;n_neighbors).fit(X,y)\n   mglearn.plots.plot_2d_separator(clf,X,fill&#x3D;True,eps&#x3D;0.5,ax&#x3D;ax,alpha&#x3D;.4)\n   mglearn.discrete_scatter(X[:,0],X[:,1],y,ax&#x3D;ax)\n   ax.set_title(&quot;&#123;&#125;neighbor(s)&quot;.format(n_neighbors))\n   ax.set_xlabel(&quot;feature 0&quot;)\n   ax.set_ylabel(&quot;feature 1&quot;)\naxes[0].legend(loc&#x3D;3)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/b6ad727b95a34d3c96d84e5f6f320117.png#pic_center\" alt=\"在这里插入图片描述\"><br> 从左图可以看出，使用单一邻居绘制的决策边界紧跟着训练数据。随着邻居个数越来越多，决策边界也越来越平滑。<br> 使用更少的邻居对应更高的模型复杂度。</p>\n<p>我们来研究一下是否能证实模型复杂度和泛化能力之间的关系。<br>我们将在现实世界的乳腺癌数据集上进行研究。<br>先将数据集分成训练集和测试集，然后用不同的邻居个数对训练集和测试集的性能进行评估。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_breast_cancer\n\ncancer &#x3D; load_breast_cancer()\nX_train,X_test,y_train,y_test&#x3D;train_test_split(cancer.data,cancer.target,stratify&#x3D;cancer.target,random_state&#x3D;66)\ntraining_accuracy&#x3D;[]\ntest_accuracy&#x3D;[]\nneighbors_settings&#x3D;range(1,11)\n\nfor n_neighbors in  neighbors_settings:\n  #构建模型\n  clf&#x3D;KNeighborsClassifier(n_neighbors&#x3D;n_neighbors)\n  clf.fit(X_train,y_train)\n  #记录数据集精度\n  training_accuracy.append(clf.score(X_train,y_train))\n  #记录泛化精度\n  test_accuracy.append(clf.score(X_test,y_test))\nplt.plot(neighbors_settings,training_accuracy,label&#x3D;&quot;training accuracy&quot;)\nplt.plot(neighbors_settings,test_accuracy,label&#x3D;&quot;test accuracy&quot;)\nplt.ylabel(&quot;Accuracy&quot;)\nplt.xlabel(&quot;n_neighbors&quot;)\nplt.legend()\nplt.show()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/346b6294c7e646e09d74808fbc1b83b9.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"k近邻回归\"><a href=\"#k近邻回归\" class=\"headerlink\" title=\"k近邻回归\"></a>k近邻回归</h3><p>k近邻算法可以用于回归。<br>我们从单一近邻开始，这次使用wave数据集。<br>我们添加了3个测试数据点，在x轴上用绿色五角星表示。<br>利用单一邻居的预测结果就是最近邻的目标值。在图中用蓝色五角星表示：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_knn_regression(n_neighbors&#x3D;1)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/42dd50bb96444ba1987976ae0f1167eb.png#pic_center\" alt=\"在这里插入图片描述\"><br>同样，也可以用多个近邻进行回归。<br>在使用多个近邻时，预测结果为这些邻居的平均值：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_knn_regression(n_neighbors&#x3D;3)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/8095610d518c4765b785dedee30f6c28.png#pic_center\" alt=\"在这里插入图片描述\"><br>用于回归的k近邻算法在scikit-learn的KNeighborsRegressor类中实现。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.neighbors import KNeighborsRegressor\nX,y&#x3D;mglearn.datasets.make_wave(n_samples&#x3D;40)\n#将wave数据集分为训练集和测试集\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X,y,random_state&#x3D;0)\n\n#模型实例化，并将邻居个数设为3\nreg&#x3D;KNeighborsRegressor(n_neighbors&#x3D;3)\n#利用训练数据和训练目标值来拟合模型\nreg.fit(X_train,y_train)\n</code></pre>\n<p>现在可以对测试集进行预测：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Test set predictions:\\n&#123;&#125;&quot;.format(reg.predict(X_test)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set predictions:<br>[-0.05396539  0.35686046  1.13671923 -1.89415682 -1.13881398 -1.63113382<br>0.35686046  0.91241374 -0.44680446 -1.13881398]</p></blockquote>\n<p>我们还可以用score方法来 评估模型，对于回归问题，这一方法返回的是$R^2$分数。$R^2$分数也叫做决定系数，是回归模型预测的优先度量，位于0到1之间。$R^2=1$对应完美预测，$R^2=0$对应常数模型，即总是预测训练集响应（y_train）的平均值：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Test set R^2:&#123;:.2f&#125;&quot;.format(reg.score(X_test,y_test)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set R^2:0.83</p></blockquote>\n<h3 id=\"分析KNeighborsRegressor\"><a href=\"#分析KNeighborsRegressor\" class=\"headerlink\" title=\"分析KNeighborsRegressor\"></a>分析KNeighborsRegressor</h3><p>对于我们的一维数据集，可以查看所有特征值对应的预测结果。<br>为了便于绘图，我们创建一个由许多点组成的测试数据集：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">fig,axes&#x3D;plt.subplots(1,3,figsize&#x3D;(15,4))\n#创建1000个数据点，在-3和3之间均匀分布\nline&#x3D;np.linspace(-3,3,1000).reshape(-1,1)\nfor n_neighbors,ax in zip([1,3,9],axes):\n#利用1个，3个或9个邻居分别进行预测\n   reg&#x3D;KNeighborsRegressor(n_neighbors&#x3D;n_neighbors)\n   reg.fit(X_train,y_train)\n   ax.plot(line,reg.predict(line))\n   ax.plot(X_train,y_train,&#39;^&#39;,c&#x3D;mglearn.cm2(0),markersize&#x3D;8)\n   ax.plot(X_test,y_test,&#39;v&#39;,c&#x3D;mglearn.cm2(1),markersize&#x3D;8)\n   ax.set_title(\n       &quot;&#123;&#125;neighbor(s)\\n train score:&#123;:.2f&#125; test score:&#123;:.2f&#125;&quot;.format(\n     n_neighbors,reg.score(X_train,y_train),\n     reg.score(X_test,y_test)))\n   ax.set_xlabel(&quot;Feature&quot;)\n   ax.set_ylabel(&quot;Target&quot;)\naxes[0].legend([&quot;Model predictions&quot;,&quot;Training data&#x2F;target&quot;,&quot;Test data&#x2F;target&quot;],loc&#x3D;&quot;best&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/4b16e10c990b424ba236f2b568d4735d.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"线性模型\"><a href=\"#线性模型\" class=\"headerlink\" title=\"线性模型\"></a>线性模型</h2><p>线性模型利用输入特征的<strong>线性函数</strong>进行预测。</p>\n<h3 id=\"用于回归的线性模型\"><a href=\"#用于回归的线性模型\" class=\"headerlink\" title=\"用于回归的线性模型\"></a>用于回归的线性模型</h3><p>对于回归问题，线性模型预测的一般公式如下：<br>$\\hat{y}=w[0]<em>x[0]+w[1]</em>x[1]+···+w[p]*x[p]+b$<br>这里$x[0]$到$x[p]$表示单个数据点的特征（本例中特征个数为p+1），$w$和$b$是学习模型的参数，$\\hat{y}$是模型的预测结果。</p>\n<p>对于单一特征的数据集，公式如下：<br>$\\hat{y}=w[0]*x[0]+b$</p>\n<p>下列代码可以在一维wave数据集上学习参数$w[0]$和$b$</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_linear_regression_wave()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/1256637674d04e299f60dac4e897e180.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>w[0]: 0.393906  b: -0.031804</p></blockquote>\n<h3 id=\"线性回归（最小二乘法）\"><a href=\"#线性回归（最小二乘法）\" class=\"headerlink\" title=\"线性回归（最小二乘法）\"></a>线性回归（最小二乘法）</h3><p>线性回归，或者<strong>普通最小二乘法</strong>，是回归问题最简单也是最经典的线性方法。线性回归寻找参数$w$和$b$，使得对训练集的预测值与真实的回归目标值$y$之间的<strong>均方误差</strong>最小。</p>\n<p>均方误差是预测值与真实值之差的平方和除以样本数。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.linear_model import LinearRegression\nX,y&#x3D;mglearn.datasets.make_wave(n_samples&#x3D;60)\nX_train,X_tset,y_train,y_test&#x3D;train_test_split(X,y,random_state&#x3D;42)\n\nlr&#x3D;LinearRegression().fit(X_train,y_train)</code></pre>\n<p>斜率$w$（也叫做权重或<strong>系数</strong>），被保存在coef_属性中，<br>而偏移或<strong>截距</strong>（$b$）被保存在intercept_属性中。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;lr.coef_:&#123;&#125;&quot;.format(lr.coef_))\nprint(&quot;lr.intercept_:&#123;&#125;&quot;.format(lr.intercept_))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>lr.coef_:[0.39390555]<br>lr.intercept_:-0.031804343026759746</p></blockquote>\n<h3 id=\"岭回归\"><a href=\"#岭回归\" class=\"headerlink\" title=\"岭回归\"></a>岭回归</h3><p>岭回归也是一种用于回归的线性模型，因此他的预测公式与普通最小二乘法相同。<br>在岭回归中，对系数的选择不仅要在训练集上得到好的预测效果，还要拟合附加约束。我们还希望系数尽量小。</p>\n<p>正则化是指对模型做显式约束，避免过拟合。<br>岭回归用到的这种被称为L2正则化。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_ridge_n_samples()</code></pre>\n<p>岭回归和线性回归在波士顿房价数据集上的学习曲线<br><img src=\"https://img-blog.csdnimg.cn/b599cbe6d36248e28ad22210dfe987e5.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"lasso\"><a href=\"#lasso\" class=\"headerlink\" title=\"lasso\"></a>lasso</h3><p>除了Ridge，还有一种正则化的线性回归是lasso。<br>与岭回归相同，使用lasso也是约束系数使其接近于0，但用的是L1正则化。<br>L1正则化的结果是，使用lasso时某些系数刚好为0.</p>\n<p>在实践中，两个模型中一般首选岭回归。但如果特征很多，你认为只有其中几个是重要的，那选择lasso可能更好。</p>\n<h3 id=\"用于分类的线性模型\"><a href=\"#用于分类的线性模型\" class=\"headerlink\" title=\"用于分类的线性模型\"></a>用于分类的线性模型</h3><p>对于用于分类的线性模型，<strong>决策边界</strong>是输入的线性函数。<br>换句话说，（二元）线性分类器是利用直线、平面或超平面来分开两个类别的分类器。</p>\n<p>学习线性模型有很多种算法。这些算法的区别在于以下两点：</p>\n<ul>\n<li>系数和截距的特定组合对训练数据拟合好坏的度量方法；</li>\n<li>是否使用正则化，以及使用哪种正则化方法。</li>\n</ul>\n<p>最常见的两种线性分类算法是<strong>Logistic回归</strong>和<strong>线性支持向量机</strong>。<br>前者在linear_model.LogisticRegression中实现，后者在svm.LinearSVC中实现。</p>\n<p>我们可将LogisticRegression和LinearSVC模型 应用到forge数据集上，并将线性模型找到的决策边界可视化。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nX,y &#x3D;mglearn.datasets.make_forge()\n\nfig,axes&#x3D;plt.subplots(1,2,figsize&#x3D;(10,3))\n\nfor model ,ax in zip([LinearSVC(),LogisticRegression()],axes):\n    clf &#x3D; model.fit(X,y)\n    mglearn.plots.plot_2d_separator(clf,X,fill&#x3D;False,eps&#x3D;0.5,ax&#x3D;ax,alpha&#x3D;.7)\n    mglearn.discrete_scatter(X[:,0],X[:,1],y,ax&#x3D;ax)\n    ax.set_title(&quot;&#123;&#125;&quot;.format(clf._class_._name_))\n    ax.set_xlabel(&quot;Feature 0&quot;)\n    ax.set_ylabel(&quot;Feature 1&quot;)\naxes[0].legend()\n        </code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/28efe71608544b74bd77602839159d52.png#pic_center\" alt=\"在这里插入图片描述\"><br>在这张图中，forge数据集的第一个特征位于x轴，第二个特征位于y轴。<br>图中分别展示了LinearSVC和LogisticRegression得到的决策边界，都是直线，将顶部归为类别1的区域和底部归为类别0 的区域分开了。<br>换句话说，对于每个分类器而言，位于黑线上方的新数据点都会被划为类别1，而在黑线下方的点都会被划为类别0.</p>\n<p>对于上述二者，决定正则化强度的权衡参数叫做C。C值越大，对应的正则化<strong>越弱</strong>。<br>换句话说，如果参数C值较大，二者将尽可能将训练集拟合到最好，而如果C值较小，那么模型更强调使系数向量（$w$）接近于0 .</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_linear_svc_regularization()</code></pre>\n<p>不同C值的线性SVM在forge数据集上的决策边界<br><img src=\"https://img-blog.csdnimg.cn/73aef0da63c840c2bb456d2dab0b58e2.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们在乳腺癌数据集上详细分析LogisticRegression</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_breast_cancer\ncancer&#x3D;load_breast_cancer()\nX_train,X_test,y_train,y_test&#x3D;train_test_split(cancer.data,cancer.target,stratify&#x3D;cancer.target,random_state&#x3D;42)\nlogreg&#x3D;LogisticRegression().fit(X_train,y_train)\n\nlogreg100&#x3D;LogisticRegression(C&#x3D;100).fit(X_train,y_train)\n\nlogreg001&#x3D;LogisticRegression(C&#x3D;0.01).fit(X_train,y_train)\n\nplt.plot(logreg.coef_.T,&#39;o&#39;,label&#x3D;&quot;C&#x3D;1&quot;)\nplt.plot(logreg100.coef_.T,&#39;^&#39;,label&#x3D;&quot;C&#x3D;100&quot;)\nplt.plot(logreg001.coef_.T,&#39;v&#39;,label&#x3D;&quot;C&#x3D;0.001&quot;)\nplt.xticks(range(cancer.data.shape[1]),cancer.feature_names,rotation&#x3D;90)\nplt.hlines(0,0,cancer.data.shape[1])\nplt.ylim(-5,5)\nplt.xlabel(&quot;Coefficient index&quot;)\nplt.ylabel(&quot;Coefficient magnitude&quot;)\nplt.legend()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/db8157e2c7774b69b55b8903c020abeb.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"用于多分类的线性模型\"><a href=\"#用于多分类的线性模型\" class=\"headerlink\" title=\"用于多分类的线性模型\"></a>用于多分类的线性模型</h3><p>将二分类算法推广到多分类算法的一种常见方法是”<strong>一对其余</strong>“方法。<br>在这个方法中，对每个类别都学习一个二分类模型，将这个类别与所有其他类别尽量分开，这样就生成了与类别个数一样多的二分类模型。<br>在测试点运行所有二类分类器来进行预测。<br>在对应类别上分数最高的分类器”胜出”,将这个类别标签返回作为预测结果。</p>\n<p>我们将”一对其余“方法应用在一个简单的三分类数据集上。<br>我们用到了一个二维数据集，每个类别的数据都是从一个高斯分布中采样得出的。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import make_blobs\nX,y&#x3D; make_blobs(random_state&#x3D;42)\n\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)\nplt.legend([&quot;Class 0&quot;,&quot;Class 1&quot;,&quot;Class 2&quot;])</code></pre>\n<p>包含3个类别的二位玩具数据集：<br><img src=\"https://img-blog.csdnimg.cn/6c4075b0c4784b7db920b1ef494916dc.png#pic_center\" alt=\"在这里插入图片描述\"><br>现在，在这个数据集上训练一个LinearSVC分类器：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">linear_svm&#x3D;LinearSVC().fit(X,y)\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\nline&#x3D;np.linspace(-15,15)\nfor coef,intercept,color in zip(linear_svm.coef_,linear_svm.intercept_,[&#39;b&#39;,&#39;r&#39;,&#39;g&#39;]):\n   plt.plot(line,-(line*coef[0]+intercept)&#x2F;coef[1],c&#x3D;color)\nplt.ylim(-10,15)\nplt.xlim(-10,8)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)\nplt.legend([&#39;Class 0&#39;,&#39;Class 1&#39;,&#39;Class 2&#39;,&#39; Line Class 0&#39;,&#39; Line Class 1&#39;,&#39; Line Class 2&#39;],loc&#x3D;(1.01,0.3))\n\n</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/ecc58874e07b439bbb071caea38f3d21.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_2d_classification(linear_svm,X,fill&#x3D;True,alpha&#x3D;.7)\nmglearn.discrete_scatter(X[:,0],X[:,1],y)\nline&#x3D;np.linspace(-15,15)\nfor coef,intercept,color in zip(linear_svm.coef_,linear_svm.intercept_,[&#39;b&#39;,&#39;r&#39;,&#39;g&#39;]):\n   plt.plot(line,-(line*coef[0]+intercept)&#x2F;coef[1],c&#x3D;color)\nplt.ylim(-10,15)\nplt.xlim(-10,8)\nplt.xlabel(&quot;Feature 0&quot;)\nplt.ylabel(&quot;Feature 1&quot;)\nplt.legend([&#39;Class 0&#39;,&#39;Class 1&#39;,&#39;Class 2&#39;,&#39; Line Class 0&#39;,&#39; Line Class 1&#39;,&#39; Line Class 2&#39;],loc&#x3D;(1.01,0.3))</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/334de8d0e87943f1858b2221c7631f24.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n","text":"主要使用python和scikit-learn库 必要的库和工具SciPy所有代码默认导入以下库： import numpy as np import matplotlib.pyplot as plt import pandas as pd import mglearn SciP...","link":"","photos":[],"count_time":{"symbolsCount":"18k","symbolsTime":"16 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":9,"path":"api/categories/机器学习.json"}],"tags":[{"name":"python","slug":"python","count":9,"path":"api/tags/python.json"},{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%BF%85%E8%A6%81%E7%9A%84%E5%BA%93%E5%92%8C%E5%B7%A5%E5%85%B7\"><span class=\"toc-text\">必要的库和工具</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#SciPy\"><span class=\"toc-text\">SciPy</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#matplotlib\"><span class=\"toc-text\">matplotlib</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#pandas\"><span class=\"toc-text\">pandas</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%EF%BC%9A%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB\"><span class=\"toc-text\">第一个应用：鸢尾花分类</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE\"><span class=\"toc-text\">训练数据与测试数据</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%A7%82%E5%AF%9F%E6%95%B0%E6%8D%AE\"><span class=\"toc-text\">观察数据</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">k近邻算法</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%81%9A%E5%87%BA%E9%A2%84%E6%B5%8B\"><span class=\"toc-text\">做出预测</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">评估模型</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">监督学习</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">分类与回归</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%B3%9B%E5%8C%96%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88\"><span class=\"toc-text\">泛化、过拟合与欠拟合</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95\"><span class=\"toc-text\">监督学习算法</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#k%E8%BF%91%E9%82%BB\"><span class=\"toc-text\">k近邻</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#k%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB\"><span class=\"toc-text\">k近邻分类</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%88%86%E6%9E%90KNeighborsClassifier\"><span class=\"toc-text\">分析KNeighborsClassifier</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#k%E8%BF%91%E9%82%BB%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">k近邻回归</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%88%86%E6%9E%90KNeighborsRegressor\"><span class=\"toc-text\">分析KNeighborsRegressor</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">线性模型</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%94%A8%E4%BA%8E%E5%9B%9E%E5%BD%92%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">用于回归的线性模型</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%EF%BC%89\"><span class=\"toc-text\">线性回归（最小二乘法）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%B2%AD%E5%9B%9E%E5%BD%92\"><span class=\"toc-text\">岭回归</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#lasso\"><span class=\"toc-text\">lasso</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">用于分类的线性模型</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%94%A8%E4%BA%8E%E5%A4%9A%E5%88%86%E7%B1%BB%E7%9A%84%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">用于多分类的线性模型</span></a></li></ol></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"【GO语言编程】（二）","uid":"5c84b4745a97bb70561ce70d97fa094d","slug":"GO2","date":"2022-11-03T14:53:49.000Z","updated":"2022-11-08T15:50:29.945Z","comments":true,"path":"api/articles/GO2.json","keywords":null,"cover":[],"text":"第一个go程序注释package main &#x2F;&#x2F; 声明 main 包 import &quot;fmt&quot; &#x2F;&#x2F; 导入 fmt 包，打印字符串时需要用到 func main()&#123; &#x2F;&#x2F; 声明 main ...","link":"","photos":[],"count_time":{"symbolsCount":"41k","symbolsTime":"37 mins."},"categories":[{"name":"编程语言","slug":"编程语言","count":13,"path":"api/categories/编程语言.json"}],"tags":[{"name":"GO","slug":"GO","count":4,"path":"api/tags/GO.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"【Go语言编程】（一）","uid":"355b3b6cd1787b40c18a74c859cf58d1","slug":"GO1","date":"2022-11-03T14:50:49.000Z","updated":"2022-11-08T15:50:20.238Z","comments":true,"path":"api/articles/GO1.json","keywords":null,"cover":[],"text":"w3c环境搭建 w3cschool第一个go程序配置路径： $env:Path &#x3D; [System.Environment]::GetEnvironmentVariable(&quot;Path&quot;,&quot;Machine&quot;) package ma...","link":"","photos":[],"count_time":{"symbolsCount":"15k","symbolsTime":"14 mins."},"categories":[{"name":"编程语言","slug":"编程语言","count":13,"path":"api/categories/编程语言.json"}],"tags":[{"name":"GO","slug":"GO","count":4,"path":"api/tags/GO.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}