{"title":"【计算与人工智能概论】（进阶）","uid":"15c6023f64ef1e6727ffc77c5b7783db","slug":"计算与人工智能概论进阶","date":"2022-11-03T14:40:49.000Z","updated":"2022-11-03T14:40:09.552Z","comments":true,"path":"api/articles/计算与人工智能概论进阶.json","keywords":null,"cover":[],"content":"<h1 id=\"算法思维\"><a href=\"#算法思维\" class=\"headerlink\" title=\"算法思维\"></a>算法思维</h1><h2 id=\"方程求根\"><a href=\"#方程求根\" class=\"headerlink\" title=\"方程求根\"></a>方程求根</h2><h3 id=\"二分法\"><a href=\"#二分法\" class=\"headerlink\" title=\"二分法\"></a>二分法</h3><p>画图猜f(x)=0的大概范围[a,b]再缩小范围，保证f(a)f(b)&lt;0,在[a,b]上一定有实根。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/7ddb03459ceb4c69b2b1bd60c3b0bdf1.png\" alt=\"在这里插入图片描述\"><br>中点x0=a+(b-a)/2=(a+b)/2  即两端值和的一半。若中点与右端符号相同(如左图)，则缩小到[a,x0]。若中点与左端符号相同(如右图)，则缩小到[x0,b],依次得到不断减半的区间。<br>最终区间[an,bn]的长度=(b-a)/2n,称为解方程的二分法。</p>\n<p>算法流程：</p>\n<ol>\n<li>画图得到根初始区间left=1,right=2，f(left)f(right)&lt;0</li>\n<li>计算中点middle,及中间f(middle)</li>\n<li>当|f(middle)|&gt;=error时： 如果f(middle)与f(left)的符号相同，则解在[中,右]，否则解在[左,中]。相应改变区间。</li>\n<li>计算中点middle，及f(middle)</li>\n<li>回到第3步</li>\n</ol>\n<p><img src=\"https://img-blog.csdnimg.cn/ecc80228c7874c7f9c72864ad13768c1.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#例1：求x2-2&#x3D;0的根，即求2的平方根\ndef f(x):  return x**2-2\nleft&#x3D;1;right&#x3D;2;n&#x3D;0#n用来统计二分次数\nerr&#x3D;1e-6#误差\nwhile True:\n    n+&#x3D;1; middle&#x3D;(left+right)*0.5\n    if abs(f(middle))&lt;err:\n        print(&#39;f(x)&#x3D;%.8f x&#x3D;%.8f n&#x3D;%d&#39;%(f(middle),middle,n)) \n        break\n    if f(middle)*f(left)&gt;0:  left&#x3D;middle #中点变为左边界\n    else: right&#x3D;middle #中点变为右边界\n</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>f(x)=0.00000027 x=1.41421366 n=21</p></blockquote>\n<h3 id=\"牛顿迭代法\"><a href=\"#牛顿迭代法\" class=\"headerlink\" title=\"牛顿迭代法\"></a>牛顿迭代法</h3><p><img src=\"https://img-blog.csdnimg.cn/aa1a828b3be0407390030b08fce6858b.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/2c29da5675424038a7bf088ef4788947.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/55c6026cbeb245d3b2ed2f33c3171644.png\" alt=\"在这里插入图片描述\"><br>注意，如果初值选取不当，可能越过根（漏掉根），尽量依据绘图，选取根附近的点作为初始点。</p>\n<p><strong>算法流程：</strong></p>\n<ol>\n<li><strong>画图得到根的初始值x</strong></li>\n<li><strong>求出函数的导数，计算f(x), f’(x) 。</strong></li>\n<li><strong>当|f(x)|&gt;=err时，重复以下：</strong><br>   <strong>x=x-f(x)/f’(x)<br>   计算f(x), f’(x)</strong><pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#例1：求x2-2&#x3D;0的根，即求2的平方根\ndef f(x):\n    return x ** 2 -2\ndef df(x):\n    return x * 2 \nx&#x3D;4;err&#x3D;1e-6;n&#x3D;0\nwhile True:\n    if abs(f(x))&lt;err:\n        print(&#39;fx&#x3D;%.8f x&#x3D;%.8f n&#x3D;%d&#39;%(f(x),x,n))\n        break\n    x&#x3D;x-f(x)&#x2F;df(x) #\n    n+&#x3D;1</code></pre>\n</li>\n</ol>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>fx=0.00000000 x=1.41421356 n=5</p></blockquote>\n<h3 id=\"离散Newton法\"><a href=\"#离散Newton法\" class=\"headerlink\" title=\"离散Newton法\"></a>离散Newton法</h3><p>离散Newton法，亦称割线法。<br><img src=\"https://img-blog.csdnimg.cn/ffeb3f70fc9d443b993f4197686248c8.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/55c23ebd08d944978a8bf0b04b8d318a.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/16a5f11a9c8546379c08e31d8391f521.png\" alt=\"在这里插入图片描述\"><br>（1）画图得到根附近的一对值，x0,x1<br>（2）计算 f0=f(x0),f1=f(x1)<br>（3）当|f(x0)|&gt;=err 时：<br>            x=x1-f(x1)*(x1-x0)/(f(x1)-f(x0))<br>            x0=x1<br>         x1=x<br>f0=f1<br>f1=f(x1)</p>\n<p><img src=\"https://img-blog.csdnimg.cn/c4706ec3602245bf9f0db940ed20177d.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#例1：求x2-2&#x3D;0的根，即求2的平方根\nx0&#x3D;1;x1&#x3D;2;err&#x3D;1e-6;n&#x3D;0\ndef f(x):\n    return x**2-2\nwhile True:\n    if f(x1)-f(x0)&#x3D;&#x3D;0:\n        print(&#39;False&#39;)\n        break\n    x&#x3D;x1-f(x1)&#x2F;(f(x1)-f(x0))*(x1-x0)\n    n+&#x3D;1\n    if abs(f(x))&lt;err:\n        print(&#39;fx&#x3D;%.8f x&#x3D;%.8f n&#x3D;%d&#39;%(f(x),x,n))\n        break\n    x0&#x3D;x1\n    x1&#x3D;x</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>fx=-0.00000000 x=1.41421356 n=5</p></blockquote>\n<h2 id=\"函数求最值-梯度下降法\"><a href=\"#函数求最值-梯度下降法\" class=\"headerlink\" title=\"函数求最值(梯度下降法)\"></a>函数求最值(梯度下降法)</h2><h3 id=\"梯度下降法，一元函数\"><a href=\"#梯度下降法，一元函数\" class=\"headerlink\" title=\"梯度下降法，一元函数\"></a>梯度下降法，一元函数</h3><p>求函数最值是应用数学最重要的任务之一。</p>\n<p>梯度下降法是最早最简单，也是最为常用的最优化方法。大部分的机器学习算法的本质都是先建立初始模型，然后通过最优化方法对目标函数(损失函数)进行优化，从而训练出最好的模型。</p>\n<p>梯度下降法(亦称最速下降法)与Newton迭代法的相似之处在于都利用了函数求导。</p>\n<p><strong>梯度下降法</strong>：找到f’(x)=0的点<br>然而，在实际问题中，方程f’(x)=0通常不容易求解，特别是当有多个自变量时。梯度下降法不直接求方程f’(x)=0的解，而是通过逐步改变x值进行摸索，从而找出函数的最小值。<br><img src=\"https://img-blog.csdnimg.cn/19ba5a86606344d2a8539d86da5f85f3.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/17229835194d49468dab39098a2259d3.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/31c3cacf39484ca2a68bb227649aecbb.png\" alt=\"在这里插入图片描述\"><br>为了找到函数的最小值处，只要让函数值朝着梯度方向的反方向(下降)走一小步，再求出此处的梯度方向，继续往梯度方向的反方向走一小步，如此往复，就能找到函数的最小值。</p>\n<p>算法流程：</p>\n<ol>\n<li>随机生成初始x值</li>\n<li>循环迭代max_iters次<br>   当|f’(x)|&lt;err时,跳出循环<br>   x=x-f’(x)*learning_rate</li>\n</ol>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#例2：求函数y&#x3D;x2-10x-30  的最小值\nimport numpy as np\nnp.random.seed(3)\nx&#x3D;np.random.randn();learning_rate&#x3D;0.1\nerr&#x3D;0.000001;max_iters&#x3D;10000 \ndef f(x):  return x**2-10*x-30   \ndef df(x):  return 2*x-10\nfor i in range(max_iters):\n    print(&quot;第 %d 次迭代：x&#x3D;%.8f y&#x3D;%.8f&quot;%(i,x,f(x))) \n    if abs(df(x)) &lt;err:      break \n    x&#x3D;x-df(x) * learning_rate#xk+1&#x3D;xk- η* f’(xk)  (迭代公式)\n</code></pre>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">第 0 次迭代：x&#x3D;1.78862847 y&#x3D;-44.68709292\n第 1 次迭代：x&#x3D;2.43090278 y&#x3D;-48.39973947\n第 2 次迭代：x&#x3D;2.94472222 y&#x3D;-50.77583326\n第 3 次迭代：x&#x3D;3.35577778 y&#x3D;-52.29653329\n第 4 次迭代：x&#x3D;3.68462222 y&#x3D;-53.26978130\n第 5 次迭代：x&#x3D;3.94769778 y&#x3D;-53.89266003\n第 6 次迭代：x&#x3D;4.15815822 y&#x3D;-54.29130242\n第 7 次迭代：x&#x3D;4.32652658 y&#x3D;-54.54643355\n第 8 次迭代：x&#x3D;4.46122126 y&#x3D;-54.70971747\n第 9 次迭代：x&#x3D;4.56897701 y&#x3D;-54.81421918\n第 10 次迭代：x&#x3D;4.65518161 y&#x3D;-54.88110028\n第 11 次迭代：x&#x3D;4.72414529 y&#x3D;-54.92390418\n第 12 次迭代：x&#x3D;4.77931623 y&#x3D;-54.95129867\n第 13 次迭代：x&#x3D;4.82345298 y&#x3D;-54.96883115\n第 14 次迭代：x&#x3D;4.85876239 y&#x3D;-54.98005194\n第 15 次迭代：x&#x3D;4.88700991 y&#x3D;-54.98723324\n第 16 次迭代：x&#x3D;4.90960793 y&#x3D;-54.99182927\n第 17 次迭代：x&#x3D;4.92768634 y&#x3D;-54.99477073\n第 18 次迭代：x&#x3D;4.94214907 y&#x3D;-54.99665327\n第 19 次迭代：x&#x3D;4.95371926 y&#x3D;-54.99785809\n第 20 次迭代：x&#x3D;4.96297541 y&#x3D;-54.99862918\n第 21 次迭代：x&#x3D;4.97038033 y&#x3D;-54.99912267\n第 22 次迭代：x&#x3D;4.97630426 y&#x3D;-54.99943851\n第 23 次迭代：x&#x3D;4.98104341 y&#x3D;-54.99964065\n第 24 次迭代：x&#x3D;4.98483473 y&#x3D;-54.99977001\n第 25 次迭代：x&#x3D;4.98786778 y&#x3D;-54.99985281\n第 26 次迭代：x&#x3D;4.99029423 y&#x3D;-54.99990580\n第 27 次迭代：x&#x3D;4.99223538 y&#x3D;-54.99993971\n第 28 次迭代：x&#x3D;4.99378830 y&#x3D;-54.99996141\n第 29 次迭代：x&#x3D;4.99503064 y&#x3D;-54.99997531\n第 30 次迭代：x&#x3D;4.99602451 y&#x3D;-54.99998420\n第 31 次迭代：x&#x3D;4.99681961 y&#x3D;-54.99998989\n第 32 次迭代：x&#x3D;4.99745569 y&#x3D;-54.99999353\n第 33 次迭代：x&#x3D;4.99796455 y&#x3D;-54.99999586\n第 34 次迭代：x&#x3D;4.99837164 y&#x3D;-54.99999735\n第 35 次迭代：x&#x3D;4.99869731 y&#x3D;-54.99999830\n第 36 次迭代：x&#x3D;4.99895785 y&#x3D;-54.99999891\n第 37 次迭代：x&#x3D;4.99916628 y&#x3D;-54.99999930\n第 38 次迭代：x&#x3D;4.99933302 y&#x3D;-54.99999956\n第 39 次迭代：x&#x3D;4.99946642 y&#x3D;-54.99999972\n第 40 次迭代：x&#x3D;4.99957314 y&#x3D;-54.99999982\n第 41 次迭代：x&#x3D;4.99965851 y&#x3D;-54.99999988\n第 42 次迭代：x&#x3D;4.99972681 y&#x3D;-54.99999993\n第 43 次迭代：x&#x3D;4.99978145 y&#x3D;-54.99999995\n第 44 次迭代：x&#x3D;4.99982516 y&#x3D;-54.99999997\n第 45 次迭代：x&#x3D;4.99986013 y&#x3D;-54.99999998\n第 46 次迭代：x&#x3D;4.99988810 y&#x3D;-54.99999999\n第 47 次迭代：x&#x3D;4.99991048 y&#x3D;-54.99999999\n第 48 次迭代：x&#x3D;4.99992838 y&#x3D;-54.99999999\n第 49 次迭代：x&#x3D;4.99994271 y&#x3D;-55.00000000\n第 50 次迭代：x&#x3D;4.99995417 y&#x3D;-55.00000000\n第 51 次迭代：x&#x3D;4.99996333 y&#x3D;-55.00000000\n第 52 次迭代：x&#x3D;4.99997067 y&#x3D;-55.00000000\n第 53 次迭代：x&#x3D;4.99997653 y&#x3D;-55.00000000\n第 54 次迭代：x&#x3D;4.99998123 y&#x3D;-55.00000000\n第 55 次迭代：x&#x3D;4.99998498 y&#x3D;-55.00000000\n第 56 次迭代：x&#x3D;4.99998798 y&#x3D;-55.00000000\n第 57 次迭代：x&#x3D;4.99999039 y&#x3D;-55.00000000\n第 58 次迭代：x&#x3D;4.99999231 y&#x3D;-55.00000000\n第 59 次迭代：x&#x3D;4.99999385 y&#x3D;-55.00000000\n第 60 次迭代：x&#x3D;4.99999508 y&#x3D;-55.00000000\n第 61 次迭代：x&#x3D;4.99999606 y&#x3D;-55.00000000\n第 62 次迭代：x&#x3D;4.99999685 y&#x3D;-55.00000000\n第 63 次迭代：x&#x3D;4.99999748 y&#x3D;-55.00000000\n第 64 次迭代：x&#x3D;4.99999798 y&#x3D;-55.00000000\n第 65 次迭代：x&#x3D;4.99999839 y&#x3D;-55.00000000\n第 66 次迭代：x&#x3D;4.99999871 y&#x3D;-55.00000000\n第 67 次迭代：x&#x3D;4.99999897 y&#x3D;-55.00000000\n第 68 次迭代：x&#x3D;4.99999917 y&#x3D;-55.00000000\n第 69 次迭代：x&#x3D;4.99999934 y&#x3D;-55.00000000\n第 70 次迭代：x&#x3D;4.99999947 y&#x3D;-55.00000000\n第 71 次迭代：x&#x3D;4.99999958 y&#x3D;-55.00000000\n</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/7a7e8c4b77b24a0a9a1f61d1fed2e719.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"梯度下降法，二元函数\"><a href=\"#梯度下降法，二元函数\" class=\"headerlink\" title=\"梯度下降法，二元函数\"></a>梯度下降法，二元函数</h3><p><img src=\"https://img-blog.csdnimg.cn/5c8a28390e1a4219846b27fa0d2ecaaa.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/dad9c63b019e420c8c14e399c82633c9.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/e24ec46986b942ad9dd4872fcc4e4fb9.png\" alt=\"在这里插入图片描述\"><br>几何含义：<br>梯度下降法如同人们每走一步都沿着当前最陡的方向下山。<br>梯度下降法的迭代公式如下：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/3a9fd730f7134f65ac111a8bb393f8ef.png\" alt=\"在这里插入图片描述\"><br><strong>案例：给定一个二元函数f(x,y)= - exp(x-y)*(x</strong>2-2<em>y<em>*2)，用梯度下降法求其在x&lt;0,y&lt;0范围上的极小值</em></em></p>\n<ol>\n<li>梯度下降法x、y初值设置。案例描述中要求在x&lt;0,y&lt;0区域内寻找函数最小值，因此初值可以设置为两个负数，本例初值设置：x=-1, y=-1, 也可以尝试设置为其它负数值。</li>\n<li>梯度下降法学习率的设置。可以先设置为0.1,如果效果不理想再进行微调。</li>\n<li>梯度下降法循环迭代终止设置。循环的终止方式可以有多种。一种是直接迭代一个较大的循环次数，直到循环自然终止。一种效率更高的方式是判断函数导数值与0值的差异，当差异足够小时（达到一定精度要求时）用break语句跳出循环。还可以根据连续两轮或多轮迭代x值的改变以及y值的改变达到某个微小值以内，认为迭代趋于稳定而跳出循环。或者根据函数值减小的幅度达到某个微小值以内，认为迭代趋于稳定；等等。本例采用第一种和第二种方式的结合：给一个较大的迭代次数10000次（也可以设置得更大）；为了提高代码效率，在循环体内判断当前函数的两个偏导数值是否接近0，误差设置为0.000001（根据实际情况设置，该值可调），满足条件就跳出循环。</li>\n<li>函数和两个偏导函数的函数自定义</li>\n<li>构造循环结构，利用梯度下降法的x和y值迭代公式进行迭代。输出最终结果。</li>\n</ol>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">import numpy as np\nx&#x3D;-1;y&#x3D;-1\nlearning_rate&#x3D;0.1\nerr&#x3D;0.000001;max_iters&#x3D;10000 \ndef f(x,y):\n    return -np.exp(x-y)*(x**2-2*y**2)\ndef dx(x,y):\n    return -(np.exp(x-y)*(2*x)+(x**2-2*y**2)*np.exp(x-y))\ndef dy(x,y):\n    return -(np.exp(x-y)*(-4*y)+(x**2-2*y**2)*np.exp(x-y)*(-1))\nfor t in range(max_iters):\n    if t%100&#x3D;&#x3D;0:\n        print(&quot;Iter %d, x&#x3D;%.8f,y&#x3D;%.8f,z&#x3D;%.8f,dx&#x3D;%.8f,dy&#x3D;%.8f&quot;%(t,x,y,f(x,y),dx(x,y),dy(x,y)))\n    if abs(dx(x,y))&lt;err and abs(dy(x,y))&lt;err:\n        print(&quot;Iter %d, x&#x3D;%.8f,y&#x3D;%.8f,z&#x3D;%.8f,dx&#x3D;%.8f,dy&#x3D;%.8f&quot;%(t,x,y,f(x,y),dx(x,y),dy(x,y)))\n        break\n    x&#x3D;x-learning_rate*dx(x,y); y&#x3D;y-learning_rate*dy(x,y)  #迭代公式‘</code></pre>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">Iter 0, x&#x3D;-1.00000000,y&#x3D;-1.00000000,z&#x3D;1.00000000,dx&#x3D;3.00000000,dy&#x3D;-5.00000000\nIter 100, x&#x3D;-3.09175581,y&#x3D;-1.37866718,z&#x3D;-1.03812484,dx&#x3D;0.07681195,dy&#x3D;0.04378580\nIter 200, x&#x3D;-3.58508879,y&#x3D;-1.71467625,z&#x3D;-1.07420439,dx&#x3D;0.03043384,dy&#x3D;0.01755166\nIter 300, x&#x3D;-3.79630426,y&#x3D;-1.85967410,z&#x3D;-1.08071845,dx&#x3D;0.01405238,dy&#x3D;0.00814006\nIter 400, x&#x3D;-3.89691917,y&#x3D;-1.92893257,z&#x3D;-1.08218878,dx&#x3D;0.00690599,dy&#x3D;0.00400866\nIter 500, x&#x3D;-3.94708386,y&#x3D;-1.96350427,z&#x3D;-1.08255344,dx&#x3D;0.00349371,dy&#x3D;0.00203005\nIter 600, x&#x3D;-3.97264220,y&#x3D;-1.98112811,z&#x3D;-1.08264800,dx&#x3D;0.00179285,dy&#x3D;0.00104229\nIter 700, x&#x3D;-3.98580483,y&#x3D;-1.99020700,z&#x3D;-1.08267306,dx&#x3D;0.00092669,dy&#x3D;0.00053889\nIter 800, x&#x3D;-3.99262086,y&#x3D;-1.99490901,z&#x3D;-1.08267978,dx&#x3D;0.00048077,dy&#x3D;0.00027961\nIter 900, x&#x3D;-3.99616039,y&#x3D;-1.99735092,z&#x3D;-1.08268159,dx&#x3D;0.00024990,dy&#x3D;0.00014535\nIter 1000, x&#x3D;-3.99800112,y&#x3D;-1.99862089,z&#x3D;-1.08268208,dx&#x3D;0.00013003,dy&#x3D;0.00007563\nIter 1100, x&#x3D;-3.99895913,y&#x3D;-1.99928185,z&#x3D;-1.08268222,dx&#x3D;0.00006769,dy&#x3D;0.00003937\nIter 1200, x&#x3D;-3.99945792,y&#x3D;-1.99962599,z&#x3D;-1.08268225,dx&#x3D;0.00003525,dy&#x3D;0.00002050\nIter 1300, x&#x3D;-3.99971766,y&#x3D;-1.99980520,z&#x3D;-1.08268226,dx&#x3D;0.00001836,dy&#x3D;0.00001068\nIter 1400, x&#x3D;-3.99985294,y&#x3D;-1.99989854,z&#x3D;-1.08268226,dx&#x3D;0.00000956,dy&#x3D;0.00000556\nIter 1500, x&#x3D;-3.99992340,y&#x3D;-1.99994715,z&#x3D;-1.08268227,dx&#x3D;0.00000498,dy&#x3D;0.00000290\nIter 1600, x&#x3D;-3.99996010,y&#x3D;-1.99997247,z&#x3D;-1.08268227,dx&#x3D;0.00000259,dy&#x3D;0.00000151\nIter 1700, x&#x3D;-3.99997922,y&#x3D;-1.99998566,z&#x3D;-1.08268227,dx&#x3D;0.00000135,dy&#x3D;0.00000079\nIter 1747, x&#x3D;-3.99998471,y&#x3D;-1.99998945,z&#x3D;-1.08268227,dx&#x3D;0.00000099,dy&#x3D;0.00000058\n</code></pre>\n<h2 id=\"数据拟合\"><a href=\"#数据拟合\" class=\"headerlink\" title=\"数据拟合\"></a>数据拟合</h2><p> <strong>数据拟合(梯度下降法)</strong></p>\n<p>测量所得的数据一般都有一定的误差，如何从大量测量所得的数据点中找出数据服从的一般规律性，即找出一个函数来刻画数据点的分布，可以称为数据的函数拟合。拟合好的函数，可以用来对新的测量数据进行预测。</p>\n<h3 id=\"线性拟合\"><a href=\"#线性拟合\" class=\"headerlink\" title=\"线性拟合\"></a>线性拟合</h3><p>给定呈线性分布的100个点，用梯度下降法求出其隐含的线性函数。已知这100个数据点分布如下图的蓝色小圆点所示，目测服从线性分布，现在请用梯度下降法找出能够较好拟合这100个数据点的未知线性函数y=ax+b(如图中的蓝色线条所示),求出a和b值。<br><img src=\"https://img-blog.csdnimg.cn/5f51085498bf4b52b9420312771062b4.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/43f6e2cdaf92448c82d7bc2ee6197f22.png\" alt=\"在这里插入图片描述\"></p>\n<ol>\n<li>梯度下降法a,b初值设置。本例用随机函数为a,b产生一个随机初值。</li>\n<li>梯度下降法学习率的设置。可以先设置为0.1,如果效果不理想再进行微调。</li>\n<li>误差计算。为了观察误差是否随着a,b值的调整，逐步在变小，需要对给定的a,b值，计算这100个样本点的平均误差。</li>\n<li>构造循环结构，利用梯度下降法的迭代公式对a,b进行迭代。输出最终结果。<br><img src=\"https://img-blog.csdnimg.cn/ae64d2d3c3eb45fdae31993a6ba0a15d.png\" alt=\"在这里插入图片描述\"><br>为了能够更好的理解线性拟合的本质，本例采用模拟数据点，随机生成100个数据点对，这些数据点服从线性分布，真实线性函数是y=3x+15(图中蓝色线), x坐标值为(0,2)的均匀分布，为了模拟测量数据中的测量误差，这里为每个对应的y坐标值增加一个标准正态分布的扰动。</li>\n</ol>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">import numpy as np\nimport matplotlib.pyplot as plt\n#生成100个数据点\nnp.random.seed(3)\nX&#x3D;2*np.random.rand(100)#生成100个随机数，模拟x\nY&#x3D;15+3*X+np.random.randn(100)#生成100个随机数，模拟y，真实的a&#x3D;3,b&#x3D;15\nlearning_rate&#x3D;0.1; roundN &#x3D; 5#对数据点集的轮数\nnp.random.seed(3); a&#x3D;np.random.randn()\nnp.random.seed(4) ;b&#x3D;np.random.randn()\ndef errorCompute(a,b):\n    error&#x3D;0\n    for j in range(len(X)):\n        error+&#x3D;1&#x2F;2*(a*X[j]+b-Y[j])**2\n    return error&#x2F;len(X)\nfor i in range(roundN):\n    for j in range(len(X)):\n        if j%50&#x3D;&#x3D;0:\n            print(&quot;round&#x3D;%d,iter&#x3D;%d,a&#x3D;%f,b&#x3D;%f,E&#x3D;%f&quot;%(i,j,a,b,errorCompute(a,b)))\n        gradA&#x3D;(a*X[j]+b-Y[j])*X[j]; gradB&#x3D;a*X[j]+b-Y[j]  #求偏导的公式\n        a&#x3D;a-learning_rate*gradA; b&#x3D;b-learning_rate*gradB  #迭代公式\n\n#下面绘制图形\nmaxX&#x3D;max(X); minX&#x3D;min(X); maxY&#x3D;max(Y); minY&#x3D;min(Y)\nX_fit&#x3D;np.arange(minX,maxX,0.01); Y_fit&#x3D;a*X_fit+b\nplt.plot(X,Y,&#39;.&#39;)#数据点\nplt.plot(X_fit,Y_fit,&#39;r-&#39;,label&#x3D;&#39;Gradient Descent&#39;)\nplt.plot(X_fit,15+3*X_fit,&#39;b-&#39;,label&#x3D;&#39;True&#39;)\nplt.legend()\nplt.show()\n</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/563e15c2e9c6470ea67ef973d5a5dae1.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">round&#x3D;0,iter&#x3D;0,a&#x3D;1.788628,b&#x3D;0.050562,E&#x3D;129.724642\nround&#x3D;0,iter&#x3D;50,a&#x3D;7.157349,b&#x3D;11.162271,E&#x3D;2.998449\nround&#x3D;1,iter&#x3D;0,a&#x3D;4.984254,b&#x3D;13.194898,E&#x3D;1.066988\nround&#x3D;1,iter&#x3D;50,a&#x3D;4.116652,b&#x3D;14.082373,E&#x3D;0.661608\nround&#x3D;2,iter&#x3D;0,a&#x3D;3.513805,b&#x3D;14.893728,E&#x3D;0.563870\nround&#x3D;2,iter&#x3D;50,a&#x3D;3.253854,b&#x3D;14.910921,E&#x3D;0.449889\nround&#x3D;3,iter&#x3D;0,a&#x3D;3.096573,b&#x3D;15.375762,E&#x3D;0.541890\nround&#x3D;3,iter&#x3D;50,a&#x3D;3.009039,b&#x3D;15.146017,E&#x3D;0.426155\nround&#x3D;4,iter&#x3D;0,a&#x3D;2.978185,b&#x3D;15.512537,E&#x3D;0.545377\nround&#x3D;4,iter&#x3D;50,a&#x3D;2.939574,b&#x3D;15.212725,E&#x3D;0.422347\n</code></pre>\n<h2 id=\"BP神经网络\"><a href=\"#BP神经网络\" class=\"headerlink\" title=\"BP神经网络\"></a>BP神经网络</h2><p>数据拟合部分的案例都是预先假设了数据隐含的函数分布的形式，使用梯度下降法求出其参数，但通常我们并不知道数据隐含的分布函数是线性还是多项式或者是其他函数形式，因此上面的应用也是非常有限，神经网络的出现打破了这一僵局。</p>\n<p> 理论上已证明，具有一个隐含层的三层网络可以逼近任意非线性函数，即一个输入层、一个隐含层、一个输出层可以近似表示任意非线性函数。</p>\n<p>BP神经网络是现今多种深度学习网络的基础组成部分，对BP神经网络的了解有助于帮助理解更多更复杂的深度学习网络，BP神经网络正是利用梯度下降法来寻求合适的参数值使得其误差函数最小化。</p>\n<p>BP神经网络可以解决回归问题和分类问题。简单来讲，回归问题相当于预测值是连续的数字类型（例如房屋价格预测，股票值预测等等），而分类问题可以看成预测值是离散值类型（例如，物种类别，猫，狗，牛等等）。下面从最简单的三层BP神经网络的回归任务开始阐明BP神经网络结构及其学习原理，其结构图如下：<br><img src=\"https://img-blog.csdnimg.cn/5504c181d91242e6846cab791a72c387.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/7f127284f6ba410c87a02f78bd7be256.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/96552f5331934bdeadb1350c62ed8427.png\" alt=\"在这里插入图片描述\"><br>隐藏层神经元zj值的计算原理是对每个输入x1,….,xI进行加权求和之后加入偏置bj得到（类似于我们构造一元线性函数时采用y=kx+b的方式，这里是扩展为多元线性函数的构造）。<br>这里权重w上的上标(1)是为了和后面构造输出层神经元值时的w区分开来。每个隐藏层神经元对应I个权重参数w, 共有J个隐藏神经单元，因此共有I<em>J个参数，将其看成一个J行I列的权重矩阵，另外还有J个偏置b, 所以隐藏层的参数个数共有J</em>(I+1)个，这些参数的初始值可用随机函数获得。<br>梯度下降法就是对这些参数进行迭代更新的方法，目标是最小化误差函数。<br><img src=\"https://img-blog.csdnimg.cn/4611a565faea4a67b8f25b786ecbf417.png\" alt=\"在这里插入图片描述\"><br>其图示如下：<br><img src=\"https://img-blog.csdnimg.cn/aa6e2fcfc2924bd3abcafd931384fd7c.png\" alt=\"在这里插入图片描述\"><br>Sigmoid函数可以使得任何输入值z转换到输出值0~1之间。且其导数值非常方便计算：<br>σ’ (z)=σ(z)(1-σ(z))   </p>\n<p><img src=\"https://img-blog.csdnimg.cn/fde7d524cfbf4d4b83c47d0d0f1190c5.png\" alt=\"在这里插入图片描述\"><br>如果为这J<em>(I+1)+ J+1个参数分别都给一个随机值，对于一个有确定值的输入样本(x1,x2,…,xi,…,xI)，可以得到一个预测值y。可以想象，这个预测值y和该样本对应的真实值t之间肯定有差异，这个差异我们称为误差，因此误差函数可以用如下公式表示：<br><img src=\"https://img-blog.csdnimg.cn/18c3b56f88484adb950671e5783f8609.png\" alt=\"在这里插入图片描述\"><br>如何调整这J</em>(I+1)+ J+1个参数值，使得这个误差值E最小，这就是网络学习的目标。换句话说，就是对于所有的输入样本和对应的真实值，让网络学习出J<em>(I+1)+ J+1个合适的参数值，使得误差最小。这里误差函数公式中包含的样本值(x1,x2,…,xi,…,xI)是已知的，样本对应的真实值t也是已知的，未知的是J</em>(I+1)+ J+1个参数。因此误差函数值的最小化问题可以转化为利用梯度下降法对这J<em>(I+1)+ J+1个参数进行迭代的问题，其迭代公式表述如下：<br><img src=\"https://img-blog.csdnimg.cn/a525cec2e31a48d1bb978f38fefb3abd.png\" alt=\"在这里插入图片描述\"><br>这里最难解决的是各个参数的偏导𝜕𝐸/(𝜕𝑤_𝑘 )的计算，下面对这J</em>(I+1)+ J+1个参数从后往前分层进行偏导计算推理。<br><img src=\"https://img-blog.csdnimg.cn/1e1e35d489a0420196f91feec604eda5.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/31b44e598f014ee3b6c1c9434eab2647.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/cc4d2c56cc594444963f2c453f5b8f2f.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">from sklearn.datasets import load_boston\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):#激活函数\n    return 1&#x2F;(1+np.exp(-x))\n\ndef minMaxScaler(X):#数据归一化\n    X_std&#x3D;(X-X.min(axis&#x3D;0))&#x2F;(X.max(axis&#x3D;0)-X.min(axis&#x3D;0))\n    return X_std\n\ndef shuffle(X,y):#数据乱序，并划分训练集和测试集\n    lenX&#x3D;len(X)\n    np.random.seed(10)\n    random_sort&#x3D;np.random.permutation(lenX)\n    trainX&#x3D;X[random_sort[0:int(0.8*lenX)]]\n    trainy&#x3D;y[random_sort[0:int(0.8*lenX)]]\n    testX&#x3D;X[random_sort[int(0.8*lenX):]]\n    testy&#x3D;y[random_sort[int(0.8*lenX):]]\n    return trainX,trainy,testX,testy\n\ndef forward_propagate(an_input):#信息前向传递\n    #计算并激活隐藏层\n    for j in range(hidden_n):\n        total&#x3D;0\n        for i in range(input_n):\n            total+&#x3D;an_input[i]*input_hidden_weights[j][i]\n        total+&#x3D;input_hidden_bs[j]#加入最后的偏置\n        hidden_as[j]&#x3D;sigmoid(total)\n    #计算并激活输出层\n    total&#x3D;0\n    for j in range(hidden_n):\n        total+&#x3D;hidden_as[j]*hidden_output_weights[j]\n    total+&#x3D;hidden_output_b\n    global output_z #由于output_z是标量，需要声明为全局\n    output_z&#x3D;total\n\n#误差反向信息传递\ndef back_propagate(an_input,y_true,learn_rate):\n    forward_propagate(an_input)#前向信息传递\n    #计算隐藏层到输出层权重的偏导数及偏置\n    d_y&#x3D;output_z-y_true\n    d_hidden_output_weights&#x3D;np.zeros(hidden_n)#初始化\n    for j in range(hidden_n):\n        d_hidden_output_weights[j]&#x3D;d_y*hidden_as[j]\n    d_hidden_output_b&#x3D;d_y\n    #更新隐藏层到输出层权重\n    for j in range(hidden_n):#更新隐藏层到输出层的权重\n        hidden_output_weights[j]-&#x3D;learn_rate*d_hidden_output_weights[j]\n    global hidden_output_b\n    #更新隐藏层到输出层偏置\n    hidden_output_b-&#x3D;learn_rate*d_hidden_output_b\n\n    # 计算输入层到隐藏层权重的偏导数\n    d_hidden_as &#x3D; np.zeros(hidden_n)\n    for j in range(hidden_n):\n        d_hidden_as[j] &#x3D; d_y * hidden_output_weights[j]\n    d_hidden_zs &#x3D; np.zeros(hidden_n)\n    for j in range(hidden_n):\n        d_hidden_zs[j] &#x3D; d_hidden_as[j] * hidden_as[j] * (1 - hidden_as[j])\n    d_input_hidden_weights &#x3D; np.zeros((hidden_n, input_n))\n    d_input_hidden_bs &#x3D; d_hidden_zs\n    for j in range(hidden_n):\n        for i in range(input_n):\n            d_input_hidden_weights[j][i] &#x3D; d_hidden_zs[j] * an_input[i]\n    # 更新输入层到隐藏层权重\n    for j in range(hidden_n):\n        for i in range(input_n):\n            input_hidden_weights[j][i] -&#x3D; learn_rate * d_input_hidden_weights[j][i]\n    # 更新输入层到隐藏层的偏置\n    for j in range(hidden_n):\n        input_hidden_bs[j] -&#x3D; learn_rate * d_input_hidden_bs[j]\n\n\n\ndef computeError(Xdata, Ydata):  # 对数据集计算误差，并计算预测结果\n    error &#x3D; 0\n    predictValues &#x3D; np.zeros(len(Xdata))\n    for i in range(len(Xdata)):\n        forward_propagate(Xdata[i])\n        error +&#x3D; 0.5 * (output_z - Ydata[i]) ** 2\n        predictValues[i] &#x3D; output_z\n    return error &#x2F; len(Xdata), predictValues\n\n\ndef train(trainX, trainy, iterate&#x3D;50, learn_rate&#x3D;0.05):  # 训练过程设置\n    for t in range(iterate):  # 训练神经网络\n        for i in range(len(trainX)):\n            back_propagate(trainX[i], trainy[i], learn_rate)\n        if t % 100 &#x3D;&#x3D; 0:  # 输出迭代之后的误差，以便观察误差降低的效果\n            totalError, preValues &#x3D; computeError(trainX, trainy)\n            print(&#39;iterate:&#39;, t, &#39; the training error&#x3D;&#39;, totalError)\n\n\n#初始化\ninput_n&#x3D;11#输入层特征数\nhidden_n&#x3D;6#隐藏层神经单元个数\noutput_n&#x3D;1#输出单元个数\nnp.random.seed(10)\ninput_hidden_weights&#x3D;np.random.rand(hidden_n,input_n)#输入到隐藏层的权值\nnp.random.seed(10)\ninput_hidden_bs&#x3D;np.random.rand(hidden_n)\nnp.random.seed(10)\nhidden_output_weights&#x3D;np.random.rand(hidden_n)#隐藏层到输出层的权值\nnp.random.seed(10)\nhidden_output_b&#x3D;np.random.rand(1)\nhidden_as&#x3D;np.zeros(hidden_n) #隐藏单元激活值\noutput_z&#x3D;np.ones(1) #输出单元值\n\n#数据处理\ndataset&#x3D;load_boston()\nX&#x3D;dataset.data[:,[0,2,4,5,6,7,8,9,10,11,12]]\ny&#x3D;dataset.target\nX&#x3D;minMaxScaler(X)\ntrainX,trainy,testX,testy&#x3D;shuffle(X,y)\n\ntrain(trainX,trainy,1000,0.002)#训练\n\n#需要计算测试集的总误差以及测试集预测的结果，以便和真实结果进行画图比对\ntotalError,predictValues&#x3D;computeError(testX,testy)\n\nprint(&#39;test error&#x3D;&#39;,totalError)\n#真实值从小到大排列\ntesty_ascending_index&#x3D;testy.argsort()\nplt.plot(range(len(testX)),testy[testy_ascending_index],&#39;o&#39;,label&#x3D;&#39;True value&#39;)\nplt.plot(range(len(testX)),predictValues[testy_ascending_index],&#39;*&#39;,label&#x3D;&#39;predict value&#39;)\nplt.legend()\nplt.show()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/debd7c4924e7470aa8a545e036164ef3.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">iterate: 0  the training error&#x3D; [45.03945242]\niterate: 100  the training error&#x3D; [6.18205346]\niterate: 200  the training error&#x3D; [5.59847866]\niterate: 300  the training error&#x3D; [5.57628389]\niterate: 400  the training error&#x3D; [5.5587542]\niterate: 500  the training error&#x3D; [5.54749627]\niterate: 600  the training error&#x3D; [5.54091243]\niterate: 700  the training error&#x3D; [5.53675292]\niterate: 800  the training error&#x3D; [5.53367205]\niterate: 900  the training error&#x3D; [5.5305169]\ntest error&#x3D; [12.82731241]</code></pre>\n<h1 id=\"数据思维\"><a href=\"#数据思维\" class=\"headerlink\" title=\"数据思维\"></a>数据思维</h1><h2 id=\"数据分析\"><a href=\"#数据分析\" class=\"headerlink\" title=\"数据分析\"></a>数据分析</h2><p><strong>广义的数据分析包括狭义数据分析和数据挖掘</strong><br><strong>狭义的数据分析</strong>是指根据分析目的，采用对比分析、分组分析、交叉分析等分析方法，对收集来的数据进行处理与分析，提取有价值的信息，发挥数据的作用，得到特定统计结果的过程。<br><strong>数据挖掘</strong>则是从大量的、不完全的、有噪声的、模糊的、随机的实际应用数据中，通过应用回归、分类、聚类等技术，挖掘潜在价值的过程。</p>\n<p><strong>典型的数据分析的流程：</strong></p>\n<ol>\n<li>问题定义</li>\n<li>数据采集</li>\n<li>数据预处理</li>\n<li>分析与建模</li>\n<li>模型评价与优化</li>\n</ol>\n<ul>\n<li>问题定义：明确分析目标，既本次分析要研究的主要问题和预期的分析目标等。</li>\n<li>数据采集：从企业数据库中调取相关业务数据、编写网页爬虫或到特定网站下载公开的数据集</li>\n<li>数据预处理：对原始数据进行清洗、变换和标准化</li>\n<li>分析与建模：指通过各种统计分析方法以及数据挖掘模型分析数据中有价值的信息，并得出结论的过程</li>\n<li>模型评价与优化：对已经建立的一个或多个模型，根据其模型的类别，使用对应的指标评价其性能优劣的过程</li>\n</ul>\n<h3 id=\"创建dataframe\"><a href=\"#创建dataframe\" class=\"headerlink\" title=\"创建dataframe\"></a>创建dataframe</h3><p>第三方库pandas：<br>导入：import pandas as pd<br>提供了DataFrame对象。 DataFrame是一 个表格型的数据结构，它含有一组 有序的列。<br>每列可以是不同的值类型（数值、字符串、布尔值等）。<br>DataFrame既有行索引也有列索引。</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">import pandas as pd\n\n#从字典创建dataframe:\ndata &#x3D; &#123;&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], \n        &#39;year&#39;: [2000, 2001, 2002, 2001, 2002], \n        &#39;pop&#39;: [1.5, 1.7, 3.6, 2.4, 2.9]&#125; \ndf&#x3D;pd.DataFrame(data)\n#指定列的顺序:\ndf&#x3D;pd.DataFrame(data, columns&#x3D;[&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;])\n#指定行索引\ndf&#x3D;pd.DataFrame(data,columns&#x3D;[&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;],index&#x3D;[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;])\n</code></pre>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>pandas中有三种索引方法：[]、.loc、.iloc</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">基于行名和列名的索引：.loc\n取得其中一行或多行:\ndf.loc[&#39;one&#39;]\ndf.loc[&#39;one&#39;:&#39;three&#39;]\ndf.loc[[&#39;one&#39;,&#39;three&#39;]]  #列表索引\n取得n行及n列\ndf.loc[&#39;one&#39;,&#39;year&#39;]\ndf.loc[&#39;one&#39;:&#39;three&#39;,&#39;year&#39;:&#39;pop&#39;]\ndf.loc[[&#39;one&#39;,&#39;three&#39;],[&#39;year&#39;,&#39;pop&#39;]]</code></pre>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">基于行索引值和列索引值的索引：.iloc\n取得其中一行或多行:\ndf.iloc[0]   #df.loc[&#39;one&#39;]\ndf.iloc[0:3]   #df.loc[&#39;one&#39;:&#39;three&#39;]\ndf.iloc[[0,2]]  # df.loc[[&#39;one’,&#39;three’]]   列表索引\n取得n行及n列\ndf.iloc[0,0]    #df.loc[&#39;one&#39;,&#39;year&#39;]\ndf.iloc[0:3,0:3]    #df.loc[&#39;one&#39;:&#39;three&#39;,&#39;year&#39;:&#39;pop&#39;]\ndf.iloc[[0,2],[0,2]]    #df.loc[[&#39;one&#39;,&#39;three&#39;],[&#39;year&#39;,&#39;pop&#39;]]\n</code></pre>\n<p>Pandas除三种基本索引方法外，还可以使用布尔索引</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">布尔索引：与前三种索引方式结合使用\n筛选符合条件的行:与[]结合使用\ndf[df.year&gt;2001]  #2001年以后\n筛选符合条件的行:与.loc结合使用\ndf.loc[df.year&gt;2001]\ndf.loc[df.year&gt;2001,&#39;year&#39;:&#39;pop’]\n筛选符合条件的行:与.iloc结合使用\ndf.iloc[list(df.year&gt;2001)]\ndf.iloc[list(df.year&gt;2001),0:2]\n</code></pre>\n<h3 id=\"基本操作：添加、删除、修改、排序、合并数据集\"><a href=\"#基本操作：添加、删除、修改、排序、合并数据集\" class=\"headerlink\" title=\"基本操作：添加、删除、修改、排序、合并数据集\"></a>基本操作：添加、删除、修改、排序、合并数据集</h3><pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">添加\ndf&#x3D;df.append(&#123;&#39;year&#39;:2003,&#39;state&#39;:&#39;Louisiana&#39;,&#39;pop&#39;:1.4&#125;,ignore_index&#x3D;True)#行\ndf[&#39;total&#39;]&#x3D;0    #列 \ndf[&#39;total&#39;]&#x3D;df[&#39;pop&#39;]-df[&#39;debt&#39;]   #先将debt列值设为1\n删除\ndf.drop(5,inplace&#x3D;True)  #行\ndf.drop(‘debt’,axis&#x3D;1,inplace&#x3D;True) #列\n修改\n#获取和修改列类型:\ndf[&#39;pop&#39;].dtype\ndf[&#39;pop&#39;]&#x3D;df[&#39;pop&#39;].astype(str)\n#修改值\ndf[&#39;pop&#39;]&#x3D;df[&#39;pop&#39;]+3\ndf.loc[df[&#39;year&#39;]&gt;&#x3D;2002,&#39;pop&#39;] +&#x3D;1\n排序\ndf.sort_values(by&#x3D;[&#39;pop&#39;],ascending&#x3D;True)\ndf.sort_values(by&#x3D;[&#39;year&#39;,&#39;pop&#39;],ascending&#x3D;False)\n\n\n合并数据集\ndf3&#x3D;pd.concat([df1,df],ignore_index&#x3D;True)   #首先df1&#x3D;df.copy()\ndf3&#x3D;pd.concat([df1,df], axis&#x3D;1) #轴1方向拼接\n\n\n\n</code></pre>\n<h3 id=\"文件操作\"><a href=\"#文件操作\" class=\"headerlink\" title=\"文件操作\"></a>文件操作</h3><pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">载入数据:csv、 excel、 txt等\ndf&#x3D;pd.read_csv(‘xxxx.csv’,encoding&#x3D;‘xxx’,sep&#x3D;‘xxx’,header&#x3D;None) #sep:分隔符，默认为逗号； header&#x3D;None没有列标签\ndf&#x3D; pd.read_excel(‘xxxx.xlsx’,header&#x3D;None)#header&#x3D;None没有列标签\n写入数据:csv、 excel、 txt等\ndf.to_csv(‘xxxx.csv’,sep&#x3D;‘xxx’,encoding&#x3D;‘xxx’)\ndf.to_excel(&#39;xxxx.xlsx&#39;,index&#x3D;False) #不写入索引列\n\n</code></pre>\n<p>第三方库:matplotlib<br>Matplotlib是一个Python 2D绘图库，可以轻松生成折线图、柱状图、饼图、散点图等</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#导入库\nimport matplotlib.pyplot as plt\n#中文问题：\nplt.rcParams[&#39;font.sans-serif&#39;] &#x3D; [&#39;SimHei&#39;] \n# 步骤一（替换sans-serif字体）\nplt.rcParams[&#39;axes.unicode_minus&#39;] &#x3D; False  \n# 步骤二（解决坐标轴负数的负号显示问题）\n#折线图\nx &#x3D; [1, 2, 3, 4]\ny &#x3D; [1.2, 2.5, 4.5, 7.3]\nplt.plot(x, y)  # plot函数作图\nplt.savefig(&#39;line.png&#39;) #保存\nplt.show()  #看得到图形则不用这条语句\n</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/681e38dee06443fba3c9925234f05f13.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">x &#x3D; [1, 2, 3, 4]\ny &#x3D; [1.2, 2.5, 4.5, 7.3]\n\n\nplt.plot(x, y, color&#x3D;&quot;r&quot;, linestyle&#x3D;&quot;--&quot;, marker&#x3D;&quot;*&quot;, linewidth&#x3D;1.0)\n\n\nplt.show()  #看得到图形则不用这条语句</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/7af9521b6ae44c78aa4e55707a5a429f.jpeg#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/3584a2c2a00745d0968a841047a54694.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/dde2ad39184a4e87a07bf9a3f3a340ea.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/9d645b54116a4dc4909f479a9a669fb1.png\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#x轴和y轴标题\nplt.ylabel(&#39;Y&#39;)\nplt.xlabel(&#39;X&#39;)\n#图形标题\nplt.title(&#39;折线图&#39;)\n#图例\nplt.legend(loc&#x3D;&#39;upper left&#39;)\n#网格\nplt.grid()\n#注释\nplt.text(x,y,&#39;注释&#39;)\n</code></pre>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#导入库\nimport matplotlib.pyplot as plt\n#中文问题：\nplt.rcParams[&#39;font.sans-serif&#39;] &#x3D; [&#39;SimHei&#39;] \n# 步骤一（替换sans-serif字体）\nplt.rcParams[&#39;axes.unicode_minus&#39;] &#x3D; False  \n# 步骤二（解决坐标轴负数的负号显示问题）\n\n#条形图：bar()\nx &#x3D; [0,1,2,3] #季度\ny &#x3D; [1000, 1500, 1300, 1800] #销量\ncolors&#x3D;[&#39;red&#39;,&#39;green&#39;,&#39;cyan&#39;,&#39;blue&#39;]\n#plt.bar(x, y,width&#x3D;0.8,color&#x3D;colors)\nplt.yticks(x,[&#39;春&#39;, &#39;夏&#39;, &#39;秋&#39;, &#39;冬&#39;])\n#水平条形图：barh()\nplt.barh(x, y,height&#x3D;0.8,color&#x3D;colors)\n#值标签\nfor i,j in zip(x,y):\n    plt.text(i,j,j)\n\n\nplt.show()  #看得到图形则不用这条语句\n</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/5f9641a5b71b4e2494dcd8901dbb4176.jpeg#pic_center\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/b1d2eb06e807475cbeb7e08c425124ae.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">#饼图：pie()\nlabels &#x3D; [&#39;娱乐&#39;,&#39;育儿&#39;,&#39;饮食&#39;,&#39;房贷&#39;,&#39;交通&#39;,&#39;其它&#39;]\nsizes &#x3D; [2,5,12,70,2,9]\nexplode &#x3D; (0,0,0,0.1,0,0)\nplt.pie(sizes,explode&#x3D;explode,labels&#x3D;labels,autopct&#x3D;&#39;%1.1f%%&#39;)\nplt.title(&quot;8月份家庭支出&quot;)\n\n\nplt.show()  </code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/73f3a9434bce4c49bf1248595b03d431.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">分析目标\n查看数据基本情况，并完成对数据的基本统计。统计2004年各季度销售量并以条图方式呈现，绘制2003年、2004年销售额曲线对比图，并将2004年不同季度销售额占比以饼图方式呈现。\n载入数据\ndf&#x3D;pd.read_csv(&#39;sales_data_sample9.csv&#39;,encoding&#x3D;&#39;gbk&#39;)\n数据查看\ndf.info() #查看数据整体情况\nprint(df.head(3))  #前面n行\nprint(df.tail(3))     #末尾n行\nprint(df.shape)    #数据集规模\ndf.dtypes   #数据类型\n</code></pre>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">数据预处理\n缺失值：\n检查是否有缺失值：df.count()\ndf&#x3D;df.dropna() \t\t   #默认丢弃任何含有NaN的行\ndf&#x3D;df.dropna(how&#x3D;&#39;all&#39;) #只丢弃全为NaN的行\ndf&#x3D;df.dropna(how&#x3D;&#39;all&#39;,axis&#x3D;1) #只丢弃全为NaN的列\ndf&#x3D;df.fillna(0)             #用0填充缺失数据\ndf[&#39;总分&#39;].fillna(df[&#39;总分&#39;].mean(),inplace&#x3D;True) #用列均值填充\n变换：\n异常值：\n</code></pre>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">探索性分析：\n基本统计方法\nsum() 求和\nmax() 最大值  \t\tidxmax() 最大值对应索引号\nmin() 最小值\t\tidxmin() 最小值对应索引号\nvar() 方差\t\tstd()  标准差\nmedian() 中位数\t\tmean() 均值\nnlargest（n,列名）该列最大的n个值\ndescribe():计算各列的频率、均值、标准差、级值、四分位数\n</code></pre>\n<h2 id=\"数值预测\"><a href=\"#数值预测\" class=\"headerlink\" title=\"数值预测\"></a>数值预测</h2><p>狭义的数据分析：根据分析目的，采用对比分析、分组分析、交叉分析等统计分析方法，对采集来的数据进行处理与分析，从中提取有价值的信息<br><strong>数据挖掘</strong>：从大量的、不完全的、有噪声的、模糊的、随机的实际应用数据中，通过应用回归、分类、聚类等技术，挖掘潜在价值的过程</p>\n<p><strong>机器学习</strong><br>机器学习（Machine Learning）根据已知数据来不断学习和积累经验，然后总结出规律并尝试预测未知数据的属性，是一门综合性非常强的多领域交叉学科，涉及线性代数、概率论、逼近论、凸分析、算法复杂度理论等多门学科<br>目前机器学习已经有了十分广泛的应用，例如数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、推荐系统、战略游戏和机器人运用等。</p>\n<p>机器学习的过程与人类学习过程类似，例如识别图像需要几个步骤：<br>首先要收集大量样本图像，并标明这些图像的<strong>类别</strong>（称为样本标签，就像告诉孩子这是一只恐龙），这些样本图像就是<strong>数据集</strong>。<br>把样本和标注送给算法学习（称为<strong>训练</strong>），训练完成之后得到一个模型，这个模型是从这些样本中总结归纳得到的知识。<br>接下来，可以用这个模型对新的图像进行识别，称为<strong>预测</strong>。</p>\n<p>以下是机器学习中的经典监督学习过程：<br><img src=\"https://img-blog.csdnimg.cn/dede69a86d4943098cb49151b344b6c5.png\" alt=\"在这里插入图片描述\"><br>机器学习<br><strong>有监督学习</strong>：训练样本包含对应的‘标签’（例如每个样本所属的类别）。训练集需要包括输入和输出，也就是特征和目标，其中目标是由人工标注的‘标签’。通过大量已知的数据不断训练和减少错误来提高认知能力，最后根据积累的经验去预测未知数据的属性。</p>\n<ul>\n<li><strong>回归</strong></li>\n<li><strong>分类</strong></li>\n</ul>\n<p><strong>无监督学习</strong>：训练数据包含一组输入向量而没有相应的目标值。这类算法的目标可能是发现原始数据中相似样本的组合（称作聚类），或者确定数据的分布（称作密度估计），或者把数据从高维空间投影到低维空间（称作降维）以便进行可视化。</p>\n<ul>\n<li>聚类</li>\n</ul>\n","text":"算法思维方程求根二分法画图猜f(x)=0的大概范围[a,b]再缩小范围，保证f(a)f(b)&lt;0,在[a,b]上一定有实根。 中点x0=a+(b-a)/2=(a+b)/2 即两端值和的一半。若中点与右端符号相同(如左图)，则缩小到[a,x0]。若中点与左端符号相同(如右图)...","link":"","photos":[],"count_time":{"symbolsCount":"25k","symbolsTime":"23 mins."},"categories":[],"tags":[{"name":"python","slug":"python","count":9,"path":"api/tags/python.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%AE%97%E6%B3%95%E6%80%9D%E7%BB%B4\"><span class=\"toc-text\">算法思维</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%96%B9%E7%A8%8B%E6%B1%82%E6%A0%B9\"><span class=\"toc-text\">方程求根</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BA%8C%E5%88%86%E6%B3%95\"><span class=\"toc-text\">二分法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E6%B3%95\"><span class=\"toc-text\">牛顿迭代法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%A6%BB%E6%95%A3Newton%E6%B3%95\"><span class=\"toc-text\">离散Newton法</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%87%BD%E6%95%B0%E6%B1%82%E6%9C%80%E5%80%BC-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95\"><span class=\"toc-text\">函数求最值(梯度下降法)</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%8C%E4%B8%80%E5%85%83%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">梯度下降法，一元函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%EF%BC%8C%E4%BA%8C%E5%85%83%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">梯度下降法，二元函数</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E6%8B%9F%E5%90%88\"><span class=\"toc-text\">数据拟合</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%BA%BF%E6%80%A7%E6%8B%9F%E5%90%88\"><span class=\"toc-text\">线性拟合</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">BP神经网络</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E6%80%9D%E7%BB%B4\"><span class=\"toc-text\">数据思维</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90\"><span class=\"toc-text\">数据分析</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%88%9B%E5%BB%BAdataframe\"><span class=\"toc-text\">创建dataframe</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%B4%A2%E5%BC%95\"><span class=\"toc-text\">索引</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%EF%BC%9A%E6%B7%BB%E5%8A%A0%E3%80%81%E5%88%A0%E9%99%A4%E3%80%81%E4%BF%AE%E6%94%B9%E3%80%81%E6%8E%92%E5%BA%8F%E3%80%81%E5%90%88%E5%B9%B6%E6%95%B0%E6%8D%AE%E9%9B%86\"><span class=\"toc-text\">基本操作：添加、删除、修改、排序、合并数据集</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C\"><span class=\"toc-text\">文件操作</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%95%B0%E5%80%BC%E9%A2%84%E6%B5%8B\"><span class=\"toc-text\">数值预测</span></a></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"python实训 俄罗斯方块 面向对象编程MVC模式","uid":"ff813a6890c74120b08ea0e357e75669","slug":"俄罗斯方块","date":"2022-11-03T14:41:49.000Z","updated":"2022-11-08T15:13:25.231Z","comments":true,"path":"api/articles/俄罗斯方块.json","keywords":null,"cover":[],"text":"前端演示viewtest.pyfrom tkinter import * from random import * import threading from tkinter.messagebox import showinfo from tkinter.messagebox i...","link":"","photos":[],"count_time":{"symbolsCount":"27k","symbolsTime":"25 mins."},"categories":[{"name":"信管","slug":"信管","count":12,"path":"api/categories/信管.json"}],"tags":[{"name":"python","slug":"python","count":9,"path":"api/tags/python.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"【python机器学习基础教程】（五）","uid":"01c87ce279dfe7327e075971fdffa3c0","slug":"python机器学习5","date":"2022-11-03T14:38:49.000Z","updated":"2022-11-03T14:38:59.371Z","comments":true,"path":"api/articles/python机器学习5.json","keywords":null,"cover":[],"text":"模型评估与改进交叉验证交叉验证是一种评估泛化性能的统计学方法，它比单次划分训练集和测试集的方法更加稳定、全面。在交叉验证中，数据被多次划分，并且需要训练多个模型。最常用的交叉验证是k折交叉验证，其中k是由用户指定的数字，通常取5或者10。在执行5折交叉验证时，首先将数据划分为（大...","link":"","photos":[],"count_time":{"symbolsCount":"7k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}