{"title":"《基于深度学习的自然语言处理》笔记","uid":"1be85ad561954f8e295d5b1c686f2a6f","slug":"基于深度学习的自然语言处理","date":"2022-11-03T14:20:49.000Z","updated":"2022-11-08T16:18:09.514Z","comments":true,"path":"api/articles/基于深度学习的自然语言处理.json","keywords":null,"cover":null,"content":"<p>深度学习一般是指建立在含有多层非线性变换的神经网络结构之上，对数据的表示进行抽象和学习的一系列机器学习算法。</p>\n<p>深度学习主要为自然语言处理的研究带来了两方面的变化：一方面是使用<strong>统一的分布式</strong>（低维、稠密、连续）向量表示不同粒度的语言单元，如词、短语、句子和篇章等；另一方面是使用循环、卷积、递归等神经网络模型对不同的语言单元进行<strong>组合</strong>，获得更大语言单元的表示。除了不同粒度的单语语言单元外，不同种类的语言甚至不同模态（语言、图像等）的数据都可以通过类似的组合方式表示在相同的语义向量空间中，然后通过在向量空间中的运算来实现分类、推理、生成等各种任务并应用于各种相关的任务之中。</p>\n<p>自然语言处理（NLP）指的是对人类语言进行自动的计算处理。<br>它包括两类算法：将人类产生的文本作为输入；产生看上去很自然的文本作为输出。</p>\n<h2 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h2><h3 id=\"自然语言处理中的深度学习\"><a href=\"#自然语言处理中的深度学习\" class=\"headerlink\" title=\"自然语言处理中的深度学习\"></a>自然语言处理中的深度学习</h3><p>将神经网络用于语言的一个主要组件是使用<strong>嵌入层</strong>，即将离散的符号映射为相对低维的连续向量。</p>\n<p>有两种主要的神经网络，即<strong>前馈网络</strong>和<strong>循环</strong>/<strong>递归网络</strong>，它们可以以各种方式组合。</p>\n<p><strong>前馈网络</strong>：也叫多层感知机（MLP），其输入大小固定，对于变化的输入长度，我们可以忽略元素的顺序。<strong>网络的非线性以及易于整合预训练词嵌入的能力经常导致更高的分类精度。</strong><br>卷积前馈网络是一类特殊的结构，其善于抽取数据中有意义的局部模式，<strong>这些工作适合于识别长句子或者文本中有指示性的短语和惯用语</strong>。</p>\n<p><strong>循环神经网络（RNN）</strong>：是适于序列数据的特殊模型，网络接收输入序列作为输入，产生固定大小的向量作为序列的摘要。<br>语言模型：指预测序列中下一个单词的频率（等价于预测下一个序列的概率），是许多自然语言处理应用的核心。</p>\n<h1 id=\"有监督分类与前馈神经网络\"><a href=\"#有监督分类与前馈神经网络\" class=\"headerlink\" title=\"有监督分类与前馈神经网络\"></a>有监督分类与前馈神经网络</h1><h2 id=\"学习基础与线性模型\"><a href=\"#学习基础与线性模型\" class=\"headerlink\" title=\"学习基础与线性模型\"></a>学习基础与线性模型</h2><p>留一法：在训练过程中，我们必须评估已训练函数在未见实例上的准确率。<br>一种方法是<strong>留一交叉验证</strong></p>\n<p>留存集：就计算时间而言，一个更有效的方法是划分训练集为两个子集，在较大的子集（训练集）上训练模型，在较小的子集（留存集）上测试模型的准确率。</p>\n<p><strong>三路划分</strong>：训练集、验证集和测试集。</p>\n<h3 id=\"独热（one-hot）和稠密向量表示\"><a href=\"#独热（one-hot）和稠密向量表示\" class=\"headerlink\" title=\"独热（one-hot）和稠密向量表示\"></a>独热（one-hot）和稠密向量表示</h3><p>连续单词词袋</p>\n<h3 id=\"训练和最优化\"><a href=\"#训练和最优化\" class=\"headerlink\" title=\"训练和最优化\"></a>训练和最优化</h3><p>损失函数<br>hinge(二分类)<br>hinge(多分类)</p>\n<p><strong>二元交叉熵</strong> ：二元交叉熵损失也叫做Logistic损失，被用于输出为条件概率分布的二元分类中。</p>\n<p><strong>分类交叉熵损失</strong></p>\n<p><strong>等级损失</strong></p>\n<h4 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h4><p>通过向优化目标中加入正则化R来完成以上的目的，这样做可以控制参数值的复杂性，避免过拟合情况的发生。<br>R的常用选择有L2范数、L1范数和弹性网络。</p>\n<h3 id=\"基于梯度的最优化\"><a href=\"#基于梯度的最优化\" class=\"headerlink\" title=\"基于梯度的最优化\"></a>基于梯度的最优化</h3><p>随机梯度下降</p>\n<h2 id=\"前馈神经网络\"><a href=\"#前馈神经网络\" class=\"headerlink\" title=\"前馈神经网络\"></a>前馈神经网络</h2><h3 id=\"常见非线性函数\"><a href=\"#常见非线性函数\" class=\"headerlink\" title=\"常见非线性函数\"></a>常见非线性函数</h3><p><strong>sigmoid</strong>  ：sigmoid激活函数$\\sigma(x)=1/(1+e^{-x})$，也称作逻辑斯蒂函数，是一个S型的函数。它将每一个值$x$变换到[0,1]区间中。</p>\n<p><strong>tanh(双曲正切)</strong>：双曲正切激活函数$tanh(x)={e^{2x}+1\\over e{2x}+1}$是一个S型函数，它将值$x$变换到[-1,1]区间中。</p>\n<p><strong>线性修正单元（ReLU）</strong>：修正激活函数，也被称为修正线性单元，是一种非常简单的激活函数。</p>\n<h3 id=\"正则化与丢弃法\"><a href=\"#正则化与丢弃法\" class=\"headerlink\" title=\"正则化与丢弃法\"></a>正则化与丢弃法</h3><h3 id=\"相似和距离层\"><a href=\"#相似和距离层\" class=\"headerlink\" title=\"相似和距离层\"></a>相似和距离层</h3><p>点积</p>\n<p>欧氏距离</p>\n<h2 id=\"神经网络计算\"><a href=\"#神经网络计算\" class=\"headerlink\" title=\"神经网络计算\"></a>神经网络计算</h2><p>与线性模型相似，神经网络也是可微分的参数化函数，它使用了基于梯度的优化方法来进行训练。</p>\n<p>前向计算<br>反向计算（导数、反向传播）</p>\n<h1 id=\"处理自然语言数据\"><a href=\"#处理自然语言数据\" class=\"headerlink\" title=\"处理自然语言数据\"></a>处理自然语言数据</h1><h2 id=\"文本特征构造\"><a href=\"#文本特征构造\" class=\"headerlink\" title=\"文本特征构造\"></a>文本特征构造</h2><h3 id=\"NLP分类问题中的拓扑结构\"><a href=\"#NLP分类问题中的拓扑结构\" class=\"headerlink\" title=\"NLP分类问题中的拓扑结构\"></a>NLP分类问题中的拓扑结构</h3><p>通常来说，自然语言中的分类问题能够被分为几个宽泛的方向，其依赖于被分类的事项。<br>词<br>文本<br>成对文本<br>上下文中的词<br>词之间的关系</p>\n","text":"深度学习一般是指建立在含有多层非线性变换的神经网络结构之上，对数据的表示进行抽象和学习的一系列机器学习算法。 深度学习主要为自然语言处理的研究带来了两方面的变化：一方面是使用统一的分布式（低维、稠密、连续）向量表示不同粒度的语言单元，如词、短语、句子和篇章等；另一方面是使用循环、...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":9,"path":"api/categories/机器学习.json"}],"tags":[{"name":"NLP","slug":"NLP","count":2,"path":"api/tags/NLP.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%BC%95%E8%A8%80\"><span class=\"toc-text\">引言</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">自然语言处理中的深度学习</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB%E4%B8%8E%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">有监督分类与前馈神经网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%B8%8E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B\"><span class=\"toc-text\">学习基础与线性模型</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%8B%AC%E7%83%AD%EF%BC%88one-hot%EF%BC%89%E5%92%8C%E7%A8%A0%E5%AF%86%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA\"><span class=\"toc-text\">独热（one-hot）和稠密向量表示</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%AD%E7%BB%83%E5%92%8C%E6%9C%80%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">训练和最优化</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%AD%A3%E5%88%99%E5%8C%96\"><span class=\"toc-text\">正则化</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%9C%80%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">基于梯度的最优化</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\"><span class=\"toc-text\">前馈神经网络</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E8%A7%81%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0\"><span class=\"toc-text\">常见非线性函数</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%B8%A2%E5%BC%83%E6%B3%95\"><span class=\"toc-text\">正则化与丢弃法</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%9B%B8%E4%BC%BC%E5%92%8C%E8%B7%9D%E7%A6%BB%E5%B1%82\"><span class=\"toc-text\">相似和距离层</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97\"><span class=\"toc-text\">神经网络计算</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%A4%84%E7%90%86%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%95%B0%E6%8D%AE\"><span class=\"toc-text\">处理自然语言数据</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0\"><span class=\"toc-text\">文本特征构造</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#NLP%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84\"><span class=\"toc-text\">NLP分类问题中的拓扑结构</span></a></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"【python机器学习基础教程】（四）","uid":"1ee3586cac227bddb968ecf686dc5d5b","slug":"python机器学习4","date":"2022-11-03T14:37:49.000Z","updated":"2022-11-03T14:37:40.370Z","comments":true,"path":"api/articles/python机器学习4.json","keywords":null,"cover":[],"text":"数据表示与特征工程到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的连续特征。对于许多应用而言，数据的收集方式并不是这样。一种特别常见的特征类型就是分类特征，也叫离散特征。 对于某个特定应用而言，如何找到最佳数据表示，这个问题被称为特征工程。 分类变量...","link":"","photos":[],"count_time":{"symbolsCount":"9.3k","symbolsTime":"8 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"【电子羊的奇妙冒险 】初试深度学习（4）","uid":"0ace423a65608d78c82e5378fc3daeb8","slug":"电子羊4","date":"2022-11-03T14:18:49.000Z","updated":"2022-11-08T16:16:30.225Z","comments":true,"path":"api/articles/电子羊4.json","keywords":null,"cover":[],"text":"代码注释随机数 出于我们的目的，我将创建一组具有根的此类多项式。实际上，我将首先创建根，然后创建多项式，如下所示： import numpy as np MIN_ROOT &#x3D; -1 MAX_ROOT &#x3D; 1 def make(n_samples, n_degr...","link":"","photos":[],"count_time":{"symbolsCount":"14k","symbolsTime":"13 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":9,"path":"api/categories/机器学习.json"}],"tags":[{"name":"深度学习","slug":"深度学习","count":9,"path":"api/tags/深度学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}