{"title":"【python机器学习基础教程】（四）","uid":"1ee3586cac227bddb968ecf686dc5d5b","slug":"python机器学习4","date":"2022-11-03T14:37:49.000Z","updated":"2022-11-03T14:37:40.370Z","comments":true,"path":"api/articles/python机器学习4.json","keywords":null,"cover":[],"content":"<h1 id=\"数据表示与特征工程\"><a href=\"#数据表示与特征工程\" class=\"headerlink\" title=\"数据表示与特征工程\"></a>数据表示与特征工程</h1><p>到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的<strong>连续特征</strong>。对于许多应用而言，数据的收集方式并不是这样。一种特别常见的特征类型就是<strong>分类特征</strong>，也叫<strong>离散特征</strong>。</p>\n<p>对于某个特定应用而言，如何找到最佳数据表示，这个问题被称为<strong>特征工程</strong>。</p>\n<h2 id=\"分类变量\"><a href=\"#分类变量\" class=\"headerlink\" title=\"分类变量\"></a>分类变量</h2><h3 id=\"One-Hot编码（虚拟变量）\"><a href=\"#One-Hot编码（虚拟变量）\" class=\"headerlink\" title=\"One-Hot编码（虚拟变量）\"></a>One-Hot编码（虚拟变量）</h3><p>到目前为止，表示分类变量最常用的方法就是使用one-hot编码或N取一编码 ，也叫虚拟变量。<br>虚拟变量背后的思想是将一个分类变量替换为一个或多个新特征，新特征取值为0和1。</p>\n<p>首先，我们使用pandas从逗号分隔值（CSV）文件中加载数据：<br>数据来源于1994年美国人口普查数据库。(下载地址<a href=\"https://archive.ics.uci.edu/ml/datasets/Adult\">https://archive.ics.uci.edu/ml/datasets/Adult</a>)<br><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">import pandas as pd\nfrom IPython.display import display\n\ndata&#x3D;pd.read_csv(&quot;data&#x2F;adult.data&quot;,header&#x3D;None,index_col&#x3D;False,names&#x3D;[&#39;age&#39;,&#39;workclass&#39;,&#39;fnlwgt&#39;,&#39;education&#39;,&#39;education-num&#39;,&#39;marital-status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;gender&#39;,&#39;capital-gain&#39;,&#39;capital-loss&#39;,&#39;hours-per-week&#39;,&#39;native-country&#39;,&#39;income&#39;])\n\n#为方面说明，我们只选了其中几列\ndata&#x3D;data[[&#39;age&#39;,&#39;workclass&#39;,&#39;education&#39;,&#39;gender&#39;,&#39;hours-per-week&#39;,&#39;occupation&#39;,&#39;income&#39;]]\ndisplay(data.head())</code></pre><br>结果：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>age          workclass  …          occupation  income<br>0   39          State-gov  …        Adm-clerical   &lt;=50K<br>1   50   Self-emp-not-inc  …     Exec-managerial   &lt;=50K<br>2   38            Private  …   Handlers-cleaners   &lt;=50K<br>3   53            Private  …   Handlers-cleaners   &lt;=50K<br>4   28            Private  …      Prof-specialty   &lt;=50K</p></blockquote>\n<p>1.检查字符串编码的分类数据<br>读完数据集之后，最好先检查每一列是否包含有意义的分类数据。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(data.gender.value_counts())</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>[5 rows x 7 columns]<br> Male      21790<br> Female    10771<br>Name: gender, dtype: int64</p></blockquote>\n<p>用pandas编码数据有一种非常简单的方法，就是使用get_dummies函数。<br>get_dummies函数自动变换所有具有对象类型（比如字符串）的列或所有分类的列：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;Original features:\\n&quot;,list(data.columns),&quot;\\n&quot;)\ndata_dummies&#x3D;pd.get_dummies(data)\nprint(&quot;features after get_dummies:\\n&quot;,list(data_dummies.columns))</code></pre>\n<p>输出：<br><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">Original features:\n [&#39;age&#39;, &#39;workclass&#39;, &#39;education&#39;, &#39;gender&#39;, &#39;hours-per-week&#39;, &#39;occupation&#39;, &#39;income&#39;] \n\nfeatures after get_dummies:\n [&#39;age&#39;, &#39;hours-per-week&#39;, &#39;workclass_ ?&#39;, &#39;workclass_ Federal-gov&#39;, &#39;workclass_ Local-gov&#39;, &#39;workclass_ Never-worked&#39;, &#39;workclass_ Private&#39;, &#39;workclass_ Self-emp-inc&#39;, &#39;workclass_ Self-emp-not-inc&#39;, &#39;workclass_ State-gov&#39;, &#39;workclass_ Without-pay&#39;, &#39;education_ 10th&#39;, &#39;education_ 11th&#39;, &#39;education_ 12th&#39;, &#39;education_ 1st-4th&#39;, &#39;education_ 5th-6th&#39;, &#39;education_ 7th-8th&#39;, &#39;education_ 9th&#39;, &#39;education_ Assoc-acdm&#39;, &#39;education_ Assoc-voc&#39;, &#39;education_ Bachelors&#39;, &#39;education_ Doctorate&#39;, &#39;education_ HS-grad&#39;, &#39;education_ Masters&#39;, &#39;education_ Preschool&#39;, &#39;education_ Prof-school&#39;, &#39;education_ Some-college&#39;, &#39;gender_ Female&#39;, &#39;gender_ Male&#39;, &#39;occupation_ ?&#39;, &#39;occupation_ Adm-clerical&#39;, &#39;occupation_ Armed-Forces&#39;, &#39;occupation_ Craft-repair&#39;, &#39;occupation_ Exec-managerial&#39;, &#39;occupation_ Farming-fishing&#39;, &#39;occupation_ Handlers-cleaners&#39;, &#39;occupation_ Machine-op-inspct&#39;, &#39;occupation_ Other-service&#39;, &#39;occupation_ Priv-house-serv&#39;, &#39;occupation_ Prof-specialty&#39;, &#39;occupation_ Protective-serv&#39;, &#39;occupation_ Sales&#39;, &#39;occupation_ Tech-support&#39;, &#39;occupation_ Transport-moving&#39;, &#39;income_ &lt;&#x3D;50K&#39;, &#39;income_ &gt;50K&#39;]\n\n</code></pre><br>连续特征age和hours-per-week没有发生变化，而分类特征的每个可能取值都被扩展为一个新特征：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">display(data_dummies.head())</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p> age  hours-per-week  …  income_ &lt;=50K  income_ &gt;50K<br> 0   39              40  …              1             0<br> 1   50              13  …              1             0<br> 2   38              40  …              1             0<br> 3   53              40  …              1             0<br> 4   28              40  …              1             0<br> [5 rows x 46 columns]</p></blockquote>\n<p>下面我们使用values属性将data_dummies数据框（DataFrame）转换为Numpy数组，然后在其上训练一个机器学习模型。<br>在训练模型之前，注意要把目标变量（现在被编码为两个income列）从数据中分离出来。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">features &#x3D; data_dummies.loc[:, &#39;age&#39;:&#39;occupation_ Transport-moving&#39;]\n\n#提取Numpy数组\nX&#x3D;features.values\n\ny &#x3D; data_dummies[&#39;income_ &gt;50K&#39;].values\nprint(&quot;X.shape:&#123;&#125;  y.shape:&#123;&#125;&quot;.format(X.shape,y.shape))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>X.shape:(32561, 44)  y.shape:(32561,)</p></blockquote>\n<p>现在数据的表示方式可以被scikit-learn处理，我们可以像之前一样进行下一步：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X,y,random_state&#x3D;0)\nlogreg&#x3D;LogisticRegression()\nlogreg.fit(X_train,y_train)\nprint(&quot;Test score:&#123;:.2f&#125;&quot;.format(logreg.score(X_test,y_test)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test score:0.81</p>\n<h3 id=\"分箱、离散化、线性模型与树\"><a href=\"#分箱、离散化、线性模型与树\" class=\"headerlink\" title=\"分箱、离散化、线性模型与树\"></a>分箱、离散化、线性模型与树</h3><p>数据表示的最佳方法不仅取决于数据的语义，还取决于所使用的模型种类。<br>线性模型和基于树的模型（比如决策树、梯度提升树和随机森林）是两种成员很多同时又非常实用的模型，它们在处理不同的特征表示时就具有非常不同的性质。</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nX,y&#x3D;mglearn.datasets.make_wave(n_samples&#x3D;100)\n\nline&#x3D;np.linspace(-3,3,1000,endpoint&#x3D;False).reshape(-1,1)\nreg&#x3D;DecisionTreeRegressor(min_samples_split&#x3D;3).fit(X,y)\n\nplt.plot(line,reg.predict(line),label&#x3D;&quot;decision tree&quot;)\n\nreg&#x3D;LinearRegression().fit(X,y)\nplt.plot(line,reg.predict(line),label&#x3D;&#39;linear regression &#39;)\nplt.plot(X[:,0],y,&#39;o&#39;,c&#x3D;&#39;k&#39;)\n\nplt.legend(loc&#x3D;&quot;best&quot;)\nplt.ylabel(&quot;Regression output&quot;)\nplt.xlabel(&quot;Input feature&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/4c89cba9eb5549d39002069c2ef89eba.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<p>有一种方法可以让线性模型在连续数据上变得更加强大，就是使用特征<strong>分箱</strong>（也叫<strong>离散化</strong>）将其划分为多个特征。<br>我们假设将特征的输入范围划分成固定个数的<strong>箱子</strong>（bin）,比如10个，那么数据点就可以利用它所在的箱子来表示。<br>为了确定这一点，我们首先需要定义箱子。<br>在这个例子中，我们在-3和3之间定义10个均匀分布的箱子。<br>我们用np.linspace函数创造11个元素，从而创建10个箱子，即两个连续边界之间的空间：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">bins&#x3D;np.linspace(-3,3,11)\nprint(&quot;bins:&#123;&#125;&quot;.format(bins))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>bins:[-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]</p></blockquote>\n<p>这里第一个箱子包含的特征取值 在-3到-2.4之间的所有数据点 ，第二个箱子包含特征取值在-2.4到-1.8之间的所有数据点，以此类推。</p>\n<p>接下来，我们记录每个 数据点所属的箱子。<br>这可以用np.digitize函数轻松计算出来：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">which_bin&#x3D;np.digitize(X,bins&#x3D;bins)\nprint(&quot;\\nData points:\\n&quot;,X[:5])\nprint(&quot;\\nBin membership for data points:\\n&quot;,which_bin[:5])</code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">Data points:\n [[-0.75275929]\n [ 2.70428584]\n [ 1.39196365]\n [ 0.59195091]\n [-2.06388816]]\n\nBin membership for data points:\n [[ 4]\n [10]\n [ 8]\n [ 6]\n [ 2]]</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.preprocessing import OneHotEncoder\n#使用OneHotEncoder进行变换\nencoder&#x3D;OneHotEncoder(sparse&#x3D;False)\n#encoder.fit找到which_bin中的唯一值\nencoder.fit(which_bin)\n#transform创建one-hot编码\nX_binned&#x3D;encoder.transform(which_bin)\nprint(X_binned[:,5])</code></pre>\n<p>输出：<br><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 1. 0. 0. 0.]</code></pre></p>\n<p>下面我们在one-hot编码后的数据上构建新的线性模型和新的决策树模型。结果如下，箱子的边界由黑色虚线表示： </p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">line_binned&#x3D;encoder.transform(np.digitize(line,bins&#x3D;bins))\n\nreg&#x3D;LinearRegression().fit(X_binned,y)\nplt.plot(line,reg.predict(line_binned),label&#x3D;&#39;linear regression binned&#39;)\n\nreg&#x3D;DecisionTreeRegressor(min_samples_split&#x3D;3).fit(X_binned,y)\nplt.plot(line,reg.predict(line_binned),label&#x3D;&#39;linear regression binned&#39;)\nplt.plot(X[:,0],y,&#39;o&#39;,c&#x3D;&#39;k&#39;)\nplt.vlines(bins,-3,3,linewidth&#x3D;1,alpha&#x3D;.2)\nplt.legend(loc&#x3D;&quot;best&quot;)\nplt.ylabel(&quot;Regression output&quot;)\nplt.xlabel(&quot;Input feature&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/d8881980f01e4124be1398ae4dcb8cd6.jpeg#pic_center\" alt=\"在这里插入图片描述\"><br>虚线和实线完全重合，说明线性回归模型和决策树做出了完全相同的预测。</p>\n<h3 id=\"交互特征与多项式特征\"><a href=\"#交互特征与多项式特征\" class=\"headerlink\" title=\"交互特征与多项式特征\"></a>交互特征与多项式特征</h3><p>想要丰富特征表示，特别是对于线性模型而言，另一种方法是添加原始数据的<strong>交互特征</strong>和<strong>多项式特征</strong>。这种特征工程通常用于统计建模，但也常用于许多实际的机器学习应用中。</p>\n<h3 id=\"单变量非线性变换\"><a href=\"#单变量非线性变换\" class=\"headerlink\" title=\"单变量非线性变换\"></a>单变量非线性变换</h3><p>下面我们使用一个模拟的计数数据集，其性质与在自然状态下能找到的数据集类似。<br>特征全都是整数值，而响应是连续的：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">rnd&#x3D;np.random.RandomState(0)\nX_org&#x3D;rnd.normal(size&#x3D;(1000,3))\nw&#x3D;rnd.normal(size&#x3D;3)\n\nX&#x3D;rnd.poisson(10*np.exp(X_org))\ny&#x3D;np.dot(X_org,w)\n\nprint(&quot;Number of feature appearances:\\n&#123;&#125;&quot;.format(np.bincount(X[:,0])))</code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">Number of feature appearances:\n[28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9\n 17  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5\n  2  1  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1\n  0  2  0  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0\n  0  0  0  0  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0\n  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\n</code></pre>\n<p>我们将其计数可视化：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">bins&#x3D;np.bincount(X[:,0])\nplt.bar(range(len(bins)),bins,color&#x3D;&#39;r&#39;)\nplt.ylabel(&quot;Number of appearances&quot;)\nplt.xlabel(&quot;value&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/9edeb38e707d4c5da4e7019c6331bc3a.jpeg#pic_center\" alt=\"在这里插入图片描述\"><br>我们尝试拟合一个岭回归模型：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.linear_model import Ridge\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X,y,random_state&#x3D;0)\nscore&#x3D;Ridge().fit(X_train,y_train).score(X_test,y_test)\nprint(&quot;Test score:&#123;:.3f&#125;&quot;.format(score))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test score:0.622</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">X_train_log &#x3D; np.log(X_train+1)\nX_test_log&#x3D;np.log(X_test+1)\n\nplt.hist(X_train_log[:,0],bins&#x3D;25,color&#x3D;&#39;red&#39;)\nplt.ylabel(&quot;number of appearances&quot;)\nplt.xlabel(&quot;value&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/ad0850c7ee97441fa9758721efc744f3.jpeg#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"自动化特征选择\"><a href=\"#自动化特征选择\" class=\"headerlink\" title=\"自动化特征选择\"></a>自动化特征选择</h2><h3 id=\"单变量统计\"><a href=\"#单变量统计\" class=\"headerlink\" title=\"单变量统计\"></a>单变量统计</h3><p>在单变量统计中，我们计算每个特征和目标值之间的关系是否存在统计显著性，然后选择具有最高置信度的特征。对于分类问题，这也被称为<strong>方差分析</strong>。<br>这些测试的一个关键性质就是它们是<strong>单变量的</strong>，即它们只单独考虑每个特征。</p>\n<h3 id=\"基于模型的特征选择\"><a href=\"#基于模型的特征选择\" class=\"headerlink\" title=\"基于模型的特征选择\"></a>基于模型的特征选择</h3><p>基于模型的特征选择使用一个监督机器学习模型来判断每个特征的重要性，并且仅保留最重要的特征。</p>\n<h3 id=\"迭代特征选择\"><a href=\"#迭代特征选择\" class=\"headerlink\" title=\"迭代特征选择\"></a>迭代特征选择</h3><h2 id=\"利用专家知识\"><a href=\"#利用专家知识\" class=\"headerlink\" title=\"利用专家知识\"></a>利用专家知识</h2>","text":"数据表示与特征工程到目前为止，我们一直假设数据是由浮点数组成的二维数组，其中每一列是描述数据点的连续特征。对于许多应用而言，数据的收集方式并不是这样。一种特别常见的特征类型就是分类特征，也叫离散特征。 对于某个特定应用而言，如何找到最佳数据表示，这个问题被称为特征工程。 分类变量...","link":"","photos":[],"count_time":{"symbolsCount":"9.3k","symbolsTime":"8 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B\"><span class=\"toc-text\">数据表示与特征工程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F\"><span class=\"toc-text\">分类变量</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#One-Hot%E7%BC%96%E7%A0%81%EF%BC%88%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F%EF%BC%89\"><span class=\"toc-text\">One-Hot编码（虚拟变量）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%88%86%E7%AE%B1%E3%80%81%E7%A6%BB%E6%95%A3%E5%8C%96%E3%80%81%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%A0%91\"><span class=\"toc-text\">分箱、离散化、线性模型与树</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BA%A4%E4%BA%92%E7%89%B9%E5%BE%81%E4%B8%8E%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81\"><span class=\"toc-text\">交互特征与多项式特征</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8D%95%E5%8F%98%E9%87%8F%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2\"><span class=\"toc-text\">单变量非线性变换</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%87%AA%E5%8A%A8%E5%8C%96%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">自动化特征选择</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BB%9F%E8%AE%A1\"><span class=\"toc-text\">单变量统计</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">基于模型的特征选择</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%BF%AD%E4%BB%A3%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9\"><span class=\"toc-text\">迭代特征选择</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%88%A9%E7%94%A8%E4%B8%93%E5%AE%B6%E7%9F%A5%E8%AF%86\"><span class=\"toc-text\">利用专家知识</span></a></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"【python机器学习基础教程】（五）","uid":"01c87ce279dfe7327e075971fdffa3c0","slug":"python机器学习5","date":"2022-11-03T14:38:49.000Z","updated":"2022-11-03T14:38:59.371Z","comments":true,"path":"api/articles/python机器学习5.json","keywords":null,"cover":[],"text":"模型评估与改进交叉验证交叉验证是一种评估泛化性能的统计学方法，它比单次划分训练集和测试集的方法更加稳定、全面。在交叉验证中，数据被多次划分，并且需要训练多个模型。最常用的交叉验证是k折交叉验证，其中k是由用户指定的数字，通常取5或者10。在执行5折交叉验证时，首先将数据划分为（大...","link":"","photos":[],"count_time":{"symbolsCount":"7k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"《基于深度学习的自然语言处理》笔记","uid":"1be85ad561954f8e295d5b1c686f2a6f","slug":"基于深度学习的自然语言处理","date":"2022-11-03T14:20:49.000Z","updated":"2022-11-08T16:18:09.514Z","comments":true,"path":"api/articles/基于深度学习的自然语言处理.json","keywords":null,"cover":null,"text":"深度学习一般是指建立在含有多层非线性变换的神经网络结构之上，对数据的表示进行抽象和学习的一系列机器学习算法。 深度学习主要为自然语言处理的研究带来了两方面的变化：一方面是使用统一的分布式（低维、稠密、连续）向量表示不同粒度的语言单元，如词、短语、句子和篇章等；另一方面是使用循环、...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"机器学习","slug":"机器学习","count":9,"path":"api/categories/机器学习.json"}],"tags":[{"name":"NLP","slug":"NLP","count":2,"path":"api/tags/NLP.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}