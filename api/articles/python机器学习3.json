{"title":"【Python机器学习基础教程】（三）","uid":"8239787d32df6c476a6ebbc5a4594a62","slug":"python机器学习3","date":"2022-11-03T13:17:49.000Z","updated":"2022-11-03T15:57:25.209Z","comments":true,"path":"api/articles/python机器学习3.json","keywords":null,"cover":[],"content":"<h1 id=\"无监督学习与预处理\"><a href=\"#无监督学习与预处理\" class=\"headerlink\" title=\"无监督学习与预处理\"></a>无监督学习与预处理</h1><h2 id=\"无监督学习的预处理\"><a href=\"#无监督学习的预处理\" class=\"headerlink\" title=\"无监督学习的预处理\"></a>无监督学习的预处理</h2><p>本章研究两类无监督学习：<strong>数据集变换</strong>与<strong>聚类</strong></p>\n<p>数据集的<strong>无监督变换</strong>是创建数据新的表示的算法。</p>\n<p>无监督变换的一个常见应用是降维，它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新方法，用较少的特征就可以概括其特性。<br>降维的一个常见应用是为了可视化将数据将为2维。</p>\n<p>与之相反，<strong>聚类算法</strong>将数据划分成不同的组，每组包含相似的物项。</p>\n<h2 id=\"预处理和缩放\"><a href=\"#预处理和缩放\" class=\"headerlink\" title=\"预处理和缩放\"></a>预处理和缩放</h2><p>一些算法（如神经网络和SVM）对数据缩放非常敏感。<br>因此，通常的做法是对特征进行调节，使数据表示更适合于这些算法。<br>通常来说，这是对数据的一种简单的按特征的缩放 和移动。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_scaling()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/d58a900e15124f4fb3c5ca36926746b3.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"降维、特征提取与流形学习\"><a href=\"#降维、特征提取与流形学习\" class=\"headerlink\" title=\"降维、特征提取与流形学习\"></a>降维、特征提取与流形学习</h2><p>利用无监督学习进行数据变换可能有很多目的。最常见的 目的就是可视化、压缩数据，以及寻找数据量更大的数据表示以用于进一步的处理。</p>\n<p>为实现这些目的，最简单也最常用的一种算法就是主成分分析。<br>我们也将学习另外两种算法：非负矩阵分解和t-SNE，前者通常用于特征提取，后者通常用于二维散点图的可视化。</p>\n<h3 id=\"主成分分析\"><a href=\"#主成分分析\" class=\"headerlink\" title=\"主成分分析\"></a>主成分分析</h3><p>主成分分析（PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_pca_illustration()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/0800697b29a144e5a86958b60b8a8ad1.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"将PCA应用于cancer数据集并可视化\"><a href=\"#将PCA应用于cancer数据集并可视化\" class=\"headerlink\" title=\"将PCA应用于cancer数据集并可视化\"></a>将PCA应用于cancer数据集并可视化</h4><p>PCA最常见的应用之一就是将高维数据集可视化。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\ncancer &#x3D; load_breast_cancer()\n\nfig,axes &#x3D; plt.subplots(15,2,figsize&#x3D;(10,20))\nmalignant&#x3D;cancer.data[cancer.target&#x3D;&#x3D;0]\nbenign&#x3D;cancer.data[cancer.target&#x3D;&#x3D;1]\n\nax&#x3D;axes.ravel()\n\nfor i in range(30):\n    _,bins &#x3D; np.histogram(cancer.data[:,i],bins&#x3D;50)\n    ax[i].hist(malignant[:,i],bins&#x3D;bins,color&#x3D;mglearn.cm3(0),alpha&#x3D;.5)\n    ax[i].hist(benign[:,i],bins&#x3D;bins,color&#x3D;mglearn.cm3(2),alpha&#x3D;.5)\n    ax[i].set_title(cancer.feature_names[i])\n    ax[i].set_yticks(())\nax[0].set_xlabel(&quot;Feature magnitude&quot;)\nax[0].set_ylabel(&quot;Frequency&quot;)\nax[0].legend([&quot;malignant&quot;,&quot;benign&quot;],loc&#x3D;&quot;best&quot;)\nfig.tight_layout()</code></pre>\n<p><strong>乳腺癌数据集中每个类别的特征直方图：</strong><br><img src=\"https://img-blog.csdnimg.cn/e9b0e4d761894e72bb2d68dc48edc1eb.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们为每个特征创建一个直方图，计算具有某一特征的数据点在特定范围内（bin）的出现频率。</p>\n<p>每张图都包含两个直方图，一个是良性类别的所有点（蓝色），一个是恶性类别的所有点（红色）。</p>\n<p>但这种图无法向我们展示变量之间的相互作用以及这种相互作用与类别之间的关系。<br>利用PCA，我们可以获取到主要的相互作用，并得到稍微完整的图像。<br>我们可以找到前两个主成分，并在这个新的二维空间中用散点图将数据可视化。</p>\n<p>在应用PCA之前，我们利用StandardScaler缩放数据，使每个特征的方差均为1：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_breast_cancer\n\ncancer &#x3D; load_breast_cancer()\n\nscaler&#x3D;StandardScaler()\nscaler.fit(cancer.data)\nX_scaled&#x3D;scaler.transform(cancer.data)</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.decomposition import PCA\n#保留数据的前两个成分\npca&#x3D;PCA(n_components&#x3D;2)\n#对乳腺癌数据拟合PCA模型\npca.fit(X_scaled)\n\n#将数据变换到前两个主成分的方向上\nX_pca&#x3D;pca.transform(X_scaled)\nprint(&quot;Original shape:&#123;&#125;&quot;.format(str(X_scaled.shape)))\nprint(&quot;Reduced shape:&#123;&#125;&quot;.format(str(X_pca.shape)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Original shape:(569, 30)<br>Reduced shape:(569, 2)</p></blockquote>\n<p>现在我们可以对前两个主成分作图：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#对第一个和第二个 主成分作图，按类别着色\nplt.figure(figsize&#x3D;(8,8))\nmglearn.discrete_scatter(X_pca[:,0],X_pca[:,1],cancer.target)\nplt.legend(cancer.target_names,loc&#x3D;&quot;best&quot;)\nplt.gca().set_aspect(&quot;equal&quot;)\nplt.xlabel(&quot;First principal component&quot;)\nplt.ylabel(&quot;Second principal component&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/eee84f576ec447709c1cc95b87ea3ce6.png#pic_center\" alt=\"在这里插入图片描述\"><br>主成分对应于原始数据中的方向，所以他们是原始特征的组合。</p>\n<p>在拟合过程中，主成分被保存在PCA对象的components_属性 中：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;PCA component shape:&#123;&#125;&quot;.format(pca.components_.shape))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>PCA component shape:(2, 30)</p></blockquote>\n<p>看一下components_的内容：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;PCA components:\\n&#123;&#125;&quot;.format(pca.components_))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>PCA components:<br>[[ 0.21890244  0.10372458  0.22753729  0.22099499  0.14258969  0.23928535<br>0.25840048  0.26085376  0.13816696  0.06436335  0.20597878  0.01742803<br>0.21132592  0.20286964  0.01453145  0.17039345  0.15358979  0.1834174<br>0.04249842  0.10256832  0.22799663  0.10446933  0.23663968  0.22487053<br>0.12795256  0.21009588  0.22876753  0.25088597  0.12290456  0.13178394]<br> [-0.23385713 -0.05970609 -0.21518136 -0.23107671  0.18611302  0.15189161<br>0.06016536 -0.0347675   0.19034877  0.36657547 -0.10555215  0.08997968<br>-0.08945723 -0.15229263  0.20443045  0.2327159   0.19720728  0.13032156<br>0.183848    0.28009203 -0.21986638 -0.0454673  -0.19987843 -0.21935186<br>0.17230435  0.14359317  0.09796411 -0.00825724  0.14188335  0.27533947]]</p></blockquote>\n<p>我们还可以用热图将系数可视化：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">plt.matshow(pca.components_,cmap&#x3D;&#39;viridis&#39;)\nplt.yticks([0,1],[&quot;First component&quot;,&quot;Second component&quot;])\nplt.colorbar()\nplt.xticks(range(len(cancer.feature_names)),cancer.feature_names,rotation&#x3D;60,ha&#x3D;&#39;left&#39;)\nplt.xlabel(&quot;Feature&quot;)\nplt.ylabel(&quot;Principal components&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/5f3baaff5ccb4c6abc257461e1f989fb.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"特征提取的特征脸\"><a href=\"#特征提取的特征脸\" class=\"headerlink\" title=\"特征提取的特征脸\"></a>特征提取的特征脸</h4><p>特征提取背后的思想是，可以找到一种数据表示，比给定的原始表示更适合于分析。</p>\n<p>特征提取很有用，它的一个很好的应用实例就是图像。<br>图像由像素组成，通常存储为红绿蓝（RGB）强度。图像中的对象通常由上千个 像素组成，它们只有放在一起才有意义。</p>\n<p>我们将给出PCA对图像做特征提取的一个简单应用，即处理Wild数据集Labeled Faces(标记人脸)中的人脸图像。<br>这一数据集包含从互联网上下载的名人脸部图像，它包含从21世纪初开始的政治家、歌手、演员和运动员的人脸图像。</p>\n<p>我们使用这些图像的灰度版本，并将它们按照比例缩小以加快处理速度。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import fetch_lfw_people\npeople &#x3D; fetch_lfw_people(min_faces_per_person&#x3D;20,resize&#x3D;0.7)\n\nimage_shape&#x3D;people.images[0].shape\nfix,axes&#x3D;plt.subplots(2,5,figsize&#x3D;(15,8),subplots_kw&#x3D;&#123;&#39;xticks&#39;:(),&#39;yticks&#39;:()&#125;)\nfor target,image,ax in zip(people.target,people.images,axes.ravel()):\n    ax.imshow(image)\n    ax.set_title(people.target_names[target])</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/4ab0d5246e6f4feba75bfba214953e02.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">print(&quot;people.image.shape:&#123;&#125;&quot;.format(people.image.shape))\nprint(&quot;Number of class :&#123;&#125;&quot;.format(len(people.target_names)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>people.image.shape:(1780, 87, 65)<br>Number of class :23</p></blockquote>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#计算每个目标出现的次数\ncounts&#x3D;np.bincount(people.target)\n#将次数与目标名称一起打印出来\nfor i,(count,name) in enumerate(zip(counts,people.target_names)):\n    print(&quot;&#123;0:25&#125;&#123;1:3&#125;&quot;.format(name,count),end&#x3D;&#39;  &#39;)\n    if (i+1)%3&#x3D;&#x3D;0:\n        print()\n    </code></pre>\n<p>输出：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">Alejandro Toledo          39  Amelie Mauresmo           21  Ariel Sharon              77  \nAtal Bihari Vajpayee      24  Bill Clinton              29  Colin Powell             236  \nDonald Rumsfeld          121  George W Bush            530  Gerhard Schroeder        109  \nGloria Macapagal Arroyo   44  Hamid Karzai              22  Hans Blix                 39  \nIgor Ivanov               20  Junichiro Koizumi         60  Kofi Annan                32  \nLaura Bush                41  Lleyton Hewitt            41  Megawati Sukarnoputri     33  \nSerena Williams           38  Tiger Woods               23  Tom Daschle               25  \nTony Blair               144  Vicente Fox               32</code></pre>\n<p>为了降低数据偏斜，我们对每个人最多只取50张图像</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mask &#x3D;np.zeros(people.target.shape,dtype&#x3D;np.bool)\nfor target in np.unique(people.target):\n    mask[np.where(people.target&#x3D;&#x3D;target)[0][:50]]&#x3D;1\nX_people&#x3D;people.data[mask]\ny_people&#x3D;people.target[mask]\n\n#将灰度值缩放到0到1之间，而不是在0到255之间\n#以得到更好的数据稳定性\nX_people&#x3D;X_people&#x2F;255</code></pre>\n<p>使用单一最近邻分类器，寻找与要分类的人脸最为相似的人脸。<br>这个分类器原则上可以处理每个类别只有一个训练器样例的情况。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.neighbors import KNeighborsClassifier\n#将数据分为训练集和测试集\nX_train,X_test,y_train,y_test&#x3D;train_test_split(X_people,y_people,stratify&#x3D;y_people,random_state&#x3D;0)\n#使用一个邻居构建KNeighborsClassifier\nknn &#x3D;KNeighborsClassifier(n_neighbors&#x3D;1)\nknn.fit(X_train,y_train)\nprint(&quot;Test set score of 1-nn:&#123;:.2f&#125;&quot;.format(knn.score(X_test,y_test)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set score of 1-nn:0.40</p>\n<p>我们得到的精度为40%<br>我们希望，使用沿着主成分方向的距离可以提高精度。</p></blockquote>\n<p>这里我们启用PCA的<strong>白化</strong>选项，它将主成分缩放到相同的尺度。<br>变换后的结果与使用StandardScaler相同。<br>再次使用数据，白化不仅对应于旋转数据，还对应于缩放数据 使其形状是圆形而不是椭圆。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_pca_whitening()</code></pre>\n<p><strong>利用启用白化的PCA进行数据变换：</strong><br><img src=\"https://img-blog.csdnimg.cn/0bd6d2a389ca449aa17285ef4f355ad2.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们对训练数据拟合PCA对象，并提取前100个主成分。<br>然后对训练数据和测试数据进行变换：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">pca&#x3D;PCA(n_components&#x3D;100,whiten&#x3D;True,random_state&#x3D;0).fit(X_train)\nX_train_pca&#x3D;pca.transform(X_train)\nX_test_pca&#x3D;pca.transform(X_test)\n\nprint(&quot;X_train_pca.shape:&#123;&#125;&quot;.format(X_train_pca.shape))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>X_train_pca.shape:(639, 100)</p></blockquote>\n<p>新数据有100个特征，即前100个主成分。<br>现在，可以用新表示使用单一最近邻分类器来将我们的图像分类。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">knn &#x3D;KNeighborsClassifier(n_neighbors&#x3D;1)\nknn.fit(X_train_pca,y_train)\nprint(&quot;Test set accuracy:&#123;:.2f&#125;&quot;.format(knn.score(X_test_pca,y_test)))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Test set accuracy:0.43</p></blockquote>\n<p>精度提升为43%</p>\n<p>对于图像数据，我们还可以很容易地将找到的主成分可视化。成分对应于输入空间里的方向。<br>这里的输入空间是87像素x65像素的灰度图像，所以在这个空间中的方向也是87像素x65像素的灰度图像。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">image_shape&#x3D;people.images[0].shape\nfix,axes&#x3D;plt.subplots(3,5,figsize&#x3D;(15,12),subplot_kw&#x3D;&#123;&#39;xticks&#39;:(),&#39;yticks&#39;:()&#125;)\nfor i,(component,ax) in enumerate(zip(pca.components_,axes.ravel())):\n    ax.imshow(component.reshape(image_shape),cmap&#x3D;&#39;viridis&#39;)\n    ax.set_title(&quot;&#123;&#125;.component&quot;.format((i+1)))</code></pre>\n<p><strong>人脸数据集前15个主成分的成分向量：</strong><br><img src=\"https://img-blog.csdnimg.cn/2cc1a3c8249c4d16a7eeac8182cc947b.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_pca_faces(X_train,X_test,image_shape)</code></pre>\n<p><strong>这里我们分别用10个、50个、100个和500个成分对一些人脸进行重建并将其可视化：</strong><br><img src=\"https://img-blog.csdnimg.cn/2d3dfee962824fa0a1fb19f200c50999.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们还可以尝试使用PCA的前两个主成分，将数据集中的所有人脸在散点图中可视化，其类别在图中给出。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.discrete_scatter(X_train_pca[:,0],X_train_pca[:,1],y_train)\nplt.xlabel(&quot;First principal component&quot;)\nplt.ylabel(&quot;Second principal component&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/e382a482cf95402a8a893209ef85a6d3.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"非负矩阵分解\"><a href=\"#非负矩阵分解\" class=\"headerlink\" title=\"非负矩阵分解\"></a>非负矩阵分解</h3><p>非负矩阵分解（NMF）是另一种无监督学习算法，其目的在于提取有用的特征。</p>\n<h4 id=\"将NMF应用于模拟数据\"><a href=\"#将NMF应用于模拟数据\" class=\"headerlink\" title=\"将NMF应用于模拟数据\"></a>将NMF应用于模拟数据</h4><pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_nmf_illustration()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/9885b86eebae46e9b5aa76d2f71f206b.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h4 id=\"将NMF应用于人脸图像\"><a href=\"#将NMF应用于人脸图像\" class=\"headerlink\" title=\"将NMF应用于人脸图像\"></a>将NMF应用于人脸图像</h4><p>首先，观察分量个数如何影响NMF重建数据的好坏：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_nmf_faces(X_train,X_test,image_shape)</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>未完待续</p></blockquote>\n<p>假设我们对一个信号感兴趣，它是三个不同信号源合成的：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">S&#x3D;mglearn.datasets.make_signals()\nplt.figure(figsize&#x3D;(6,3))\nplt.plot(S,&#39;-&#39;)\nplt.xlabel(&quot;Time&quot;)\nplt.ylabel(&quot;Signal&quot;)</code></pre>\n<p><strong>原始信号源</strong><br><img src=\"https://img-blog.csdnimg.cn/f0be79c24a13415092c5f3c0191c1b3e.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们无法观测到原始信号，只能观察到三个信号的叠加混合。<br>我们想要将混合信号分解为 原始分量。<br>假设我们有许多种不同的方法来观测混合信号，每种方法都为我们提供了一系列测量结果。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#将数据混合成100维的状态\nA&#x3D;np.random.RandomState(0).uniform(size&#x3D;(100,3))\nX&#x3D;np.dot(S,A.T)\nprint(&quot;Shape of measurements:&#123;&#125;&quot;.format(X.shape))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Shape of measurements:(2000, 100)</p></blockquote>\n<p>我们可以使用NMF来还原这三个信号：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.decomposition import NMF\n\nnmf&#x3D;NMF(n_components&#x3D;3,random_state&#x3D;42)\nS_&#x3D;nmf.fit_transform(X)\nprint(&quot;Recovered signal shape:&#123;&#125;&quot;.format(S_.shape))</code></pre>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>Recovered signal shape:(2000, 3)</p></blockquote>\n<p>为了对比，我们也应用了PCA</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.decomposition import PCA\npca&#x3D;PCA(n_components&#x3D;3)\nH&#x3D;pca.fit_transform(X)</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">models&#x3D;[X,S,S_,H]\nnames&#x3D;[&#39;Observations(first three measurements)&#39;,\n&#39;True sources&#39;,&#39;NMF recovered signals&#39;,&#39;PCA recovered signals&#39;]\nfig,axes&#x3D;plt.subplots(4,figsize&#x3D;(8,4),gridspec_kw&#x3D;&#123;&#39;hspace&#39;:.5&#125;,subplot_kw&#x3D;&#123;&#39;xticks&#39;:(),&#39;yticks&#39;:()&#125;)\nfor model,name,ax in zip(models,names,axes):\n    ax.set_title(name)\n    ax.plot(model[:,:3],&#39;-&#39;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/efd3075effdb48de98708b1ec26714f8.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"用t-SNE进行流形学习\"><a href=\"#用t-SNE进行流形学习\" class=\"headerlink\" title=\"用t-SNE进行流形学习\"></a>用t-SNE进行流形学习</h3><p>有一类用于可视化的算法 叫做<strong>流形学习算法</strong>，它允许进行更复杂的映射，通常也可以给出更好的可视化。<br>其中特别有用的一个就是t-SNE算法。</p>\n<p>我们将对scikit-learn包含的一个手写数字数据集应用 t-SNE流形学习算法。<br>在这个数据集中，每个数据点都是0到9之间手写数字的一张8x8灰度图像。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.datasets import load_digits\ndigits &#x3D; load_digits()\nfig,axes&#x3D;plt.subplots(2,5,figsize&#x3D;(10,5),subplot_kw&#x3D;&#123;&#39;xticks&#39;:(),&#39;yticks&#39;:()&#125;)\nfor ax,img in zip(axes.ravel(),digits.images):\n    ax.imshow(img)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/884b7c0ae2404a41b1d4e5bf392c7c37.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们用PCA将降到二维的数据可视化。<br>我们对前两个主成分作图，并按类别对数据点着色：</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">#构建一个PCA模型\npca&#x3D;PCA(n_components&#x3D;2)\npca.fit(digits.data)\n#将digits数据变换到前两个主成分的方向上\ndigits_pca&#x3D;pca.transform(digits.data)\ncolors&#x3D;[&quot;#476A2A&quot;,&quot;#7851B8&quot;,&quot;#BD3430&quot;,&quot;#4A2D4E&quot;,&quot;#8755255&quot;,&quot;#A83683&quot;,&quot;#4E655E&quot;,&quot;#853541&quot;,&quot;#3A3120&quot;,&quot;#535D8E&quot;]\nplt.figure(figsize&#x3D;(10,10))\nplt.xlim(digits_pca[:,0].min(),digits_pca[:,0].max())\nplt.ylim(digits_pca[:,1].min(),digits_pca[:,1].max())\nfor i in range(len(digits.data)):\n    plt.text(digits_pca[i,0],digits_pca[i,1],str(digits.target[i]),color&#x3D;colors[digits.target[i]],fontdict&#x3D;&#123;&#39;weight&#39;:&#39;bold&#39;,&#39;size&#39;:9&#125;)\nplt.xlabel(&quot;First principal component&quot;)\nplt.ylabel(&quot;Second principal component&quot;)</code></pre>\n<p><strong>利用前两个主成分绘制digits数据集的散点图</strong><br><img src=\"https://img-blog.csdnimg.cn/8e99108e9fe14a3ab1f6de0152a21244.png#pic_center\" alt=\"在这里插入图片描述\"><br>我们将t-SNE应用于同一个数据集，并对结果进行比较。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">from sklearn.manifold import TSNE\ntsne&#x3D;TSNE(random_state&#x3D;42)\n#使用fit_transform而不是fit,因为TSNE没有transform方法\ndigits_tsne&#x3D;tsne.fit_transform(digits.data)</code></pre>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">plt.figure(figsize&#x3D;(10,10))\nplt.xlim(digits_tsne[:,0].min(),digits_tsne[:,0].max()+1)\nplt.ylim(digits_tsne[:,1].min(),digits_tsne[:,1].max()+1)\nfor i in range(len(digits.data)):\n    plt.text(digits_tsne[i,0],digits_tsne[i,1],str(digits.target[i]),color&#x3D;colors[digits.target[i]],fontdict&#x3D;&#123;&#39;weight&#39;:&#39;bold&#39;,&#39;size&#39;:9&#125;)\nplt.xlabel(&quot;t-SNE feature 0&quot;)\nplt.ylabel(&quot;t-SNE feature 1&quot;)</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/92cca8a25cce44ee94ae4d99a435bb23.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><p><strong>聚类</strong>是将数据集划分成组的任务，这些组叫做<strong>簇</strong>。<br>其目标是划分区域，使得<em>一个簇内的数据点**</em>非常相似<strong>且<em>不同于簇内的数据点</em></strong>非常不同**。<br>与分类算法类似，聚类算法为每个数据点分配（或预测）一个数字，表示这个点属于哪个簇。</p>\n<h3 id=\"k均值聚类\"><a href=\"#k均值聚类\" class=\"headerlink\" title=\"k均值聚类\"></a>k均值聚类</h3><p>k均值聚类是最简单也是最常用的聚类算法之一。<br>它试图找到代表数据特定区域的<strong>簇中心</strong>。<br>算法交替执行以下两个步骤：<br>将每个数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值。<br>如果簇的分配不再发生变化，那么算法结束。</p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_kmeans_algorithm()</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/147b72a2da454aad8b76ded12868687b.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n<pre class=\"line-numbers language-cpp\" data-language=\"cpp\"><code class=\"language-cpp\">mglearn.plots.plot_kmeans_boundaries()</code></pre>\n<p><strong>k均值算法找到的簇中心和簇边界</strong><br><img src=\"https://img-blog.csdnimg.cn/f184c077d72c4e7fa9024b4349709360.png#pic_center\" alt=\"在这里插入图片描述\"></p>\n","text":"无监督学习与预处理无监督学习的预处理本章研究两类无监督学习：数据集变换与聚类 数据集的无监督变换是创建数据新的表示的算法。 无监督变换的一个常见应用是降维，它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新方法，用较少的特征就可以概括其特性。降维的一个常见应用是为了可视...","link":"","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","count":5,"path":"api/tags/机器学习.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86\"><span class=\"toc-text\">无监督学习与预处理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86\"><span class=\"toc-text\">无监督学习的预处理</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E7%BC%A9%E6%94%BE\"><span class=\"toc-text\">预处理和缩放</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%99%8D%E7%BB%B4%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">降维、特征提取与流形学习</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90\"><span class=\"toc-text\">主成分分析</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%B0%86PCA%E5%BA%94%E7%94%A8%E4%BA%8Ecancer%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B9%B6%E5%8F%AF%E8%A7%86%E5%8C%96\"><span class=\"toc-text\">将PCA应用于cancer数据集并可视化</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%9A%84%E7%89%B9%E5%BE%81%E8%84%B8\"><span class=\"toc-text\">特征提取的特征脸</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3\"><span class=\"toc-text\">非负矩阵分解</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%B0%86NMF%E5%BA%94%E7%94%A8%E4%BA%8E%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE\"><span class=\"toc-text\">将NMF应用于模拟数据</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%B0%86NMF%E5%BA%94%E7%94%A8%E4%BA%8E%E4%BA%BA%E8%84%B8%E5%9B%BE%E5%83%8F\"><span class=\"toc-text\">将NMF应用于人脸图像</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%94%A8t-SNE%E8%BF%9B%E8%A1%8C%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0\"><span class=\"toc-text\">用t-SNE进行流形学习</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%81%9A%E7%B1%BB\"><span class=\"toc-text\">聚类</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB\"><span class=\"toc-text\">k均值聚类</span></a></li></ol></li></ol></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"统计学习导论 学习笔记（1）","uid":"2c79875937ce4f2b27d6f5db2a33f4e1","slug":"统计学习导论1","date":"2022-11-03T13:28:49.000Z","updated":"2022-11-08T16:31:32.232Z","comments":true,"path":"api/articles/统计学习导论1.json","keywords":null,"cover":null,"text":" 大部分统计学习问题分为以下两种类型：指导学习和无指导学习。对每一个预测变量$x_i$(i=1,…,n)都有相应变量的观测$y_i$。建模的目标是通过建立预测变量与响应变量的关系，精准预测响应变量或更好理解响应变量与预测变量的关系。 许多传统的统计学习方法，比如线性回归和逻辑斯谛...","link":"","photos":[],"count_time":{"symbolsCount":"1.5k","symbolsTime":"1 mins."},"categories":[{"name":"学习笔记","slug":"学习笔记","count":3,"path":"api/categories/学习笔记.json"}],"tags":[{"name":"算法","slug":"算法","count":4,"path":"api/tags/算法.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/68c4c7d8696c482da565ab5c8ebfa2fa.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{}}