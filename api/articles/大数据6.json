{"title":"【大数据基础】MapReduce 实验","uid":"35c915c8f544b6b45f887f7e156148c0","slug":"大数据6","date":"2023-03-15T13:50:49.000Z","updated":"2023-04-04T02:37:44.214Z","comments":true,"path":"api/articles/大数据6.json","keywords":null,"cover":[],"content":"<p><a href=\"https://dblab.xmu.edu.cn/blog/631/\">https://dblab.xmu.edu.cn/blog/631/</a></p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p><a href=\"https://dblab.xmu.edu.cn/blog/31/\">https://dblab.xmu.edu.cn/blog/31/</a></p></blockquote>\n<h2 id=\"实验过程\"><a href=\"#实验过程\" class=\"headerlink\" title=\"实验过程\"></a>实验过程</h2><p>下载hadoop-eclipse-plugin，将 release 中的 hadoop-eclipse-kepler-plugin-2.6.0.jar （还提供了 2.2.0 和 2.4.1 版本）复制到 Eclipse 安装目录的 plugins 文件夹中，运行 eclipse -clean 重启 Eclipse 即可（添加插件后只需要运行一次该命令，以后按照正常方式启动就行了）。</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">unzip -qo ~&#x2F;下载&#x2F;hadoop2x-eclipse-plugin-master.zip -d ~&#x2F;下载    # 解压到 ~&#x2F;下载 中\nsudo cp ~&#x2F;下载&#x2F;hadoop2x-eclipse-plugin-master&#x2F;release&#x2F;hadoop-eclipse-plugin-2.6.0.jar &#x2F;usr&#x2F;lib&#x2F;eclipse&#x2F;plugins&#x2F;    # 复制到 eclipse 安装目录的 plugins 目录下\n&#x2F;usr&#x2F;lib&#x2F;eclipse&#x2F;eclipse -clean    # 添加插件后需要用这种方式使插</code></pre>\n<p><img src=\"https://img-blog.csdnimg.cn/0c9d13ee6cc84f91a2e0542c16034937.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"配置-Hadoop-Eclipse-Plugin\"><a href=\"#配置-Hadoop-Eclipse-Plugin\" class=\"headerlink\" title=\"配置 Hadoop-Eclipse-Plugin\"></a>配置 Hadoop-Eclipse-Plugin</h3><p>在继续配置前请确保已经开启了 Hadoop。<br>插件需要进一步的配置。</p>\n<p>第一步：选择 Window 菜单下的 Preference。<br>此时会弹出一个窗体，窗体的左侧会多出 Hadoop Map/Reduce 选项，点击此选项，选择 Hadoop 的安装目录（如/usr/local/hadoop，Ubuntu不好选择目录，直接输入就行）。<br><img src=\"https://img-blog.csdnimg.cn/9b2063f2934a4b3bb3bd38d3d6b87a73.png\" alt=\"在这里插入图片描述\"><br>第二步：切换 Map/Reduce 开发视图，选择 Window 菜单下选择 Open Perspective -&gt; Other（CentOS 是 Window -&gt; Perspective -&gt; Open Perspective -&gt; Other），弹出一个窗体，从中选择 Map/Reduce 选项即可进行切换。<br><img src=\"https://img-blog.csdnimg.cn/c147931115a24acab7f84fcd510fea7f.png\" alt=\"在这里插入图片描述\"><br>第三步：建立与 Hadoop 集群的连接，点击 Eclipse软件右下角的 Map/Reduce Locations 面板，在面板中单击右键，选择 New Hadoop Location。<br><img src=\"https://img-blog.csdnimg.cn/799ac64b37484ce391146860f0c45b43.png\" alt=\"在这里插入图片描述\"></p>\n<p><img src=\"https://img-blog.csdnimg.cn/3c26afb8bd9f4e84884d5b1cdc5f4411.png\" alt=\"在这里插入图片描述\"><br>最后的设置如下图所示：<br><img src=\"https://img-blog.csdnimg.cn/b049e24e8212457882b85cd61385720c.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"在-Eclipse-中操作-HDFS-中的文件\"><a href=\"#在-Eclipse-中操作-HDFS-中的文件\" class=\"headerlink\" title=\"在 Eclipse 中操作 HDFS 中的文件\"></a>在 Eclipse 中操作 HDFS 中的文件</h3><p>配置好后，点击左侧 Project Explorer 中的 MapReduce Location （点击三角形展开）就能直接查看 HDFS 中的文件列表了（HDFS 中要有文件，如下图是 WordCount 的输出结果），双击可以查看内容，右键点击可以上传、下载、删除 HDFS 中的文件，无需再通过繁琐的 hdfs dfs -ls 等命令进行操作了。<br>以下output/part-r-00000文件记录了输出结果。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/440f4f8b4ee24c83928c68bf29d79409.png\" alt=\"在这里插入图片描述\"><br>如果无法查看，可右键点击 Location 尝试 Reconnect 或重启 Eclipse。</p>\n<p>Tips: HDFS 中的内容变动后，Eclipse 不会同步刷新，需要右键点击 Project Explorer中的 MapReduce Location，选择 Refresh，才能看到变动后的文件。</p>\n<h3 id=\"在-Eclipse-中创建-MapReduce-项目\"><a href=\"#在-Eclipse-中创建-MapReduce-项目\" class=\"headerlink\" title=\"在 Eclipse 中创建 MapReduce 项目\"></a>在 Eclipse 中创建 MapReduce 项目</h3><p>点击 File 菜单，选择 New -&gt; Project…:<br>选择 Map/Reduce Project，点击 Next。<br><img src=\"https://img-blog.csdnimg.cn/839c1d5df54e4af1aeeeda01a5b995af.png\" alt=\"在这里插入图片描述\"><br>填写 Project name 为 WordCount 即可，点击 Finish 就创建好了项目。<br><img src=\"https://img-blog.csdnimg.cn/64d9d127299c4e2fba535e932e7963e4.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/7fcd308db7ee41d980aa8aa6b8fa64bd.png\" alt=\"在这里插入图片描述\"><br>此时在左侧的 Project Explorer 就能看到刚才建立的项目了。<br><img src=\"https://img-blog.csdnimg.cn/eb6b548ebc974e0f8a0f8486687d2040.png\" alt=\"在这里插入图片描述\"><br>接着右键点击刚创建的 WordCount 项目，选择 New -&gt; Class<br><img src=\"https://img-blog.csdnimg.cn/1a313138cc414026b40f7b151e031bdf.png\" alt=\"在这里插入图片描述\"><br>需要填写两个地方：在 Package 处填写 org.apache.hadoop.examples；在 Name 处填写 WordCount。<br><img src=\"https://img-blog.csdnimg.cn/0874316a69b84eb683b62276d40b755c.png\" alt=\"在这里插入图片描述\"><br>创建 Class 完成后，在 Project 的 src 中就能看到 WordCount.java 这个文件。将如下 WordCount 的代码复制到该文件中。</p>\n<pre class=\"line-numbers language-java\" data-language=\"java\"><code class=\"language-java\">package org.apache.hadoop.examples;\n \nimport java.io.IOException;\nimport java.util.Iterator;\nimport java.util.StringTokenizer;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n \npublic class WordCount &#123;\n    public WordCount() &#123;\n    &#125;\n \n    public static void main(String[] args) throws Exception &#123;\n        Configuration conf &#x3D; new Configuration();\n        String[] otherArgs &#x3D; (new GenericOptionsParser(conf, args)).getRemainingArgs();\n        if(otherArgs.length &lt; 2) &#123;\n            System.err.println(&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;);\n            System.exit(2);\n        &#125;\n \n        Job job &#x3D; Job.getInstance(conf, &quot;word count&quot;);\n        job.setJarByClass(WordCount.class);\n        job.setMapperClass(WordCount.TokenizerMapper.class);\n        job.setCombinerClass(WordCount.IntSumReducer.class);\n        job.setReducerClass(WordCount.IntSumReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n \n        for(int i &#x3D; 0; i &lt; otherArgs.length - 1; ++i) &#123;\n            FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n        &#125;\n \n        FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1]));\n        System.exit(job.waitForCompletion(true)?0:1);\n    &#125;\n \n    public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;\n        private IntWritable result &#x3D; new IntWritable();\n \n        public IntSumReducer() &#123;\n        &#125;\n \n        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;\n            int sum &#x3D; 0;\n \n            IntWritable val;\n            for(Iterator i$ &#x3D; values.iterator(); i$.hasNext(); sum +&#x3D; val.get()) &#123;\n                val &#x3D; (IntWritable)i$.next();\n            &#125;\n \n            this.result.set(sum);\n            context.write(key, this.result);\n        &#125;\n    &#125;\n \n    public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123;\n        private static final IntWritable one &#x3D; new IntWritable(1);\n        private Text word &#x3D; new Text();\n \n        public TokenizerMapper() &#123;\n        &#125;\n \n        public void map(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;\n            StringTokenizer itr &#x3D; new StringTokenizer(value.toString());\n \n            while(itr.hasMoreTokens()) &#123;\n                this.word.set(itr.nextToken());\n                context.write(this.word, one);\n            &#125;\n \n        &#125;\n    &#125;\n&#125;</code></pre>\n<h3 id=\"通过-Eclipse-运行-MapReduce\"><a href=\"#通过-Eclipse-运行-MapReduce\" class=\"headerlink\" title=\"通过 Eclipse 运行 MapReduce\"></a>通过 Eclipse 运行 MapReduce</h3><p>在运行 MapReduce 程序前，还需要执行一项重要操作（也就是上面提到的通过复制配置文件解决参数设置问题）：将 /usr/local/hadoop/etc/hadoop 中将有修改过的配置文件（如伪分布式需要 core-site.xml 和 hdfs-site.xml），以及 log4j.properties 复制到 WordCount 项目下的 src 文件夹（~/workspace/WordCount/src）中：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">cp &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml ~&#x2F;workspace&#x2F;WordCount&#x2F;src\ncp &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;hdfs-site.xml ~&#x2F;workspace&#x2F;WordCount&#x2F;src\ncp &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;log4j.properties ~&#x2F;workspace&#x2F;WordCount&#x2F;src</code></pre>\n<p>这里写代码有权限问题，我们直接在图形化界面上复制：</p>\n<p><img src=\"https://img-blog.csdnimg.cn/048314cfc7764d60ad88678ad2636f8d.png\" alt=\"在这里插入图片描述\"><br>复制完成后，务必右键点击 WordCount 选择 refresh 进行刷新（不会自动刷新，需要手动刷新），可以看到文件结构如下所示：<br><img src=\"https://img-blog.csdnimg.cn/372ae8bffcbe4125945223a38a4f3c03.png\" alt=\"在这里插入图片描述\"><br>点击工具栏中的 Run 图标，或者右键点击 Project Explorer 中的 WordCount.java，选择 Run As -&gt; Run on Hadoop，就可以运行 MapReduce 程序了。不过由于没有指定参数，运行时会提示 “Usage: wordcount “，需要通过Eclipse设定一下运行参数。</p>\n<p>右键点击刚创建的 WordCount.java，选择 Run As -&gt; Run Configurations，在此处可以设置运行时的相关参数（如果 Java Application 下面没有 WordCount，那么需要先双击 Java Application）。切换到 “Arguments” 栏，在 Program arguments 处填写 “input output” 就可以了。<br><img src=\"https://img-blog.csdnimg.cn/b57e9e969868429793c8bcc1c86c1612.png\" alt=\"在这里插入图片描述\"><br><img src=\"https://img-blog.csdnimg.cn/96ab1efab4e744c4b7f7eb355484b691.png\" alt=\"在这里插入图片描述\"><br>或者也可以直接在代码中设置好输入参数。可将代码 main() 函数的 String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); 改为：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">&#x2F;&#x2F; String[] otherArgs &#x3D; new GenericOptionsParser(conf, args).getRemainingArgs();\nString[] otherArgs&#x3D;new String[]&#123;&quot;input&quot;,&quot;output&quot;&#125;; &#x2F;* 直接设置输入参数 *&#x2F;</code></pre>\n<p>设定参数后，再次运行程序，可以看到运行成功的提示，刷新 DFS Location 后也能看到输出的 output 文件夹。<br><img src=\"https://img-blog.csdnimg.cn/362c5c772c314ff2a8b85973d7f608ec.png\" alt=\"在这里插入图片描述\"><br>然后就可以使用 Eclipse 方便的进行 MapReduce程序的开发了。</p>\n<h2 id=\"问题及处理\"><a href=\"#问题及处理\" class=\"headerlink\" title=\"问题及处理\"></a>问题及处理</h2><h3 id=\"1-无法解压并移动文件\"><a href=\"#1-无法解压并移动文件\" class=\"headerlink\" title=\"1  无法解压并移动文件\"></a>1  无法解压并移动文件</h3><p><img src=\"https://img-blog.csdnimg.cn/967f7e85cbfc45cdad15b6e90f1c5004.png\" alt=\"在这里插入图片描述\"><br>原因：eclipse是在ubuntu自带的snap商店里下载的，而snap有很高的权限，即使用sudo和su也不好修改。<br><img src=\"https://img-blog.csdnimg.cn/6c262ad10ce5418eac125361add1291d.png\" alt=\"在这里插入图片描述\"><br>解决方法：从软件安装中心卸载，并使用安装包重新安装在对应地址。<br><pre class=\"line-numbers language-c\" data-language=\"c\"><code class=\"language-c\">sudo cp ~&#x2F;下载&#x2F;hadoop2x-eclipse-plugin-master&#x2F;release&#x2F;hadoop-eclipse-plugin-2.6.0.jar &#x2F;snap&#x2F;eclipse&#x2F;66&#x2F;plugins&#x2F;\n</code></pre><br><img src=\"https://img-blog.csdnimg.cn/d9634dab108845e9a09db59849312d3b.png\" alt=\"在这里插入图片描述\"></p>\n<h3 id=\"2-java安装包版本不匹配\"><a href=\"#2-java安装包版本不匹配\" class=\"headerlink\" title=\"2 java安装包版本不匹配\"></a>2 java安装包版本不匹配</h3><p>解决方法：</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p><a href=\"https://blog.csdn.net/weixin_44156420/article/details/106906023\">https://blog.csdn.net/weixin_44156420/article/details/106906023</a></p>\n<p><img src=\"https://img-blog.csdnimg.cn/362c5c772c314ff2a8b85973d7f608ec.png\" alt=\"在这里插入图片描述\"><br>这是版本的问题，为了之后实验顺畅进行，就不强行更改了。</p></blockquote>\n<h2 id=\"心得与体会\"><a href=\"#心得与体会\" class=\"headerlink\" title=\"心得与体会\"></a>心得与体会</h2><p>这次实验比较简单，最大的障碍是上文中的问题1：无法解压并移动文件，在经过一上午和老师的讨论，以及查找资料后发现，ubuntu软件安装中心的安装地址在snap下，而这个的权限很高，直接使用sudo无法修改，所以中午回去使用官网的安装包，解压安装一步到位。之后的流程就很顺畅了。</p>\n","text":"https://dblab.xmu.edu.cn/blog/631/ https://dblab.xmu.edu.cn/blog/31/ 实验过程下载hadoop-eclipse-plugin，将 release 中的 hadoop-eclipse-kepler-plugin-2...","link":"","photos":[],"count_time":{"symbolsCount":"7.1k","symbolsTime":"6 mins."},"categories":[{"name":"信管","slug":"信管","count":19,"path":"api/categories/信管.json"}],"tags":[{"name":"大数据","slug":"大数据","count":7,"path":"api/tags/大数据.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%AE%9E%E9%AA%8C%E8%BF%87%E7%A8%8B\"><span class=\"toc-text\">实验过程</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%85%8D%E7%BD%AE-Hadoop-Eclipse-Plugin\"><span class=\"toc-text\">配置 Hadoop-Eclipse-Plugin</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9C%A8-Eclipse-%E4%B8%AD%E6%93%8D%E4%BD%9C-HDFS-%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">在 Eclipse 中操作 HDFS 中的文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%9C%A8-Eclipse-%E4%B8%AD%E5%88%9B%E5%BB%BA-MapReduce-%E9%A1%B9%E7%9B%AE\"><span class=\"toc-text\">在 Eclipse 中创建 MapReduce 项目</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%9A%E8%BF%87-Eclipse-%E8%BF%90%E8%A1%8C-MapReduce\"><span class=\"toc-text\">通过 Eclipse 运行 MapReduce</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%97%AE%E9%A2%98%E5%8F%8A%E5%A4%84%E7%90%86\"><span class=\"toc-text\">问题及处理</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#1-%E6%97%A0%E6%B3%95%E8%A7%A3%E5%8E%8B%E5%B9%B6%E7%A7%BB%E5%8A%A8%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">1  无法解压并移动文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#2-java%E5%AE%89%E8%A3%85%E5%8C%85%E7%89%88%E6%9C%AC%E4%B8%8D%E5%8C%B9%E9%85%8D\"><span class=\"toc-text\">2 java安装包版本不匹配</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%BF%83%E5%BE%97%E4%B8%8E%E4%BD%93%E4%BC%9A\"><span class=\"toc-text\">心得与体会</span></a></li></ol>","author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}},"mapped":true,"prev_post":{"title":"【大数据基础】2020年美国新冠肺炎疫情数据分析","uid":"a66db97004d2cce682fbaf0db4f8334b","slug":"大数据7","date":"2023-03-27T13:50:49.000Z","updated":"2023-04-04T02:39:09.538Z","comments":true,"path":"api/articles/大数据7.json","keywords":null,"cover":[],"text":"https://dblab.xmu.edu.cn/blog/2738 https://dblab.xmu.edu.cn/blog/2636/ spark 安装安装 Spark2.4.0sudo tar -zxf ~&#x2F;下载&#x2F;spark-2.4.0-bin-wit...","link":"","photos":[],"count_time":{"symbolsCount":"43k","symbolsTime":"39 mins."},"categories":[{"name":"信管","slug":"信管","count":19,"path":"api/categories/信管.json"}],"tags":[{"name":"大数据","slug":"大数据","count":7,"path":"api/tags/大数据.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}},"next_post":{"title":"【大数据基础】HBase2.2.2安装及编程实践指南","uid":"f8ebb92f8b4529095c1c2d3c0a9053f4","slug":"大数据5","date":"2023-03-12T13:50:49.000Z","updated":"2023-04-04T02:36:23.836Z","comments":true,"path":"api/articles/大数据5.json","keywords":null,"cover":[],"text":"实验 https://dblab.xmu.edu.cn/blog/2442/ HBase2.2.2安装&gt; 解压安装包hbase-2.2.2-bin.tar.gz至路径 /usr/local，命令如下： cd ~ sudo tar -zxf ~&#x2F;下载&#x2F;hb...","link":"","photos":[],"count_time":{"symbolsCount":"12k","symbolsTime":"11 mins."},"categories":[{"name":"信管","slug":"信管","count":19,"path":"api/categories/信管.json"}],"tags":[{"name":"大数据","slug":"大数据","count":7,"path":"api/tags/大数据.json"}],"author":{"name":"Algernon","slug":"blog-author","avatar":"https://user-images.githubusercontent.com/54904760/224857900-b2e8457c-43d2-46b7-901c-6c770f24bbad.png","link":"/","description":"谁也没见过风，更别说我和你了","socials":{"github":"https://github.com/Algernon98","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"https://blog.csdn.net/Algernon98","juejin":"","customs":{"bilibili":{"icon":"/svg/bilibili-line.svg","link":"https://space.bilibili.com/281724502"}}}}}}